üîù Retour au [Sommaire](/SOMMAIRE.md)

# 11.3 Monitoring cloud-native

## Introduction au monitoring cloud-native

### Qu'est-ce que le monitoring cloud-native ?

Le **monitoring cloud-native** est l'art de surveiller et observer des applications distribu√©es qui s'ex√©cutent dans des environnements dynamiques comme Kubernetes. Contrairement au monitoring traditionnel qui surveille des serveurs fixes, le monitoring cloud-native doit g√©rer :

- **Des conteneurs √©ph√©m√®res** : qui apparaissent et disparaissent constamment
- **Des services distribu√©s** : r√©partis sur plusieurs machines et r√©gions
- **Une infrastructure dynamique** : qui s'adapte automatiquement √† la charge
- **Des d√©ploiements fr√©quents** : plusieurs fois par jour

### Les d√©fis du monitoring cloud-native

1. **Complexit√©** : Surveiller des centaines de microservices interconnect√©s
2. **Dynamisme** : Les composants changent constamment d'adresse IP
3. **Volume** : √ânorm√©ment de m√©triques et de logs √† traiter
4. **Corr√©lation** : Relier les √©v√©nements √† travers tous les services
5. **Performance** : Ne pas impacter les applications surveill√©es

### Les trois piliers de l'observabilit√©

L'observabilit√© repose sur trois types de donn√©es compl√©mentaires :

1. **M√©triques** : Donn√©es num√©riques agr√©g√©es (CPU, m√©moire, requ√™tes/seconde)
2. **Logs** : √âv√©nements textuels d√©taill√©s avec horodatage
3. **Traces** : Suivi d'une requ√™te √† travers tous les services

### L'√©cosyst√®me CNCF

La Cloud Native Computing Foundation (CNCF) a standardis√© plusieurs outils :

- **Prometheus** : Collecte et stockage de m√©triques
- **Grafana** : Visualisation et dashboards
- **Jaeger** : Tracing distribu√©
- **Fluentd/Fluent Bit** : Collecte et routage de logs
- **OpenTelemetry** : Standard pour l'instrumentation

---

## 11.3.1 Prometheus et AlertManager

### Qu'est-ce que Prometheus ?

**Prometheus** est un syst√®me de monitoring et d'alerting open-source cr√©√© par SoundCloud en 2012. Il est devenu le standard de facto pour le monitoring cloud-native gr√¢ce √† sa capacit√© √† d√©couvrir automatiquement les services et √† g√©rer les environnements dynamiques.

### Architecture de Prometheus

Prometheus fonctionne selon un mod√®le **pull** :

1. **Prometheus Server** : Collecte et stocke les m√©triques
2. **Targets** : Applications qui exposent des m√©triques
3. **Service Discovery** : D√©couverte automatique des services
4. **Storage** : Base de donn√©es de s√©ries temporelles
5. **PromQL** : Langage de requ√™te pour les m√©triques

### Installation de Prometheus sur Debian

#### M√©thode 1 : Installation binaire

```bash
# Cr√©ation d'un utilisateur prometheus
sudo useradd --no-create-home --shell /bin/false prometheus

# Cr√©ation des r√©pertoires
sudo mkdir /etc/prometheus
sudo mkdir /var/lib/prometheus
sudo chown prometheus:prometheus /etc/prometheus
sudo chown prometheus:prometheus /var/lib/prometheus

# T√©l√©chargement de Prometheus
cd /tmp
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar xvf prometheus-2.45.0.linux-amd64.tar.gz

# Installation des binaires
sudo cp prometheus-2.45.0.linux-amd64/prometheus /usr/local/bin/
sudo cp prometheus-2.45.0.linux-amd64/promtool /usr/local/bin/
sudo chown prometheus:prometheus /usr/local/bin/prometheus
sudo chown prometheus:prometheus /usr/local/bin/promtool

# Copie des fichiers de configuration
sudo cp -r prometheus-2.45.0.linux-amd64/consoles /etc/prometheus
sudo cp -r prometheus-2.45.0.linux-amd64/console_libraries /etc/prometheus
sudo chown -R prometheus:prometheus /etc/prometheus/consoles
sudo chown -R prometheus:prometheus /etc/prometheus/console_libraries
```

#### Configuration de base

```yaml
# /etc/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

rule_files:
  - "rules/*.yml"

scrape_configs:
  # Prometheus lui-m√™me
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter pour les m√©triques syst√®me
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

  # D√©couverte automatique Kubernetes (si applicable)
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
```

#### Service systemd

```bash
# Cr√©ation du service systemd
sudo tee /etc/systemd/system/prometheus.service > /dev/null <<EOF
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090 \
    --web.enable-lifecycle

[Install]
WantedBy=multi-user.target
EOF

# Activation et d√©marrage
sudo systemctl daemon-reload
sudo systemctl enable prometheus
sudo systemctl start prometheus
sudo systemctl status prometheus
```

### Installation de Node Exporter

Node Exporter collecte les m√©triques syst√®me (CPU, m√©moire, disque, r√©seau) :

```bash
# T√©l√©chargement et installation
cd /tmp
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.0/node_exporter-1.6.0.linux-amd64.tar.gz
tar xvf node_exporter-1.6.0.linux-amd64.tar.gz
sudo cp node_exporter-1.6.0.linux-amd64/node_exporter /usr/local/bin/
sudo chown prometheus:prometheus /usr/local/bin/node_exporter

# Service systemd pour Node Exporter
sudo tee /etc/systemd/system/node_exporter.service > /dev/null <<EOF
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=multi-user.target
EOF

# Activation
sudo systemctl daemon-reload
sudo systemctl enable node_exporter
sudo systemctl start node_exporter
```

### PromQL - Le langage de requ√™te

PromQL (Prometheus Query Language) permet d'interroger les m√©triques :

#### Requ√™tes de base

```promql
# M√©trique simple
up

# M√©trique avec labels
up{job="prometheus"}

# Op√©rateurs math√©matiques
node_memory_MemAvailable_bytes / 1024 / 1024  # Conversion en MB

# Fonctions de temps
rate(http_requests_total[5m])  # Taux par seconde sur 5 minutes

# Agr√©gation
sum(rate(http_requests_total[5m])) by (method)  # Somme par m√©thode HTTP
```

#### Requ√™tes avanc√©es

```promql
# Pourcentage d'utilisation CPU
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Pr√©diction bas√©e sur la tendance
predict_linear(node_filesystem_free_bytes[1h], 4*3600) < 0

# Top 5 des erreurs HTTP
topk(5, sum(rate(http_requests_total{status=~"5.."}[5m])) by (path))
```

### AlertManager

AlertManager g√®re les alertes envoy√©es par Prometheus :

#### Installation d'AlertManager

```bash
# T√©l√©chargement
cd /tmp
wget https://github.com/prometheus/alertmanager/releases/download/v0.25.0/alertmanager-0.25.0.linux-amd64.tar.gz
tar xvf alertmanager-0.25.0.linux-amd64.tar.gz

# Installation
sudo cp alertmanager-0.25.0.linux-amd64/alertmanager /usr/local/bin/
sudo cp alertmanager-0.25.0.linux-amd64/amtool /usr/local/bin/
sudo chown prometheus:prometheus /usr/local/bin/alertmanager
sudo chown prometheus:prometheus /usr/local/bin/amtool

# R√©pertoires
sudo mkdir /etc/alertmanager
sudo mkdir /var/lib/alertmanager
sudo chown prometheus:prometheus /etc/alertmanager
sudo chown prometheus:prometheus /var/lib/alertmanager
```

#### Configuration d'AlertManager

```yaml
# /etc/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alertmanager@example.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
- name: 'web.hook'
  email_configs:
  - to: 'admin@example.com'
    subject: 'Alerte Prometheus: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alerte: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Valeur: {{ .Annotations.value }}
      {{ end }}

  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts'
    title: 'Alerte Prometheus'
    text: |
      {{ range .Alerts }}
      {{ .Annotations.summary }}
      {{ end }}

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']
```

#### R√®gles d'alerte

```yaml
# /etc/prometheus/rules/alerts.yml
groups:
- name: system
  rules:
  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "CPU usage is above 80%"
      description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Memory usage is above 90%"
      description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"

  - alert: DiskSpaceLow
    expr: 100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100) > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Disk space is running low"
      description: "Disk usage is {{ $value }}% on {{ $labels.instance }}"

  - alert: ServiceDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Service {{ $labels.job }} is down"
      description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute"
```

---

## 11.3.2 Grafana et dashboards

### Qu'est-ce que Grafana ?

**Grafana** est une plateforme open-source de visualisation et d'analyse de donn√©es. Elle permet de cr√©er des dashboards interactifs pour visualiser les m√©triques collect√©es par Prometheus et d'autres sources de donn√©es.

### Fonctionnalit√©s principales de Grafana

- **Multi-sources** : Prometheus, InfluxDB, Elasticsearch, MySQL, etc.
- **Dashboards interactifs** : Graphiques dynamiques avec zoom et filtres
- **Alerting** : Syst√®me d'alertes bas√© sur les visualisations
- **Templating** : Variables pour des dashboards r√©utilisables
- **Plugins** : √âcosyst√®me riche d'extensions

### Installation de Grafana sur Debian

#### M√©thode 1 : D√©p√¥t officiel

```bash
# Ajout de la cl√© GPG
sudo apt-get install -y software-properties-common wget
wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -

# Ajout du d√©p√¥t
echo "deb https://packages.grafana.com/oss/deb stable main" | sudo tee -a /etc/apt/sources.list.d/grafana.list

# Installation
sudo apt-get update
sudo apt-get install grafana

# Activation et d√©marrage
sudo systemctl daemon-reload
sudo systemctl enable grafana-server
sudo systemctl start grafana-server
sudo systemctl status grafana-server
```

#### M√©thode 2 : Package DEB

```bash
# T√©l√©chargement et installation
wget https://dl.grafana.com/oss/release/grafana_10.0.0_amd64.deb
sudo dpkg -i grafana_10.0.0_amd64.deb
sudo apt-get install -f  # R√©soudre les d√©pendances
```

### Configuration initiale de Grafana

#### Acc√®s √† l'interface web

```bash
# Grafana est accessible sur http://localhost:3000
# Login par d√©faut : admin / admin
```

#### Configuration de la source de donn√©es Prometheus

1. **Interface web** : Settings ‚Üí Data Sources ‚Üí Add data source
2. **Type** : Prometheus
3. **URL** : http://localhost:9090
4. **Access** : Server (default)

Ou via l'API :

```bash
# Configuration automatique via API
curl -X POST \
  http://admin:admin@localhost:3000/api/datasources \
  -H 'Content-Type: application/json' \
  -d '{
    "name": "Prometheus",
    "type": "prometheus",
    "url": "http://localhost:9090",
    "access": "proxy",
    "isDefault": true
  }'
```

### Cr√©ation de dashboards

#### Dashboard de base pour monitoring syst√®me

```json
{
  "dashboard": {
    "title": "System Monitoring",
    "tags": ["prometheus", "system"],
    "timezone": "browser",
    "panels": [
      {
        "title": "CPU Usage",
        "type": "stat",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 70},
                {"color": "red", "value": 90}
              ]
            },
            "unit": "percent"
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0}
      },
      {
        "title": "Memory Usage",
        "type": "stat",
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "Memory Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 80},
                {"color": "red", "value": 95}
              ]
            },
            "unit": "percent"
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0}
      },
      {
        "title": "Disk Usage",
        "type": "stat",
        "targets": [
          {
            "expr": "100 - ((node_filesystem_avail_bytes{mountpoint=\"/\"} / node_filesystem_size_bytes{mountpoint=\"/\"}) * 100)",
            "legendFormat": "Disk Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 80},
                {"color": "red", "value": 90}
              ]
            },
            "unit": "percent"
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0}
      },
      {
        "title": "Network I/O",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(node_network_receive_bytes_total{device!=\"lo\"}[5m])",
            "legendFormat": "Receive {{device}}"
          },
          {
            "expr": "rate(node_network_transmit_bytes_total{device!=\"lo\"}[5m])",
            "legendFormat": "Transmit {{device}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "Bps"
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s"
  }
}
```

### Dashboards avec templates

#### Variables de template

```json
{
  "templating": {
    "list": [
      {
        "name": "instance",
        "type": "query",
        "query": "label_values(node_cpu_seconds_total, instance)",
        "refresh": 1,
        "includeAll": true,
        "allValue": ".*",
        "multi": true
      },
      {
        "name": "job",
        "type": "query",
        "query": "label_values(up, job)",
        "refresh": 1,
        "includeAll": false,
        "multi": false
      }
    ]
  }
}
```

#### Utilisation des variables

```promql
# Dans les requ√™tes, utiliser $variable
node_cpu_seconds_total{instance=~"$instance", job="$job"}

# Pour les titres de panels
"title": "CPU Usage - Instance: $instance"
```

### Dashboards Kubernetes

#### Dashboard pour pods

```json
{
  "dashboard": {
    "title": "Kubernetes Pods",
    "panels": [
      {
        "title": "Pod CPU Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total{pod=~\"$pod\", container!=\"POD\", container!=\"\"}[5m])) by (pod)",
            "legendFormat": "{{pod}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "cores"
          }
        }
      },
      {
        "title": "Pod Memory Usage",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(container_memory_working_set_bytes{pod=~\"$pod\", container!=\"POD\", container!=\"\"}) by (pod)",
            "legendFormat": "{{pod}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        }
      },
      {
        "title": "Pod Network I/O",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(container_network_receive_bytes_total{pod=~\"$pod\"}[5m])) by (pod)",
            "legendFormat": "Receive {{pod}}"
          },
          {
            "expr": "sum(rate(container_network_transmit_bytes_total{pod=~\"$pod\"}[5m])) by (pod)",
            "legendFormat": "Transmit {{pod}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "Bps"
          }
        }
      }
    ]
  }
}
```

### Alerting dans Grafana

#### Configuration des notifications

```json
{
  "name": "slack-notifications",
  "type": "slack",
  "settings": {
    "url": "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK",
    "username": "Grafana",
    "channel": "#alerts",
    "title": "Grafana Alert",
    "text": "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}"
  }
}
```

#### R√®gles d'alerte Grafana

```json
{
  "alert": {
    "name": "High CPU Alert",
    "message": "CPU usage is above 80%",
    "frequency": "10s",
    "conditions": [
      {
        "query": {
          "queryType": "",
          "refId": "A",
          "model": {
            "expr": "100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "interval": "",
            "refId": "A"
          }
        },
        "reducer": {
          "type": "last",
          "params": []
        },
        "evaluator": {
          "params": [80],
          "type": "gt"
        }
      }
    ],
    "executionErrorState": "alerting",
    "noDataState": "no_data",
    "for": "5m"
  }
}
```

---

## 11.3.3 Jaeger (tracing distribu√©)

### Qu'est-ce que le tracing distribu√© ?

Le **tracing distribu√©** permet de suivre une requ√™te utilisateur √† travers tous les microservices qu'elle traverse. Imaginez une commande e-commerce qui passe par les services : authentification ‚Üí catalogue ‚Üí inventaire ‚Üí paiement ‚Üí exp√©dition.

### Concepts cl√©s du tracing

- **Trace** : Le parcours complet d'une requ√™te
- **Span** : Une op√©ration individuelle dans le trace
- **Context** : Informations partag√©es entre spans
- **Sampling** : Pourcentage de traces collect√©es

### Architecture de Jaeger

Jaeger est compos√© de plusieurs composants :

- **Jaeger Client** : SDK pour instrumenter les applications
- **Jaeger Agent** : Collecteur local qui re√ßoit les spans
- **Jaeger Collector** : Agr√®ge et stocke les traces
- **Jaeger Query** : Interface de consultation des traces
- **Storage** : Backend (Cassandra, Elasticsearch, etc.)

### Installation de Jaeger

#### Installation All-in-One pour d√©veloppement

```bash
# T√©l√©chargement de Jaeger
cd /tmp
wget https://github.com/jaegertracing/jaeger/releases/download/v1.47.0/jaeger-1.47.0-linux-amd64.tar.gz
tar xzf jaeger-1.47.0-linux-amd64.tar.gz

# Installation des binaires
sudo cp jaeger-1.47.0-linux-amd64/jaeger-all-in-one /usr/local/bin/
sudo chmod +x /usr/local/bin/jaeger-all-in-one

# D√©marrage de Jaeger
jaeger-all-in-one --collector.zipkin.host-port=:9411
```

#### Installation avec Docker

```bash
# Jaeger All-in-One
docker run -d --name jaeger \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.47
```

#### Installation sur Kubernetes

```yaml
# jaeger-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.47
        ports:
        - containerPort: 16686
          name: ui
        - containerPort: 14268
          name: collector
        - containerPort: 14250
          name: grpc
        - containerPort: 9411
          name: zipkin
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: QUERY_BASE_PATH
          value: "/jaeger"
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: observability
spec:
  selector:
    app: jaeger
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
  - name: collector
    port: 14268
    targetPort: 14268
  - name: grpc
    port: 14250
    targetPort: 14250
  - name: zipkin
    port: 9411
    targetPort: 9411
  type: ClusterIP
```

### Instrumentation des applications

#### Exemple avec Python (Flask)

```python
# requirements.txt
jaeger-client==4.8.0
opentracing==2.4.0
flask==2.3.0

# app.py
from flask import Flask
from jaeger_client import Config
import opentracing

app = Flask(__name__)

# Configuration Jaeger
config = Config(
    config={
        'sampler': {
            'type': 'const',
            'param': 1,  # Sample 100% des traces
        },
        'logging': True,
        'reporter_batch_size': 1,
    },
    service_name='web-service',
    validate=True,
)

tracer = config.initialize_tracer()
opentracing.set_global_tracer(tracer)

@app.route('/api/users/<user_id>')
def get_user(user_id):
    with tracer.start_span('get_user') as span:
        span.set_tag('user.id', user_id)

        # Simuler un appel base de donn√©es
        with tracer.start_span('database_query', child_of=span) as db_span:
            db_span.set_tag('db.statement', f'SELECT * FROM users WHERE id = {user_id}')
            db_span.set_tag('db.type', 'postgresql')

            # Simulation
            import time
            time.sleep(0.1)

            user_data = {'id': user_id, 'name': 'John Doe'}

        span.set_tag('user.found', True)
        return user_data

if __name__ == '__main__':
    app.run(debug=True)
```

#### Exemple avec Java (Spring Boot)

```java
// pom.xml dependencies
<dependency>
    <groupId>io.jaegertracing</groupId>
    <artifactId>jaeger-client</artifactId>
    <version>1.8.1</version>
</dependency>
<dependency>
    <groupId>io.opentracing.contrib</groupId>
    <artifactId>opentracing-spring-jaeger-web-starter</artifactId>
    <version>3.3.1</version>
</dependency>

// application.properties
opentracing.jaeger.service-name=order-service
opentracing.jaeger.http-sender.url=http://localhost:14268/api/traces
opentracing.jaeger.sampler-type=const
opentracing.jaeger.sampler-param=1

// OrderController.java
@RestController
@RequestMapping("/api/orders")
public class OrderController {

    @Autowired
    private Tracer tracer;

    @GetMapping("/{orderId}")
    public Order getOrder(@PathVariable String orderId) {
        Span span = tracer.nextSpan()
            .name("get_order")
            .tag("order.id", orderId)
            .start();

        try (Tracer.SpanInScope ws = tracer.withSpanInScope(span)) {
            // Appel service de paiement
            Span paymentSpan = tracer.nextSpan()
                .name("check_payment")
                .tag("service", "payment")
                .start();

            try (Tracer.SpanInScope ps = tracer.withSpanInScope(paymentSpan)) {
                // Logique m√©tier
                return orderService.findById(orderId);
            } finally {
                paymentSpan.end();
            }
        } finally {
            span.end();
        }
    }
}
```

### OpenTelemetry

OpenTelemetry est le nouveau standard pour l'instrumentation :

#### Configuration OpenTelemetry Python

```python
# requirements.txt
opentelemetry-api==1.19.0
opentelemetry-sdk==1.19.0
opentelemetry-exporter-jaeger-thrift==1.19.0
opentelemetry-instrumentation-flask==0.40b0
opentelemetry-instrumentation-requests==0.40b0

# otel_setup.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

def configure_tracer():
    # Configuration du provider de traces
    trace.set_tracer_provider(TracerProvider())
    tracer = trace.get_tracer(__name__)

    # Configuration de l'exporteur Jaeger
    jaeger_exporter = JaegerExporter(
        agent_host_name="localhost",
        agent_port=6831,
    )

    # Ajout du processeur de spans
    span_processor = BatchSpanProcessor(jaeger_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)

    # Auto-instrumentation
    FlaskInstrumentor().instrument()
    RequestsInstrumentor().instrument()

    return tracer

# app_with_otel.py
from flask import Flask
from otel_setup import configure_tracer
import requests

app = Flask(__name__)
tracer = configure_tracer()

@app.route('/api/weather/<city>')
def get_weather(city):
    with tracer.start_as_current_span("get_weather") as span:
        span.set_attribute("city", city)
        span.set_attribute("service.name", "weather-api")

        # Appel API externe (automatiquement trac√© par RequestsInstrumentor)
        response = requests.get(f"http://api.weather.com/v1/current?city={city}")

        if response.status_code == 200:
            span.set_attribute("weather.status", "success")
            return response.json()
        else:
            span.set_attribute("weather.status", "error")
            span.set_attribute("http.status_code", response.status_code)
            return {"error": "Weather data not available"}, 500

if __name__ == '__main__':
    app.run(debug=True)
```

#### Analyse des traces dans Jaeger

L'interface Jaeger (http://localhost:16686) permet de :

1. **Rechercher des traces** :
   - Par service
   - Par op√©ration
   - Par tags
   - Par dur√©e

2. **Analyser les performances** :
   - Timeline des spans
   - Goulots d'√©tranglement
   - Erreurs et exceptions

3. **Visualiser les d√©pendances** :
   - Graphique des services
   - Flux de donn√©es
   - Points de d√©faillance

#### Requ√™tes utiles dans Jaeger

```
# Recherche par tag
service=order-service operation=create_order

# Recherche par erreur
tag:"error=true"

# Recherche par utilisateur
tag:"user.id=12345"

# Recherche par dur√©e minimale
minDuration=1s

# Combinaison de crit√®res
service=payment-service tag:"http.status_code=500" minDuration=500ms
```

#### M√©triques personnalis√©es avec tracing

```python
# metrics_with_tracing.py
from opentelemetry import trace, metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.exporter.prometheus import PrometheusMetricReader
import time

# Configuration des m√©triques
metrics.set_meter_provider(MeterProvider(metric_readers=[PrometheusMetricReader()]))
meter = metrics.get_meter(__name__)

# Compteurs et histogrammes
request_counter = meter.create_counter(
    "http_requests_total",
    description="Total number of HTTP requests"
)

request_duration = meter.create_histogram(
    "http_request_duration_seconds",
    description="HTTP request duration in seconds"
)

def track_request(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()

        with tracer.start_as_current_span(func.__name__) as span:
            try:
                result = func(*args, **kwargs)
                status = "success"
                span.set_attribute("http.status", "200")
            except Exception as e:
                status = "error"
                span.set_attribute("http.status", "500")
                span.set_attribute("error.message", str(e))
                raise
            finally:
                duration = time.time() - start_time

                # Enregistrer les m√©triques
                request_counter.add(1, {"status": status, "endpoint": func.__name__})
                request_duration.record(duration, {"status": status})

                # Ajouter la dur√©e au span
                span.set_attribute("duration_ms", duration * 1000)

        return result
    return wrapper

@track_request
def process_order(order_id):
    # Logique m√©tier
    time.sleep(0.1)  # Simulation
    return {"order_id": order_id, "status": "processed"}
```

---

## 11.3.4 ELK Stack sur Debian

### Qu'est-ce que ELK Stack ?

**ELK Stack** est un ensemble d'outils pour la gestion centralis√©e des logs :

- **Elasticsearch** : Moteur de recherche et d'analyse
- **Logstash** : Pipeline de traitement des donn√©es
- **Kibana** : Interface de visualisation et d'analyse

Souvent compl√©t√© par :
- **Beats** : Collecteurs l√©gers (Filebeat, Metricbeat, etc.)
- **Fluent Bit** : Alternative l√©g√®re √† Logstash

### Architecture ELK

```
Applications ‚Üí Beats/Fluentd ‚Üí Logstash ‚Üí Elasticsearch ‚Üí Kibana
```

### Installation d'Elasticsearch

#### Pr√©requis syst√®me

```bash
# Configuration du syst√®me pour Elasticsearch
echo 'vm.max_map_count=262144' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# Installation de Java (requis pour Elasticsearch)
sudo apt update
sudo apt install openjdk-11-jdk
```

#### Installation via les d√©p√¥ts officiels

```bash
# Ajout de la cl√© GPG et du d√©p√¥t Elastic
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list

# Installation d'Elasticsearch
sudo apt update
sudo apt install elasticsearch

# Configuration de base
sudo nano /etc/elasticsearch/elasticsearch.yml
```

#### Configuration Elasticsearch

```yaml
# /etc/elasticsearch/elasticsearch.yml
cluster.name: monitoring-cluster
node.name: elasticsearch-node-1
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 0.0.0.0
http.port: 9200
discovery.type: single-node

# S√©curit√© (Elasticsearch 8.x)
xpack.security.enabled: false  # Pour simplifier l'installation
xpack.security.enrollment.enabled: false
xpack.security.http.ssl.enabled: false
xpack.security.transport.ssl.enabled: false

# Performance
bootstrap.memory_lock: true
```

#### Configuration JVM

```bash
# /etc/elasticsearch/jvm.options.d/heap.options
# Allouer 50% de la RAM disponible, max 32GB
-Xms2g
-Xmx2g
```

#### D√©marrage d'Elasticsearch

```bash
# Activation et d√©marrage
sudo systemctl daemon-reload
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch

# V√©rification
curl -X GET "localhost:9200/"
sudo systemctl status elasticsearch
```

### Installation de Kibana

```bash
# Installation
sudo apt install kibana

# Configuration
sudo nano /etc/kibana/kibana.yml
```

#### Configuration Kibana

```yaml
# /etc/kibana/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
server.name: "kibana-monitoring"
elasticsearch.hosts: ["http://localhost:9200"]

# Logs et monitoring
logging.appenders.file.type: file
logging.appenders.file.fileName: /var/log/kibana/kibana.log
logging.appenders.file.layout.type: json

# Interface
i18n.locale: "fr"  # Interface en fran√ßais
```

#### D√©marrage de Kibana

```bash
# D√©marrage
sudo systemctl enable kibana
sudo systemctl start kibana

# V√©rification (peut prendre quelques minutes)
curl -X GET "localhost:5601/api/status"
sudo systemctl status kibana
```

### Installation de Logstash

```bash
# Installation
sudo apt install logstash

# Cr√©ation du r√©pertoire de configuration
sudo mkdir -p /etc/logstash/conf.d
```

#### Configuration Logstash

```ruby
# /etc/logstash/conf.d/01-input.conf
input {
  beats {
    port => 5044
  }

  # Logs syst√®mes via syslog
  syslog {
    port => 5514
  }

  # Logs d'applications via TCP
  tcp {
    port => 5000
    codec => json_lines
  }
}

# /etc/logstash/conf.d/02-filter.conf
filter {
  # Filtres pour logs Apache/Nginx
  if [fields][service] == "apache" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }

    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }

    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
    }
  }

  # Filtres pour logs d'applications JSON
  if [fields][service] == "application" {
    json {
      source => "message"
    }

    if [level] == "ERROR" {
      mutate {
        add_tag => [ "error" ]
      }
    }
  }

  # Enrichissement g√©ographique
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }
}

# /etc/logstash/conf.d/03-output.conf
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{[fields][service]}-%{+YYYY.MM.dd}"
  }

  # Sortie conditionnelle pour les erreurs
  if "error" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "errors-%{+YYYY.MM.dd}"
    }
  }

  # Debug (optionnel)
  stdout {
    codec => rubydebug
  }
}
```

#### Pipeline de configuration

```yaml
# /etc/logstash/pipelines.yml
- pipeline.id: main
  path.config: "/etc/logstash/conf.d/*.conf"
  pipeline.workers: 2
  pipeline.batch.size: 125
  pipeline.batch.delay: 50
```

#### D√©marrage de Logstash

```bash
# Test de la configuration
sudo -u logstash /usr/share/logstash/bin/logstash --path.settings /etc/logstash -t

# D√©marrage
sudo systemctl enable logstash
sudo systemctl start logstash
sudo systemctl status logstash
```

### Installation et configuration de Filebeat

Filebeat collecte et envoie les logs vers Logstash :

```bash
# Installation
sudo apt install filebeat

# Configuration
sudo nano /etc/filebeat/filebeat.yml
```

#### Configuration Filebeat

```yaml
# /etc/filebeat/filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/apache2/*.log
    - /var/log/nginx/*.log
  fields:
    service: apache
  fields_under_root: true
  multiline.pattern: '^\d{4}-\d{2}-\d{2}'
  multiline.negate: true
  multiline.match: after

- type: log
  enabled: true
  paths:
    - /var/log/myapp/*.log
  fields:
    service: application
  fields_under_root: true
  json.keys_under_root: true
  json.add_error_key: true

# Configuration syst√®me
- type: system
  enabled: true

processors:
- add_host_metadata:
    when.not.contains.tags: forwarded

# Sortie vers Logstash
output.logstash:
  hosts: ["localhost:5044"]

# Logging
logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

#### D√©marrage de Filebeat

```bash
# Activation et d√©marrage
sudo systemctl enable filebeat
sudo systemctl start filebeat
sudo systemctl status filebeat

# V√©rification des logs
sudo tail -f /var/log/filebeat/filebeat
```

### Configuration de Kibana pour l'analyse

#### Cr√©ation d'index patterns

```bash
# Via API (automatique)
curl -X POST "localhost:5601/api/saved_objects/index-pattern/logs-*" \
  -H "Content-Type: application/json" \
  -H "kbn-xsrf: true" \
  -d '{
    "attributes": {
      "title": "logs-*",
      "timeFieldName": "@timestamp"
    }
  }'
```

#### Dashboard pour logs d'applications

```json
{
  "version": "8.0.0",
  "objects": [
    {
      "id": "app-logs-dashboard",
      "type": "dashboard",
      "attributes": {
        "title": "Application Logs Dashboard",
        "panelsJSON": "[{\"version\":\"8.0.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15},\"panelIndex\":\"1\",\"embeddableConfig\":{},\"panelRefName\":\"panel_1\"}]",
        "optionsJSON": "{\"useMargins\":true,\"syncColors\":false,\"hidePanelTitles\":false}",
        "timeRestore": false,
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}"
        }
      }
    },
    {
      "id": "logs-timeline",
      "type": "visualization",
      "attributes": {
        "title": "Logs Timeline",
        "visState": "{\"title\":\"Logs Timeline\",\"type\":\"histogram\",\"params\":{\"grid\":{\"categoryLines\":false,\"style\":{\"color\":\"#eee\"}},\"categoryAxes\":[{\"id\":\"CategoryAxis-1\",\"type\":\"category\",\"position\":\"bottom\",\"show\":true,\"style\":{},\"scale\":{\"type\":\"linear\"},\"labels\":{\"show\":true,\"truncate\":100},\"title\":{}}],\"valueAxes\":[{\"id\":\"ValueAxis-1\",\"name\":\"LeftAxis-1\",\"type\":\"value\",\"position\":\"left\",\"show\":true,\"style\":{},\"scale\":{\"type\":\"linear\",\"mode\":\"normal\"},\"labels\":{\"show\":true,\"rotate\":0,\"filter\":false,\"truncate\":100},\"title\":{\"text\":\"Count\"}}],\"seriesParams\":[{\"show\":true,\"type\":\"histogram\",\"mode\":\"stacked\",\"data\":{\"label\":\"Count\",\"id\":\"1\"},\"valueAxis\":\"ValueAxis-1\",\"drawLinesBetweenPoints\":true,\"showCircles\":true}],\"addTooltip\":true,\"addLegend\":true,\"legendPosition\":\"right\",\"times\":[],\"addTimeMarker\":false},\"aggs\":[{\"id\":\"1\",\"enabled\":true,\"type\":\"count\",\"schema\":\"metric\",\"params\":{}},{\"id\":\"2\",\"enabled\":true,\"type\":\"date_histogram\",\"schema\":\"segment\",\"params\":{\"field\":\"@timestamp\",\"interval\":\"auto\",\"customInterval\":\"2h\",\"min_doc_count\":1,\"extended_bounds\":{}}}]}",
        "uiStateJSON": "{}",
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"index\":\"logs-*\",\"query\":{\"match_all\":{}},\"filter\":[]}"
        }
      }
    }
  ]
}
```

### Monitoring et maintenance ELK

#### Scripts de maintenance

```bash
#!/bin/bash
# elk-maintenance.sh

# Nettoyage des anciens indices
curl -X DELETE "localhost:9200/logs-*-$(date -d '7 days ago' +%Y.%m.%d)"

# Optimisation des indices
curl -X POST "localhost:9200/_forcemerge?max_num_segments=1"

# V√©rification de l'√©tat du cluster
curl -X GET "localhost:9200/_cluster/health?pretty"

# Statistiques des indices
curl -X GET "localhost:9200/_cat/indices?v&s=index"

# V√©rification de l'espace disque
df -h /var/lib/elasticsearch
```

#### Configuration des alertes Kibana

```json
{
  "name": "High Error Rate Alert",
  "consumer": "alerts",
  "enabled": true,
  "alertTypeId": ".index-threshold",
  "schedule": {
    "interval": "1m"
  },
  "params": {
    "index": ["logs-*"],
    "timeField": "@timestamp",
    "aggType": "count",
    "termSize": 5,
    "termField": "level.keyword",
    "thresholdComparator": ">",
    "threshold": [10],
    "timeWindowSize": 5,
    "timeWindowUnit": "m",
    "filterQuery": {
      "bool": {
        "must": [
          {
            "term": {
              "level.keyword": "ERROR"
            }
          }
        ]
      }
    }
  },
  "actions": [
    {
      "id": "email-action",
      "group": "threshold met",
      "params": {
        "to": ["admin@example.com"],
        "subject": "High Error Rate Detected",
        "message": "Error rate exceeded threshold: {{context.value}} errors in {{params.timeWindowSize}}{{params.timeWindowUnit}}"
      }
    }
  ]
}
```

### Int√©gration avec Prometheus

#### Exporteur Elasticsearch pour Prometheus

```bash
# Installation de elasticsearch_exporter
wget https://github.com/prometheus-community/elasticsearch_exporter/releases/download/v1.6.0/elasticsearch_exporter-1.6.0.linux-amd64.tar.gz
tar xzf elasticsearch_exporter-1.6.0.linux-amd64.tar.gz
sudo cp elasticsearch_exporter-1.6.0.linux-amd64/elasticsearch_exporter /usr/local/bin/

# Service systemd
sudo tee /etc/systemd/system/elasticsearch_exporter.service > /dev/null <<EOF
[Unit]
Description=Elasticsearch Exporter
After=network.target

[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/bin/elasticsearch_exporter --es.uri=http://localhost:9200
Restart=always

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable elasticsearch_exporter
sudo systemctl start elasticsearch_exporter
```

#### Configuration Prometheus pour ELK

```yaml
# Ajout dans prometheus.yml
scrape_configs:
  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['localhost:9114']
    scrape_interval: 30s
    metrics_path: /metrics

  - job_name: 'logstash'
    static_configs:
      - targets: ['localhost:9600']
    scrape_interval: 30s
    metrics_path: /metrics

  - job_name: 'kibana'
    static_configs:
      - targets: ['localhost:5601']
    scrape_interval: 30s
    metrics_path: /api/status
```

### Optimisation et bonnes pratiques

#### Performance Elasticsearch

```yaml
# Configuration optimis√©e pour production
# /etc/elasticsearch/elasticsearch.yml

# Allocation des shards
index.number_of_shards: 1
index.number_of_replicas: 1
index.refresh_interval: 30s

# Cache et m√©moire
indices.memory.index_buffer_size: 10%
indices.fielddata.cache.size: 40%

# Stockage
index.store.type: niofs
index.merge.policy.max_merged_segment: 5gb

# Logs et monitoring
logger.org.elasticsearch.discovery: DEBUG
logger.org.elasticsearch.index.search.slowlog.query: INFO, index_search_slow_log_file
logger.org.elasticsearch.index.indexing.slowlog.index: INFO, index_indexing_slow_log_file
```

#### Rotation des logs

```bash
# /etc/logrotate.d/elk-logs
/var/log/elasticsearch/*.log {
    daily
    missingok
    rotate 52
    compress
    notifempty
    create 644 elasticsearch elasticsearch
    postrotate
        /bin/kill -USR1 `cat /var/run/elasticsearch/elasticsearch.pid 2> /dev/null` 2> /dev/null || true
    endscript
}

/var/log/kibana/*.log {
    daily
    missingok
    rotate 30
    compress
    notifempty
    create 644 kibana kibana
}

/var/log/logstash/*.log {
    daily
    missingok
    rotate 30
    compress
    notifempty
    create 644 logstash logstash
}
```

### Surveillance et alerting

#### M√©triques importantes √† surveiller

1. **Elasticsearch** :
   - Utilisation CPU et m√©moire
   - Espace disque
   - Nombre de shards et r√©plicas
   - Taux d'indexation et de recherche

2. **Logstash** :
   - D√©bit de traitement
   - Taille des files d'attente
   - Erreurs de parsing
   - Latence de traitement

3. **Kibana** :
   - Temps de r√©ponse
   - Nombre d'utilisateurs connect√©s
   - Erreurs d'interface

#### Dashboard Grafana pour ELK

```json
{
  "dashboard": {
    "title": "ELK Stack Monitoring",
    "panels": [
      {
        "title": "Elasticsearch Cluster Health",
        "type": "stat",
        "targets": [
          {
            "expr": "elasticsearch_cluster_health_status",
            "legendFormat": "Status"
          }
        ]
      },
      {
        "title": "Index Rate",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(elasticsearch_indices_indexing_index_total[5m])",
            "legendFormat": "{{index}}"
          }
        ]
      },
      {
        "title": "Search Rate",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(elasticsearch_indices_search_query_total[5m])",
            "legendFormat": "{{index}}"
          }
        ]
      }
    ]
  }
}
```

---

Cette section compl√®te votre compr√©hension du monitoring cloud-native avec les outils essentiels pour une observabilit√© compl√®te : m√©triques (Prometheus), visualisation (Grafana), tracing (Jaeger), et logging (ELK Stack). Dans la section suivante, nous explorerons les solutions de stockage distribu√© qui compl√®tent l'infrastructure cloud-native moderne.

‚è≠Ô∏è
