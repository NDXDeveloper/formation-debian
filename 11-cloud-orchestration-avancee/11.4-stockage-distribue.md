üîù Retour au [Sommaire](/SOMMAIRE.md)

# 11.4 Stockage distribu√©

## Introduction au stockage distribu√©

### Qu'est-ce que le stockage distribu√© ?

Le **stockage distribu√©** est une technologie qui r√©partit les donn√©es sur plusieurs serveurs physiques, cr√©ant un syst√®me de stockage unifi√© et redondant. Contrairement au stockage traditionnel o√π vos donn√©es sont sur un seul disque dur ou serveur, le stockage distribu√© garantit :

- **Haute disponibilit√©** : Si un serveur tombe en panne, vos donn√©es restent accessibles
- **Scalabilit√©** : Ajoutez simplement des serveurs pour augmenter la capacit√©
- **Performance** : Les lectures/√©critures se font en parall√®le sur plusieurs machines
- **R√©silience** : Les donn√©es sont automatiquement r√©pliqu√©es

### Pourquoi le stockage distribu√© pour le cloud-native ?

Dans les architectures cloud-native, les applications sont distribu√©es et dynamiques :

1. **Conteneurs √©ph√©m√®res** : Les conteneurs apparaissent et disparaissent, mais les donn√©es doivent persister
2. **Scalabilit√© automatique** : Le stockage doit s'adapter automatiquement √† la demande
3. **Multi-cloud** : Les donn√©es doivent √™tre accessibles depuis diff√©rents clouds
4. **Performance** : Les applications modernes n√©cessitent des acc√®s rapides aux donn√©es

### Types de stockage distribu√©

1. **Stockage objet** : Comme Amazon S3, pour les fichiers non structur√©s
2. **Stockage bloc** : Pour les disques virtuels des machines virtuelles
3. **Stockage fichier** : Syst√®mes de fichiers partag√©s (NFS distribu√©)
4. **Stockage unifi√©** : Solutions qui combinent tous les types

### Les d√©fis du stockage distribu√©

- **Consistance** : Garantir que tous les n≈ìuds ont la m√™me version des donn√©es
- **Partition tolerance** : Continuer √† fonctionner m√™me si des n≈ìuds sont isol√©s
- **Performance** : Maintenir de bonnes performances malgr√© la distribution
- **Complexit√©** : G√©rer un syst√®me distribu√© est plus complexe qu'un stockage centralis√©

---

## 11.4.1 Ceph sur Debian

### Qu'est-ce que Ceph ?

**Ceph** est une solution de stockage distribu√© open-source qui offre trois types de stockage dans une seule plateforme :

- **Ceph Object Storage** (RADOS Gateway) : Compatible S3 et Swift
- **Ceph Block Device** (RBD) : Disques virtuels pour VMs et conteneurs
- **Ceph File System** (CephFS) : Syst√®me de fichiers distribu√© POSIX

### Architecture de Ceph

Ceph utilise plusieurs composants :

- **MON (Monitor)** : Maintient la carte du cluster et l'√©tat
- **OSD (Object Storage Daemon)** : Stocke les donn√©es sur les disques
- **MDS (Metadata Server)** : G√®re les m√©tadonn√©es pour CephFS
- **MGR (Manager)** : Fournit des services de gestion et monitoring

### Installation de Ceph sur Debian

#### Pr√©requis syst√®me

```bash
# Mise √† jour du syst√®me
sudo apt update && sudo apt upgrade -y

# Installation des d√©pendances
sudo apt install -y python3 python3-pip curl wget gnupg2
sudo apt install -y chrony  # Synchronisation du temps (critique pour Ceph)

# Configuration de chrony
sudo systemctl enable chrony
sudo systemctl start chrony

# V√©rification de la synchronisation
chrony sources -v
```

#### Installation via cephadm

Cephadm est l'outil officiel pour d√©ployer Ceph moderne :

```bash
# T√©l√©chargement de cephadm
curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm
chmod +x cephadm

# Installation de cephadm
sudo ./cephadm install

# V√©rification
cephadm version
```

#### Bootstrap du cluster

```bash
# Initialisation du premier n≈ìud
sudo cephadm bootstrap --mon-ip 192.168.1.10

# Cette commande va :
# 1. Cr√©er un cluster Ceph minimal
# 2. Installer les containers n√©cessaires
# 3. G√©n√©rer les cl√©s d'authentification
# 4. D√©marrer les services de base
```

#### Configuration apr√®s bootstrap

```bash
# Installation du CLI Ceph
sudo cephadm install ceph-common

# V√©rification de l'√©tat du cluster
sudo ceph status
sudo ceph health

# Affichage de la configuration
sudo ceph config dump
```

### Ajout de n≈ìuds au cluster

#### Pr√©paration des n≈ìuds suppl√©mentaires

```bash
# Sur chaque nouveau n≈ìud
sudo apt update
sudo apt install -y chrony python3

# Synchronisation du temps
sudo systemctl enable chrony
sudo systemctl start chrony

# Installation de cephadm
curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm
chmod +x cephadm
sudo ./cephadm install
```

#### Ajout depuis le n≈ìud principal

```bash
# Copie de la cl√© SSH publique vers les nouveaux n≈ìuds
ssh-copy-id root@node2
ssh-copy-id root@node3

# Ajout des n≈ìuds au cluster
sudo ceph orch host add node2 192.168.1.11
sudo ceph orch host add node3 192.168.1.12

# V√©rification
sudo ceph orch host ls
```

### Configuration du stockage

#### Ajout d'OSDs (Object Storage Daemons)

```bash
# Lister les disques disponibles
sudo ceph orch device ls

# Ajouter tous les disques disponibles automatiquement
sudo ceph orch apply osd --all-available-devices

# Ou ajouter des disques sp√©cifiques
sudo ceph orch daemon add osd node1:/dev/sdb
sudo ceph orch daemon add osd node2:/dev/sdc

# V√©rification des OSDs
sudo ceph osd status
sudo ceph osd tree
```

#### Configuration des pools

```bash
# Cr√©er un pool pour le stockage bloc (RBD)
sudo ceph osd pool create rbd-pool 32 32

# Activer l'application RBD sur le pool
sudo ceph osd pool application enable rbd-pool rbd

# Cr√©er un pool pour le stockage objet
sudo ceph osd pool create object-pool 32 32
sudo ceph osd pool application enable object-pool rgw

# V√©rification
sudo ceph osd pool ls
```

### Configuration de Ceph Block Device (RBD)

#### Cr√©ation et utilisation d'images RBD

```bash
# Cr√©er une image RBD de 10GB
sudo rbd create --size 10G rbd-pool/disk1

# Lister les images
sudo rbd ls rbd-pool

# Informations sur l'image
sudo rbd info rbd-pool/disk1

# Mapper l'image sur le syst√®me local
sudo rbd map rbd-pool/disk1

# Formatter et monter
sudo mkfs.ext4 /dev/rbd0
sudo mkdir /mnt/ceph-disk
sudo mount /dev/rbd0 /mnt/ceph-disk

# V√©rification
df -h /mnt/ceph-disk
```

#### Configuration pour Kubernetes

```yaml
# ceph-csi-rbd.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: <cluster-id>
  pool: rbd-pool
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
  - discard
```

### Configuration de RADOS Gateway (S3)

#### Installation du gateway

```bash
# D√©ploiement du service RGW
sudo ceph orch apply rgw default --placement="2 node1 node2" --port=8080

# V√©rification du d√©ploiement
sudo ceph orch ps --service-name rgw.default
```

#### Cr√©ation d'utilisateurs S3

```bash
# Cr√©er un utilisateur S3
sudo radosgw-admin user create --uid=s3user --display-name="S3 User" --access-key=AKIAEXAMPLE --secret-key=secretkeyexample

# Lister les utilisateurs
sudo radosgw-admin user list

# Informations d'un utilisateur
sudo radosgw-admin user info --uid=s3user
```

#### Test de l'API S3

```bash
# Installation du client S3
sudo apt install -y s3cmd

# Configuration
s3cmd --configure
# Entrer :
# Access Key: AKIAEXAMPLE
# Secret Key: secretkeyexample
# Default Region: us-east-1
# S3 Endpoint: http://node1:8080
# DNS-style bucket: No

# Test de cr√©ation de bucket
s3cmd mb s3://test-bucket

# Upload d'un fichier
echo "Hello Ceph" > test.txt
s3cmd put test.txt s3://test-bucket/

# Listing
s3cmd ls s3://test-bucket/
```

---

## 11.4.2 GlusterFS

### Qu'est-ce que GlusterFS ?

**GlusterFS** est un syst√®me de fichiers distribu√© scale-out qui agr√®ge le stockage de plusieurs serveurs en un seul namespace. Il est particuli√®rement adapt√© pour :

- **Stockage de fichiers** : Remplacement de NFS traditionnel
- **Haute disponibilit√©** : R√©plication automatique des donn√©es
- **Simplicit√©** : Installation et configuration relativement simples
- **Flexibilit√©** : Diff√©rents types de volumes selon les besoins

### Types de volumes GlusterFS

1. **Distributed** : Les fichiers sont r√©partis sur plusieurs briques
2. **Replicated** : Les fichiers sont dupliqu√©s sur plusieurs briques
3. **Striped** : Les fichiers sont d√©coup√©s en blocs r√©partis
4. **Distributed-Replicated** : Combinaison de distribution et r√©plication

### Installation de GlusterFS sur Debian

#### Installation des paquets

```bash
# Mise √† jour et installation
sudo apt update
sudo apt install -y glusterfs-server

# D√©marrage et activation du service
sudo systemctl enable glusterd
sudo systemctl start glusterd
sudo systemctl status glusterd
```

#### Configuration du cluster

```bash
# Sur tous les n≈ìuds, ajouter les entr√©es dans /etc/hosts
sudo nano /etc/hosts
# Ajouter :
# 192.168.1.10 gluster1
# 192.168.1.11 gluster2
# 192.168.1.12 gluster3

# Sur le premier n≈ìud, ajouter les pairs
sudo gluster peer probe gluster2
sudo gluster peer probe gluster3

# V√©rification du cluster
sudo gluster peer status
sudo gluster pool list
```

### Cr√©ation de volumes

#### Pr√©paration du stockage

```bash
# Sur chaque n≈ìud, pr√©parer un disque d√©di√©
sudo mkfs.xfs /dev/sdb
sudo mkdir -p /gluster/brick1
sudo mount /dev/sdb /gluster/brick1

# Ajouter au fstab pour persistance
echo '/dev/sdb /gluster/brick1 xfs defaults 0 0' | sudo tee -a /etc/fstab

# Cr√©er le r√©pertoire de la brique
sudo mkdir -p /gluster/brick1/vol1
```

#### Volume distribu√©-r√©pliqu√©

```bash
# Cr√©er un volume avec r√©plication (3 copies)
sudo gluster volume create vol1 replica 3 \
    gluster1:/gluster/brick1/vol1 \
    gluster2:/gluster/brick1/vol1 \
    gluster3:/gluster/brick1/vol1 \
    force

# D√©marrer le volume
sudo gluster volume start vol1

# V√©rification
sudo gluster volume info vol1
sudo gluster volume status vol1
```

#### Volume distribu√© simple

```bash
# Pour de la performance sans r√©plication
sudo gluster volume create vol2 \
    gluster1:/gluster/brick2/vol2 \
    gluster2:/gluster/brick2/vol2 \
    gluster3:/gluster/brick2/vol2 \
    force

sudo gluster volume start vol2
```

### Montage et utilisation

#### Montage natif GlusterFS

```bash
# Installation du client
sudo apt install -y glusterfs-client

# Montage du volume
sudo mkdir /mnt/glusterfs
sudo mount -t glusterfs gluster1:/vol1 /mnt/glusterfs

# Test d'√©criture
echo "Test GlusterFS" | sudo tee /mnt/glusterfs/test.txt
cat /mnt/glusterfs/test.txt

# Montage automatique au d√©marrage
echo 'gluster1:/vol1 /mnt/glusterfs glusterfs defaults,_netdev 0 0' | sudo tee -a /etc/fstab
```

#### Montage NFS

```bash
# Activation du service NFS sur GlusterFS
sudo gluster volume set vol1 nfs.disable off

# Montage via NFS
sudo mount -t nfs -o vers=3 gluster1:/vol1 /mnt/nfs-gluster

# V√©rification
df -h /mnt/nfs-gluster
```

### Configuration pour Kubernetes

#### Installation du provisioner GlusterFS

```yaml
# glusterfs-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: glusterfs
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://gluster1:8080"
  restuser: "admin"
  secretNamespace: "default"
  secretName: "heketi-secret"
  volumetype: "replicate:3"
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

#### Configuration avec Heketi

Heketi est un service REST pour g√©rer GlusterFS :

```bash
# Installation de Heketi
wget https://github.com/heketi/heketi/releases/download/v10.4.0/heketi-v10.4.0.linux.amd64.tar.gz
tar xzf heketi-v10.4.0.linux.amd64.tar.gz
sudo cp heketi/heketi* /usr/local/bin/

# Configuration
sudo mkdir -p /etc/heketi /var/lib/heketi
```

```json
{
  "_port_comment": "Heketi Server Port Number",
  "port": "8080",

  "_use_auth": "Enable JWT authorization",
  "use_auth": false,

  "_jwt": "Private keys for access",
  "jwt": {
    "admin": {
      "key": "admin_secret"
    },
    "user": {
      "key": "user_secret"
    }
  },

  "_glusterfs_comment": "GlusterFS Configuration",
  "glusterfs": {
    "executor": "ssh",
    "sshexec": {
      "keyfile": "/etc/heketi/private_key",
      "user": "root"
    },
    "db": "/var/lib/heketi/heketi.db"
  }
}
```

### Monitoring et maintenance

#### Commandes de surveillance

```bash
# √âtat des volumes
sudo gluster volume status
sudo gluster volume info

# Statistiques de performance
sudo gluster volume profile vol1 start
sudo gluster volume profile vol1 info

# V√©rification de l'int√©grit√©
sudo gluster volume heal vol1 info
sudo gluster volume heal vol1 info summary

# Logs
sudo tail -f /var/log/glusterfs/glusterd.log
```

#### Op√©rations de maintenance

```bash
# Ajouter une brique √† un volume
sudo gluster volume add-brick vol1 gluster4:/gluster/brick1/vol1

# Retirer une brique (avec migration des donn√©es)
sudo gluster volume remove-brick vol1 gluster4:/gluster/brick1/vol1 start
sudo gluster volume remove-brick vol1 gluster4:/gluster/brick1/vol1 status
sudo gluster volume remove-brick vol1 gluster4:/gluster/brick1/vol1 commit

# Remplacer une brique d√©faillante
sudo gluster volume replace-brick vol1 \
    gluster2:/gluster/brick1/vol1 \
    gluster2:/gluster/brick1_new/vol1 \
    commit force

# R√©√©quilibrage apr√®s ajout/suppression
sudo gluster volume rebalance vol1 start
sudo gluster volume rebalance vol1 status
```

---

## 11.4.3 MinIO (S3 compatible)

### Qu'est-ce que MinIO ?

**MinIO** est un serveur de stockage objet open-source compatible avec l'API Amazon S3. Il est con√ßu pour √™tre :

- **Haute performance** : Optimis√© pour les workloads cloud-native
- **Cloud-native** : Int√©gration native avec Kubernetes
- **Compatible S3** : API 100% compatible avec Amazon S3
- **L√©ger** : Simple √† d√©ployer et maintenir
- **S√©curis√©** : Chiffrement, authentification, policies granulaires

### Cas d'usage pour MinIO

- **Backup et archivage** : Remplacement de solutions de backup traditionnelles
- **Data lakes** : Stockage pour analytics et machine learning
- **Content distribution** : CDN et stockage de m√©dias
- **Cloud hybride** : Interface S3 uniforme entre on-premise et cloud

### Installation de MinIO sur Debian

#### Installation en mode standalone

```bash
# T√©l√©chargement du binaire MinIO
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio
sudo mv minio /usr/local/bin/

# Cr√©ation de l'utilisateur minio
sudo useradd -r -s /sbin/nologin minio

# Cr√©ation des r√©pertoires
sudo mkdir -p /opt/minio/data
sudo mkdir -p /etc/minio
sudo chown minio:minio /opt/minio/data
sudo chown minio:minio /etc/minio
```

#### Configuration

```bash
# Configuration des variables d'environnement
sudo tee /etc/default/minio > /dev/null <<EOF
# Variables d'environnement MinIO
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
MINIO_VOLUMES="/opt/minio/data"
MINIO_OPTS="--console-address :9001"
EOF

# Service systemd
sudo tee /etc/systemd/system/minio.service > /dev/null <<EOF
[Unit]
Description=MinIO
Documentation=https://docs.min.io
Wants=network-online.target
After=network-online.target
AssertFileIsExecutable=/usr/local/bin/minio

[Service]
WorkingDirectory=/usr/local/
User=minio
Group=minio
EnvironmentFile=/etc/default/minio
ExecStartPre=/bin/bash -c "if [ -z \"\${MINIO_VOLUMES}\" ]; then echo \"Variable MINIO_VOLUMES not set in /etc/default/minio\"; exit 1; fi"
ExecStart=/usr/local/bin/minio server \$MINIO_OPTS \$MINIO_VOLUMES
Restart=always
LimitNOFILE=65536
TasksMax=infinity
TimeoutStopSec=infinity
SendSIGKILL=no

[Install]
WantedBy=multi-user.target
EOF

# D√©marrage du service
sudo systemctl daemon-reload
sudo systemctl enable minio
sudo systemctl start minio
sudo systemctl status minio
```

### Installation en mode distribu√©

#### Pr√©requis pour le mode distribu√©

```bash
# Sur tous les n≈ìuds (minimum 4), configuration des hosts
sudo nano /etc/hosts
# Ajouter :
# 192.168.1.10 minio1
# 192.168.1.11 minio2
# 192.168.1.12 minio3
# 192.168.1.13 minio4

# Pr√©parer les disques sur chaque n≈ìud
sudo mkfs.xfs /dev/sdb
sudo mkdir -p /mnt/disk1
sudo mount /dev/sdb /mnt/disk1
echo '/dev/sdb /mnt/disk1 xfs defaults 0 0' | sudo tee -a /etc/fstab
```

#### Configuration distribu√©e

```bash
# Configuration sur tous les n≈ìuds
sudo tee /etc/default/minio > /dev/null <<EOF
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
MINIO_VOLUMES="http://minio{1...4}/mnt/disk1"
MINIO_OPTS="--console-address :9001"
EOF

# M√™me service systemd sur tous les n≈ìuds
# D√©marrage synchronis√©
sudo systemctl start minio
```

### Configuration et utilisation

#### Installation du client mc

```bash
# T√©l√©chargement du client MinIO
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc
sudo mv mc /usr/local/bin/

# Configuration de l'alias
mc alias set local http://localhost:9000 minioadmin minioadmin123

# Test de connexion
mc admin info local
```

#### Op√©rations de base

```bash
# Cr√©er un bucket
mc mb local/test-bucket

# Lister les buckets
mc ls local

# Upload d'un fichier
echo "Hello MinIO" > test.txt
mc cp test.txt local/test-bucket/

# Download d'un fichier
mc cp local/test-bucket/test.txt downloaded.txt

# Synchronisation de r√©pertoires
mc mirror /local/path/ local/backup-bucket/ --watch

# Politique d'acc√®s public (lecture seule)
mc policy set download local/test-bucket
```

### Gestion des utilisateurs et politiques

#### Cr√©ation d'utilisateurs

```bash
# Cr√©er un utilisateur
mc admin user add local newuser newpassword123

# Cr√©er une politique personnalis√©e
cat > /tmp/readwrite-policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetBucketLocation",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::test-bucket"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::test-bucket/*"
      ]
    }
  ]
}
EOF

# Ajouter la politique
mc admin policy add local readwrite-policy /tmp/readwrite-policy.json

# Assigner la politique √† l'utilisateur
mc admin policy set local readwrite-policy user=newuser
```

#### Groupes et service accounts

```bash
# Cr√©er un groupe
mc admin group add local developers newuser

# Assigner une politique au groupe
mc admin policy set local readwrite-policy group=developers

# Cr√©er un service account
mc admin user svcacct add local newuser
```

### Int√©gration avec Kubernetes

#### Installation avec l'operateur MinIO

```bash
# Installation de l'op√©rateur MinIO
kubectl apply -k "github.com/minio/operator?ref=v4.4.16"

# V√©rification
kubectl get pods -n minio-operator
```

#### D√©ploiement d'un tenant MinIO

```yaml
# minio-tenant.yaml
apiVersion: minio.min.io/v2
kind: Tenant
metadata:
  name: storage
  namespace: default
spec:
  image: minio/minio:latest
  pools:
  - servers: 4
    volumesPerServer: 4
    volumeClaimTemplate:
      metadata:
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        storageClassName: fast-ssd
  users:
  - name: storage-user
  buckets:
  - name: data-bucket
    objectLock: false
    region: us-east-1
  requestAutoCert: false
```

#### Configuration de l'acc√®s externe

```yaml
# minio-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minio-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
spec:
  rules:
  - host: minio.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: storage-hl
            port:
              number: 9000
  - host: console.minio.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: storage-console
            port:
              number: 9090
```

### Monitoring et observabilit√©

#### M√©triques Prometheus

```bash
# MinIO expose automatiquement des m√©triques Prometheus
# Configuration Prometheus
cat >> prometheus.yml <<EOF
  - job_name: 'minio'
    static_configs:
      - targets: ['localhost:9000']
    metrics_path: /minio/v2/metrics/cluster
    scrape_interval: 30s
EOF
```

#### Dashboard Grafana

```json
{
  "dashboard": {
    "title": "MinIO Dashboard",
    "panels": [
      {
        "title": "Total Storage",
        "type": "stat",
        "targets": [
          {
            "expr": "minio_cluster_capacity_usable_total_bytes",
            "legendFormat": "Usable Storage"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        }
      },
      {
        "title": "Storage Used",
        "type": "stat",
        "targets": [
          {
            "expr": "minio_cluster_capacity_usable_free_bytes",
            "legendFormat": "Free Space"
          }
        ]
      },
      {
        "title": "Request Rate",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(minio_http_requests_duration_seconds_count[5m])",
            "legendFormat": "Requests/sec"
          }
        ]
      }
    ]
  }
}
```

### Sauvegarde et r√©plication

#### Configuration de la r√©plication cross-site

```bash
# Configurer la r√©plication vers un autre site MinIO
mc admin bucket remote add local/source-bucket \
    https://remote-minio.example.com/target-bucket \
    --service replication \
    --region us-west-1

# Activer la r√©plication
mc replicate add local/source-bucket \
    --remote-bucket target-bucket \
    --priority 1
```

#### Sauvegarde avec restic

```bash
# Installation de restic
sudo apt install restic

# Initialisation du repository sur MinIO
export RESTIC_REPOSITORY=s3:http://localhost:9000/backups
export RESTIC_PASSWORD=backup-password
export AWS_ACCESS_KEY_ID=minioadmin
export AWS_SECRET_ACCESS_KEY=minioadmin123

restic init

# Sauvegarde
restic backup /important/data

# Restauration
restic restore latest --target /restore/path
```

---

## 11.4.4 Rook pour Kubernetes

### Qu'est-ce que Rook ?

**Rook** est un op√©rateur Kubernetes qui simplifie le d√©ploiement et la gestion de syst√®mes de stockage distribu√© dans Kubernetes. Rook supporte plusieurs backend de stockage :

- **Ceph** : Stockage unifi√© (objet, bloc, fichier)
- **Cassandra** : Base de donn√©es NoSQL distribu√©e
- **CockroachDB** : Base de donn√©es SQL distribu√©e
- **EdgeFS** : Stockage g√©odistribu√©

### Avantages de Rook

- **Cloud-native** : Con√ßu sp√©cifiquement pour Kubernetes
- **Automatisation** : D√©ploiement et gestion automatis√©s
- **Self-healing** : R√©cup√©ration automatique des pannes
- **Scaling** : Ajustement automatique de la capacit√©
- **Multi-tenant** : Isolation des workloads

### Installation de Rook-Ceph

#### Pr√©requis

```bash
# V√©rifier que le cluster Kubernetes est fonctionnel
kubectl get nodes

# V√©rifier que les n≈ìuds ont des disques bruts disponibles
lsblk

# Sur chaque n≈ìud worker, installer les d√©pendances
sudo apt update
sudo apt install -y lvm2
```

#### Installation de l'op√©rateur Rook

```bash
# Cloner le repository Rook
git clone --single-branch --branch v1.12.0 https://github.com/rook/rook.git
cd rook/deploy/examples

# Appliquer les CRDs et l'op√©rateur
kubectl create -f crds.yaml
kubectl create -f common.yaml
kubectl create -f operator.yaml

# V√©rifier le d√©ploiement
kubectl -n rook-ceph get pods
```

### Configuration du cluster Ceph

#### Cr√©ation du cluster

```yaml
# cluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v17.2.6
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10

  mon:
    count: 3
    allowMultiplePerNode: false

  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
    - name: pg_autoscaler
      enabled: true
    - name: rook
      enabled: true

  dashboard:
    enabled: true
    ssl: true

  monitoring:
    enabled: false
    createPrometheusRules: false

  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false

  crashCollector:
    disable: false

  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M

  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false

  annotations:
  labels:

  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
      podAffinity:
      podAntiAffinity:
      topologySpreadConstraints:
      tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists

  storage:
    useAllNodes: true
    useAllDevices: true
    deviceFilter: "^sd[b-z]"
    config:
      crushRoot: "default"
      metadataDevice: ""
      databaseSizeMB: "1024"
      journalSizeMB: "1024"
      osdsPerDevice: "1"
    nodes:
    - name: "worker-node-1"
      devices:
      - name: "sdb"
      - name: "sdc"
    - name: "worker-node-2"
      devices:
      - name: "sdb"
      - name: "sdc"
    - name: "worker-node-3"
      devices:
      - name: "sdb"
      - name: "sdc"

  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
```

#### D√©ploiement du cluster

```bash
# Appliquer la configuration du cluster
kubectl apply -f cluster.yaml

# Surveiller le d√©ploiement
watch kubectl -n rook-ceph get pods

# V√©rifier l'√©tat du cluster Ceph
kubectl -n rook-ceph get cephcluster
kubectl -n rook-ceph describe cephcluster rook-ceph
```

#### Acc√®s au dashboard Ceph

```bash
# Obtenir le mot de passe admin du dashboard
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo

# Port forwarding pour acc√©der au dashboard
kubectl -n rook-ceph port-forward service/rook-ceph-mgr-dashboard 7000:7000

# Le dashboard est accessible sur https://localhost:7000
# Utilisateur : admin
# Mot de passe : celui obtenu ci-dessus
```

### Configuration du stockage bloc (RBD)

#### Cr√©ation d'un pool et StorageClass

```yaml
# ceph-block-pool.yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
    requireSafeReplicaSize: true
  parameters:
    pg_num: "128"
    pgp_num: "128"
    crush_rule: "replicated_rule"
  mirroring:
    enabled: false
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
  csi.storage.k8s.io/fstype: ext4
allowVolumeExpansion: true
reclaimPolicy: Delete
```

#### Test du stockage bloc

```yaml
# test-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  storageClassName: rook-ceph-block
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test
    image: nginx:latest
    volumeMounts:
    - name: test-volume
      mountPath: /data
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: test-pvc
```

```bash
# D√©ployer le test
kubectl apply -f ceph-block-pool.yaml
kubectl apply -f test-pvc.yaml

# V√©rifier le PVC
kubectl get pvc test-pvc
kubectl describe pvc test-pvc

# Tester l'√©criture
kubectl exec test-pod -- bash -c "echo 'Hello Rook-Ceph' > /data/test.txt"
kubectl exec test-pod -- cat /data/test.txt
```

### Configuration du stockage objet (S3)

#### D√©ploiement de l'Object Store

```yaml
# object-store.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: my-store
  namespace: rook-ceph
spec:
  metadataPool:
    failureDomain: host
    replicated:
      size: 3
  dataPool:
    failureDomain: host
    erasureCoded:
      dataChunks: 2
      codingChunks: 1
  preservePoolsOnDelete: true
  gateway:
    type: s3
    sslCertificateRef:
    port: 80
    securePort: 443
    instances: 2
    priorityClassName: system-cluster-critical
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
      tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
  healthCheck:
    bucket:
      disabled: false
      interval: 60s
```

#### Configuration de l'acc√®s S3

```yaml
# object-user.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: my-user
  namespace: rook-ceph
spec:
  store: my-store
  displayName: "my display name"
  capabilities:
    user: "*"
    bucket: "*"
    metadata: "*"
    usage: "*"
    zone: "*"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-bucket
provisioner: rook-ceph.ceph.rook.io/bucket
reclaimPolicy: Delete
parameters:
  objectStoreName: my-store
  objectStoreNamespace: rook-ceph
```

#### Test de l'API S3

```bash
# Obtenir les credentials S3
kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o jsonpath='{.data.AccessKey}' | base64 --decode
kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o jsonpath='{.data.SecretKey}' | base64 --decode

# Obtenir l'endpoint S3
kubectl -n rook-ceph get service rook-ceph-rgw-my-store

# Configuration du client s3cmd
s3cmd --configure
# Endpoint: http://<service-ip>:80
# Access Key: <access-key-from-above>
# Secret Key: <secret-key-from-above>

# Test des op√©rations S3
s3cmd mb s3://test-bucket
s3cmd ls
echo "Hello Rook S3" > test.txt
s3cmd put test.txt s3://test-bucket/
s3cmd ls s3://test-bucket/
```

### Configuration du syst√®me de fichiers (CephFS)

#### D√©ploiement de CephFS

```yaml
# filesystem.yaml
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    failureDomain: host
    replicated:
      size: 3
  dataPools:
  - name: replicated
    failureDomain: host
    replicated:
      size: 3
  preserveFilesystemOnDelete: true
  metadataServer:
    activeCount: 1
    activeStandby: true
    priorityClassName: system-cluster-critical
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
      tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
    resources:
      limits:
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "4Gi"
    livenessProbe:
      disabled: false
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs
  pool: myfs-replicated
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
allowVolumeExpansion: true
reclaimPolicy: Delete
```

#### Test de CephFS

```yaml
# test-cephfs.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc
spec:
  storageClassName: rook-cephfs
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cephfs-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cephfs-test
  template:
    metadata:
      labels:
        app: cephfs-test
    spec:
      containers:
      - name: test
        image: nginx:latest
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo $(hostname) $(date) >> /shared/log.txt; sleep 10; done"]
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: cephfs-pvc
```

```bash
# D√©ployer CephFS
kubectl apply -f filesystem.yaml
kubectl apply -f test-cephfs.yaml

# V√©rifier que tous les pods √©crivent dans le m√™me fichier
kubectl exec -it deployment/cephfs-test -- tail -f /shared/log.txt
```

### Monitoring et observabilit√©

#### Configuration de Prometheus

```yaml
# prometheus-service-monitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: rook-ceph-mgr
  namespace: rook-ceph
  labels:
    team: rook
spec:
  namespaceSelector:
    matchNames:
    - rook-ceph
  selector:
    matchLabels:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
  endpoints:
  - port: http-metrics
    path: /metrics
    interval: 5s
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-ceph-rules
  namespace: rook-ceph
  labels:
    prometheus: rook-prometheus
    role: alert-rules
spec:
  groups:
  - name: ceph.rules
    rules:
    - alert: CephHealthError
      expr: ceph_health_status == 2
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Ceph cluster health is in ERROR state"
        description: "Ceph cluster {{ $labels.cluster }} health is in ERROR state for more than 5 minutes."

    - alert: CephOSDDown
      expr: ceph_osd_up == 0
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "Ceph OSD is down"
        description: "OSD {{ $labels.osd }} on cluster {{ $labels.cluster }} is down."

    - alert: CephPoolNearFull
      expr: ceph_pool_percent_used > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Ceph pool is nearly full"
        description: "Pool {{ $labels.name }} on cluster {{ $labels.cluster }} is {{ $value }}% full."
```

#### Dashboard Grafana pour Rook-Ceph

```json
{
  "dashboard": {
    "title": "Rook-Ceph Dashboard",
    "panels": [
      {
        "title": "Cluster Health",
        "type": "stat",
        "targets": [
          {
            "expr": "ceph_health_status",
            "legendFormat": "Health Status"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "mappings": [
              {"options": {"0": {"text": "OK", "color": "green"}}},
              {"options": {"1": {"text": "WARN", "color": "yellow"}}},
              {"options": {"2": {"text": "ERROR", "color": "red"}}}
            ]
          }
        }
      },
      {
        "title": "Total Storage",
        "type": "stat",
        "targets": [
          {
            "expr": "ceph_cluster_total_bytes",
            "legendFormat": "Total"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        }
      },
      {
        "title": "Storage Usage",
        "type": "piechart",
        "targets": [
          {
            "expr": "ceph_cluster_total_used_raw_bytes",
            "legendFormat": "Used"
          },
          {
            "expr": "ceph_cluster_total_bytes - ceph_cluster_total_used_raw_bytes",
            "legendFormat": "Free"
          }
        ]
      },
      {
        "title": "IOPS",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(ceph_osd_op_r[5m])",
            "legendFormat": "Read IOPS"
          },
          {
            "expr": "rate(ceph_osd_op_w[5m])",
            "legendFormat": "Write IOPS"
          }
        ]
      },
      {
        "title": "OSD Status",
        "type": "table",
        "targets": [
          {
            "expr": "ceph_osd_up",
            "legendFormat": "OSD {{osd}}"
          }
        ]
      }
    ]
  }
}
```

### Maintenance et op√©rations

#### Ajout de n≈ìuds et OSDs

```bash
# Ajouter un nouveau n≈ìud au cluster
kubectl label node new-worker-node storage-node=true

# Modifier la configuration du cluster pour inclure le nouveau n≈ìud
kubectl -n rook-ceph edit cephcluster rook-ceph

# Ajouter une section pour le nouveau n≈ìud dans spec.storage.nodes
# Le cluster d√©tectera automatiquement les nouveaux disques
```

#### Gestion des pannes

```bash
# V√©rifier l'√©tat du cluster
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- ceph status

# Identifier les OSDs d√©faillants
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- ceph osd tree

# Marquer un OSD comme "out" (pour maintenance)
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- ceph osd out osd.1

# Supprimer un OSD d√©faillant
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- ceph osd rm osd.1
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- ceph auth del osd.1
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- ceph osd crush rm osd.1
```

#### Sauvegarde et r√©cup√©ration

```bash
# Cr√©er un snapshot d'une image RBD
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- rbd snap create replicapool/csi-vol-abc123@snapshot1

# Lister les snapshots
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- rbd snap ls replicapool/csi-vol-abc123

# Restaurer √† partir d'un snapshot
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- rbd snap rollback replicapool/csi-vol-abc123@snapshot1

# Exporter une image pour sauvegarde externe
kubectl -n rook-ceph exec deployment/rook-ceph-tools -- rbd export replicapool/csi-vol-abc123 /tmp/backup.img
```

### D√©sinstallation et nettoyage

#### Proc√©dure de d√©sinstallation

```bash
# Supprimer toutes les ressources utilisateur
kubectl delete storageclass rook-ceph-block rook-cephfs rook-ceph-bucket
kubectl delete -f test-pvc.yaml
kubectl delete -f test-cephfs.yaml

# Supprimer le cluster Ceph
kubectl -n rook-ceph delete cephcluster rook-ceph

# Attendre que tous les pods soient supprim√©s
kubectl -n rook-ceph wait --for=delete pod --all --timeout=600s

# Supprimer l'op√©rateur
kubectl delete -f operator.yaml
kubectl delete -f common.yaml
kubectl delete -f crds.yaml

# Nettoyage des donn√©es sur chaque n≈ìud
sudo rm -rf /var/lib/rook
```

#### Nettoyage des disques

```bash
# Sur chaque n≈ìud, nettoyer les disques utilis√©s par Ceph
# ATTENTION : Cette op√©ration d√©truit toutes les donn√©es !

# Identifier les disques Ceph
sudo lsblk

# Nettoyer les partitions Ceph
sudo sgdisk --zap-all /dev/sdb
sudo dd if=/dev/zero of=/dev/sdb bs=1M count=100 oflag=direct,dsync
sudo partprobe /dev/sdb

# Nettoyer les m√©tadonn√©es LVM
sudo dmsetup remove_all
sudo vgremove -f $(sudo vgs --noheadings -o vg_name | grep ceph)
sudo pvremove -f /dev/sdb
```

### Bonnes pratiques

#### Configuration de production

1. **Ressources** :
   - Allouer suffisamment de CPU et m√©moire aux pods Ceph
   - Utiliser des disques SSD pour les m√©tadonn√©es
   - S√©parer les disques OS des disques de donn√©es

2. **R√©seau** :
   - Utiliser un r√©seau d√©di√© pour le trafic Ceph
   - Configurer des VLANs s√©par√©s pour public/cluster networks
   - Optimiser les param√®tres r√©seau du kernel

3. **Monitoring** :
   - Surveiller les m√©triques de performance Ceph
   - Configurer des alertes pour les seuils critiques
   - Automatiser les rapports de sant√© du cluster

4. **S√©curit√©** :
   - Activer le chiffrement en transit et au repos
   - Utiliser des certificats TLS pour tous les services
   - Impl√©menter RBAC granulaire pour l'acc√®s aux pools

#### Optimisation des performances

```yaml
# Exemple de configuration optimis√©e pour la production
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph-prod
  namespace: rook-ceph
spec:
  # Configuration r√©seau optimis√©e
  network:
    provider: host
    connections:
      encryption:
        enabled: true
      compression:
        enabled: true

  # Ressources pour les d√©mons Ceph
  resources:
    mgr:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
    mon:
      limits:
        cpu: "2000m"
        memory: "2Gi"
      requests:
        cpu: "1000m"
        memory: "1Gi"
    osd:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"

  # Configuration du stockage
  storage:
    config:
      osdsPerDevice: "1"
      databaseSizeMB: "4096"
      journalSizeMB: "4096"
      metadataDevice: "nvme0n1"  # SSD pour m√©tadonn√©es
```

---

Cette section compl√®te votre compr√©hension du stockage distribu√© dans les environnements cloud-native. Vous avez maintenant les connaissances pour d√©ployer et g√©rer des solutions de stockage robustes et scalables avec Ceph, GlusterFS, MinIO et Rook, adapt√©es aux besoins des applications modernes distribu√©es.

‚è≠Ô∏è
