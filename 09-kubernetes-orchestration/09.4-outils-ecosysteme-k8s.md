ðŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 9.4 Outils d'Ã©cosystÃ¨me K8s

*Module 9 - Kubernetes et orchestration | Niveau : AvancÃ©*

## Vue d'ensemble

L'Ã©cosystÃ¨me Kubernetes s'enrichit constamment d'outils qui simplifient et amÃ©liorent l'expÃ©rience de dÃ©veloppement et d'exploitation. Cette section explore les outils essentiels qui transforment Kubernetes d'une plateforme complexe en un environnement productif et observable.

**Pourquoi ces outils sont-ils essentiels ?**
- **ProductivitÃ©** : Automatisation des tÃ¢ches rÃ©pÃ©titives
- **Gestion simplifiÃ©e** : Abstraction de la complexitÃ©
- **ObservabilitÃ©** : VisibilitÃ© complÃ¨te sur les applications
- **Standardisation** : Bonnes pratiques intÃ©grÃ©es

### L'Ã©cosystÃ¨me CNCF

La Cloud Native Computing Foundation (CNCF) maintient un landscape d'outils organisÃ© par catÃ©gories :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Packaging     â”‚  â”‚   Management    â”‚  â”‚  Observability  â”‚
â”‚     (Helm)      â”‚  â”‚   (kubectl)     â”‚  â”‚ (Prometheus)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configuration   â”‚  â”‚    Security     â”‚  â”‚   Networking    â”‚
â”‚  (Kustomize)    â”‚  â”‚   (Falco)       â”‚  â”‚   (Istio)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Outils couverts dans cette section

| Outil | CatÃ©gorie | Fonction principale | Niveau |
|-------|-----------|-------------------|--------|
| **Helm** | Package Manager | DÃ©ploiement d'applications | Essentiel |
| **kubectl** | CLI | Gestion du cluster | Fondamental |
| **Kustomize** | Configuration | Gestion des variants | AvancÃ© |
| **Prometheus** | Monitoring | Collecte de mÃ©triques | Production |
| **Grafana** | Visualization | Dashboards et alertes | Production |

---

## 9.4.1 Helm (gestionnaire de paquets)

### Introduction Ã  Helm

**Helm** est le gestionnaire de paquets de Kubernetes, comparable Ã  `apt` pour Debian ou `npm` pour Node.js. Il simplifie l'installation, la mise Ã  jour et la gestion des applications complexes.

#### Concepts fondamentaux

**Chart (Graphique)**
- **Template d'application** : Ensemble de fichiers YAML paramÃ©trables
- **RÃ©utilisable** : Peut Ãªtre installÃ© plusieurs fois avec des configurations diffÃ©rentes
- **VersionnÃ©** : Gestion des versions et des dÃ©pendances

**Release (Livraison)**
- **Instance dÃ©ployÃ©e** : Une installation d'un chart dans un cluster
- **Nom unique** : Identifie l'installation dans un namespace
- **Historique** : Suivi des versions et rollback possible

**Repository (DÃ©pÃ´t)**
- **Collection de charts** : HÃ©bergement centralisÃ©
- **Public ou privÃ©** : Artifact Hub ou repos d'entreprise
- **VersionnÃ©** : Gestion des versions de charts

### Architecture Helm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Helm Client    â”‚â”€â”€â”€â–¶â”‚  Kubernetes     â”‚â”€â”€â”€â–¶â”‚   Application   â”‚
â”‚     (CLI)       â”‚    â”‚   API Server    â”‚    â”‚   Resources     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chart Repositoryâ”‚    â”‚  Release Store  â”‚
â”‚  (Artifact Hub) â”‚    â”‚  (Secrets/CM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation de Helm

#### Installation sur Debian

```bash
# MÃ©thode 1: Script officiel (RecommandÃ©)
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# MÃ©thode 2: TÃ©lÃ©chargement manuel
curl -LO https://get.helm.sh/helm-v3.13.0-linux-amd64.tar.gz
tar -zxvf helm-v3.13.0-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm

# MÃ©thode 3: Depuis les sources (dÃ©veloppeurs)
git clone https://github.com/helm/helm.git
cd helm
make build
sudo cp bin/helm /usr/local/bin/

# VÃ©rifier l'installation
helm version
helm help
```

#### Configuration initiale

```bash
# Ajouter des repositories populaires
helm repo add stable https://charts.helm.sh/stable
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts

# Mettre Ã  jour la liste des charts
helm repo update

# Lister les repos configurÃ©s
helm repo list

# Rechercher des charts
helm search repo nginx
helm search hub wordpress
```

### Utilisation de base de Helm

#### Installation d'applications

```bash
# Installation simple
helm install my-nginx bitnami/nginx

# Installation avec nom personnalisÃ©
helm install web-server bitnami/nginx --namespace production --create-namespace

# Installation avec valeurs personnalisÃ©es
helm install my-db bitnami/postgresql \
  --set auth.postgresPassword=secretpassword \
  --set primary.persistence.size=20Gi

# Installation depuis un fichier de valeurs
helm install my-app bitnami/wordpress -f custom-values.yaml
```

#### Gestion des releases

```bash
# Lister les installations
helm list
helm list --all-namespaces

# Voir le statut d'une installation
helm status my-nginx

# Historique des dÃ©ploiements
helm history my-nginx

# Mise Ã  jour d'une installation
helm upgrade my-nginx bitnami/nginx --set service.type=LoadBalancer

# Rollback vers une version prÃ©cÃ©dente
helm rollback my-nginx 1

# DÃ©sinstallation
helm uninstall my-nginx
```

### CrÃ©ation de Charts personnalisÃ©s

#### Structure d'un Chart

```bash
# CrÃ©er un nouveau chart
helm create my-app

# Structure gÃ©nÃ©rÃ©e:
my-app/
â”œâ”€â”€ Chart.yaml          # MÃ©tadonnÃ©es du chart
â”œâ”€â”€ values.yaml         # Valeurs par dÃ©faut
â”œâ”€â”€ charts/            # DÃ©pendances
â”œâ”€â”€ templates/         # Templates Kubernetes
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ _helpers.tpl   # Helpers et fonctions
â”‚   â””â”€â”€ NOTES.txt      # Instructions post-installation
â””â”€â”€ .helmignore        # Fichiers Ã  ignorer
```

#### Chart.yaml - MÃ©tadonnÃ©es

```yaml
# Chart.yaml
apiVersion: v2
name: my-app
description: Une application web simple
type: application
version: 0.1.0          # Version du chart
appVersion: "1.0.0"     # Version de l'application

# Informations complÃ©mentaires
keywords:
  - web
  - nginx
  - application
home: https://example.com
sources:
  - https://github.com/example/my-app
maintainers:
  - name: John Doe
    email: john@example.com
    url: https://example.com

# DÃ©pendances
dependencies:
  - name: postgresql
    version: "12.1.0"
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
```

#### values.yaml - Configuration par dÃ©faut

```yaml
# values.yaml
# Configuration par dÃ©faut de l'application

# RÃ©plicas et image
replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  tag: "1.20"

# Configuration du service
service:
  type: ClusterIP
  port: 80

# Configuration Ingress
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# Ressources
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

# Autoscaling
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

# SÃ©curitÃ©
securityContext:
  runAsNonRoot: true
  runAsUser: 1001

# Stockage persistant
persistence:
  enabled: false
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 1Gi

# Configuration de l'application
config:
  database:
    host: localhost
    port: 5432
    name: myapp
  cache:
    enabled: true
    ttl: 3600

# Base de donnÃ©es PostgreSQL (dÃ©pendance)
postgresql:
  enabled: true
  auth:
    postgresPassword: "changeme"
    database: "myapp"
  primary:
    persistence:
      enabled: true
      size: 8Gi
```

#### Templates avec Go templating

```yaml
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "my-app.fullname" . }}
  labels:
    {{- include "my-app.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "my-app.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      labels:
        {{- include "my-app.selectorLabels" . | nindent 8 }}
    spec:
      securityContext:
        {{- toYaml .Values.securityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: DATABASE_HOST
              value: {{ .Values.config.database.host | quote }}
            - name: DATABASE_PORT
              value: {{ .Values.config.database.port | quote }}
            - name: DATABASE_NAME
              value: {{ .Values.config.database.name | quote }}
            {{- if .Values.postgresql.enabled }}
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "my-app.postgresql.secretName" . }}
                  key: postgres-password
            {{- end }}
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          {{- if .Values.persistence.enabled }}
          volumeMounts:
            - name: data
              mountPath: /data
          {{- end }}
      {{- if .Values.persistence.enabled }}
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: {{ include "my-app.fullname" . }}-data
      {{- end }}
```

#### Helpers et fonctions (_helpers.tpl)

```yaml
# templates/_helpers.tpl
{{/*
Expand the name of the chart.
*/}}
{{- define "my-app.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "my-app.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "my-app.labels" -}}
helm.sh/chart: {{ include "my-app.chart" . }}
{{ include "my-app.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "my-app.selectorLabels" -}}
app.kubernetes.io/name: {{ include "my-app.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
PostgreSQL secret name
*/}}
{{- define "my-app.postgresql.secretName" -}}
{{- if .Values.postgresql.enabled }}
{{- printf "%s-postgresql" (include "my-app.fullname" .) }}
{{- else }}
{{- .Values.externalDatabase.existingSecret | default (printf "%s-external-db" (include "my-app.fullname" .)) }}
{{- end }}
{{- end }}
```

### Tests et validation des Charts

#### Tests Helm intÃ©grÃ©s

```yaml
# templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "my-app.fullname" . }}-test"
  labels:
    {{- include "my-app.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  restartPolicy: Never
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['{{ include "my-app.fullname" . }}:{{ .Values.service.port }}']
```

```bash
# ExÃ©cuter les tests
helm test my-app

# Voir les logs des tests
kubectl logs my-app-test

# Nettoyer les pods de test
helm test my-app --cleanup
```

#### Validation avec helm lint

```bash
# Valider la syntaxe du chart
helm lint my-app/

# Rendu Ã  sec (dry-run)
helm install my-app ./my-app --dry-run --debug

# Template avec valeurs spÃ©cifiques
helm template my-app ./my-app --values production-values.yaml
```

### Charts avancÃ©s et patterns

#### Hooks Helm

```yaml
# templates/pre-install-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ include "my-app.fullname" . }}-pre-install"
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pre-install
        image: postgres:13
        command:
        - /bin/bash
        - -c
        - |
          echo "Running pre-install tasks..."
          # Initialisation de la base de donnÃ©es
          psql -h $DB_HOST -U postgres -c "CREATE DATABASE IF NOT EXISTS myapp;"
```

#### Charts avec sous-charts

```yaml
# Chart.yaml avec dÃ©pendances
dependencies:
  - name: redis
    version: "17.3.7"
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
  - name: postgresql
    version: "12.1.0"
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
```

```bash
# Mettre Ã  jour les dÃ©pendances
helm dependency update

# Installation avec dÃ©pendances
helm install my-stack ./my-app
```

---

## 9.4.2 Kubectl avancÃ©

### MaÃ®trise de kubectl

**kubectl** est l'interface en ligne de commande de Kubernetes. Au-delÃ  des commandes de base, il offre des fonctionnalitÃ©s avancÃ©es pour l'administration et le dÃ©pannage.

### Configuration et contextes

#### Gestion des contextes

```bash
# Lister les contextes disponibles
kubectl config get-contexts

# Voir la configuration actuelle
kubectl config view

# Changer de contexte
kubectl config use-context production-cluster

# CrÃ©er un nouveau contexte
kubectl config set-context dev-context \
  --cluster=development \
  --user=dev-user \
  --namespace=development

# Configurer le namespace par dÃ©faut
kubectl config set-context --current --namespace=production
```

#### Configuration multi-cluster

```yaml
# ~/.kube/config
apiVersion: v1
kind: Config
current-context: production

clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTi...
    server: https://prod-k8s.example.com:6443
  name: production
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTi...
    server: https://dev-k8s.example.com:6443
  name: development

users:
- name: admin@production
  user:
    client-certificate-data: LS0tLS1CRUdJTi...
    client-key-data: LS0tLS1CRUdJTi...
- name: dev@development
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii...

contexts:
- context:
    cluster: production
    user: admin@production
    namespace: default
  name: production
- context:
    cluster: development
    user: dev@development
    namespace: dev
  name: development
```

### Commandes avancÃ©es de kubectl

#### RequÃªtes et filtrage JSONPath

```bash
# Extraire des informations spÃ©cifiques avec JSONPath
kubectl get pods -o jsonpath='{.items[*].metadata.name}'

# Afficher les images utilisÃ©es
kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'

# NÅ“uds avec leurs versions
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.nodeInfo.kubeletVersion}{"\n"}{end}'

# Pods avec leurs adresses IP
kubectl get pods -o wide --no-headers | awk '{print $1"\t"$6}'

# Services avec leurs ports
kubectl get svc -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.ports[*].port}{"\n"}{end}'
```

#### Tri et formatage avancÃ©s

```bash
# Trier les pods par Ã¢ge
kubectl get pods --sort-by=.metadata.creationTimestamp

# Trier les nÅ“uds par nom
kubectl get nodes --sort-by=.metadata.name

# Affichage personnalisÃ© avec colonnes
kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName

# Afficher les Ã©vÃ©nements rÃ©cents
kubectl get events --sort-by=.metadata.creationTimestamp --field-selector type=Warning
```

#### Labels et sÃ©lecteurs avancÃ©s

```bash
# SÃ©lection par labels multiples
kubectl get pods -l app=nginx,version=v1.0

# SÃ©lection par inÃ©galitÃ©
kubectl get pods -l environment!=production

# SÃ©lection avec opÃ©rateurs set
kubectl get pods -l 'environment in (staging,development)'
kubectl get pods -l 'tier notin (frontend,backend)'

# Ajouter des labels Ã  plusieurs ressources
kubectl label pods -l app=nginx version=v2.0

# Supprimer un label
kubectl label pod my-pod version-
```

### Patch et mise Ã  jour en place

#### Types de patch

```bash
# Strategic merge patch (par dÃ©faut)
kubectl patch deployment nginx -p '{"spec":{"replicas":3}}'

# JSON merge patch
kubectl patch deployment nginx --type='merge' -p '{"spec":{"replicas":3}}'

# JSON patch (RFC 6902)
kubectl patch deployment nginx --type='json' -p='[{"op": "replace", "path": "/spec/replicas", "value": 3}]'
```

#### Exemples de patch courants

```bash
# Changer l'image d'un deployment
kubectl patch deployment nginx -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","image":"nginx:1.21"}]}}}}'

# Ajouter une variable d'environnement
kubectl patch deployment myapp -p '{"spec":{"template":{"spec":{"containers":[{"name":"myapp","env":[{"name":"DEBUG","value":"true"}]}]}}}}'

# Mettre Ã  jour les ressources
kubectl patch deployment myapp -p '{"spec":{"template":{"spec":{"containers":[{"name":"myapp","resources":{"requests":{"memory":"256Mi","cpu":"200m"}}}]}}}}'

# Ajouter une annotation
kubectl patch service nginx -p '{"metadata":{"annotations":{"nginx.ingress.kubernetes.io/rewrite-target":"/"}}}'
```

### Debug et troubleshooting

#### Inspection dÃ©taillÃ©e des ressources

```bash
# Describe avec tous les dÃ©tails
kubectl describe pod my-pod | grep -A 10 -B 10 "Error"

# Voir les Ã©vÃ©nements d'un pod spÃ©cifique
kubectl get events --field-selector involvedObject.name=my-pod

# Logs avancÃ©s
kubectl logs -f deployment/nginx --all-containers=true --since=1h
kubectl logs -l app=nginx --prefix=true --since=10m

# Logs prÃ©cÃ©dents (conteneur crashÃ©)
kubectl logs my-pod --previous

# ExÃ©cution de commandes dans les pods
kubectl exec -it my-pod -- /bin/bash
kubectl exec my-pod -- ps aux
kubectl exec my-pod -- netstat -tulpn
```

#### Top et mÃ©triques

```bash
# Utilisation des ressources des nÅ“uds
kubectl top nodes

# Utilisation des ressources des pods
kubectl top pods --all-namespaces --sort-by=memory
kubectl top pods --containers --sort-by=cpu

# MÃ©triques dÃ©taillÃ©es (si metrics-server est installÃ©)
kubectl top pod my-pod --containers
```

### Kubectl plugins et extensions

#### Installation de krew (gestionnaire de plugins)

```bash
# Installation de krew
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

# Ajouter krew au PATH
echo 'export PATH="${PATH}:${HOME}/.krew/bin"' >> ~/.bashrc
source ~/.bashrc
```

#### Plugins utiles

```bash
# Installer des plugins populaires
kubectl krew install ctx     # Changement rapide de contexte
kubectl krew install ns      # Changement rapide de namespace
kubectl krew install tree    # Vue arborescente des ressources
kubectl krew install neat    # Nettoyage des manifests kubectl
kubectl krew install whoami  # Informations utilisateur actuel

# Utilisation des plugins
kubectl ctx                  # Lister et changer de contexte
kubectl ns production        # Changer de namespace
kubectl tree deployment nginx  # Vue arborescente du deployment
kubectl neat pod my-pod      # Manifest nettoyÃ© sans metadata
kubectl whoami              # Informations sur l'utilisateur
```

### Automatisation avec kubectl

#### Scripts d'administration

```bash
#!/bin/bash
# health-check.sh - VÃ©rification de santÃ© du cluster

echo "=== Cluster Health Check ==="

# VÃ©rifier les nÅ“uds
echo "Nodes status:"
kubectl get nodes --no-headers | while read node status roles age version; do
  if [[ "$status" != "Ready" ]]; then
    echo "âŒ Node $node is $status"
  else
    echo "âœ… Node $node is ready"
  fi
done

# VÃ©rifier les pods systÃ¨me
echo -e "\nSystem pods:"
kubectl get pods -n kube-system --no-headers | while read name ready status restarts age; do
  if [[ "$status" != "Running" && "$status" != "Completed" ]]; then
    echo "âŒ Pod $name is $status"
  fi
done

# VÃ©rifier les services critiques
echo -e "\nCritical services:"
services=("kube-dns" "metrics-server" "ingress-nginx-controller")
for svc in "${services[@]}"; do
  if kubectl get svc "$svc" -A &>/dev/null; then
    echo "âœ… Service $svc is present"
  else
    echo "âŒ Service $svc is missing"
  fi
done

echo -e "\n=== Health Check Complete ==="
```

#### DÃ©ploiement conditionnel

```bash
#!/bin/bash
# conditional-deploy.sh

ENVIRONMENT=${1:-staging}
APP_NAME=${2:-myapp}

# VÃ©rifier si l'environnement existe
if ! kubectl get namespace "$ENVIRONMENT" &>/dev/null; then
  echo "Creating namespace $ENVIRONMENT"
  kubectl create namespace "$ENVIRONMENT"
fi

# DÃ©ployer selon l'environnement
case $ENVIRONMENT in
  production)
    kubectl apply -f manifests/production/ -n "$ENVIRONMENT"
    kubectl scale deployment "$APP_NAME" --replicas=3 -n "$ENVIRONMENT"
    ;;
  staging)
    kubectl apply -f manifests/staging/ -n "$ENVIRONMENT"
    kubectl scale deployment "$APP_NAME" --replicas=2 -n "$ENVIRONMENT"
    ;;
  development)
    kubectl apply -f manifests/development/ -n "$ENVIRONMENT"
    kubectl scale deployment "$APP_NAME" --replicas=1 -n "$ENVIRONMENT"
    ;;
  *)
    echo "Unknown environment: $ENVIRONMENT"
    exit 1
    ;;
esac

# Attendre que le dÃ©ploiement soit prÃªt
kubectl rollout status deployment/"$APP_NAME" -n "$ENVIRONMENT"
echo "Deployment complete!"
```

---

## 9.4.3 Kustomize

### Introduction Ã  Kustomize

**Kustomize** est un outil de gestion de configuration pour Kubernetes qui permet de personnaliser les manifests YAML sans les modifier directement.

#### Philosophie Kustomize

**Base + Overlays**
- **Base** : Configuration commune partagÃ©e
- **Overlays** : Variantes spÃ©cifiques par environnement
- **Patches** : Modifications ciblÃ©es
- **Sans templating** : YAML natif, pas de langage de template

#### Avantages de Kustomize

**SimplicitÃ©**
- YAML standard, pas de syntaxe additionnelle
- IntÃ©grÃ© dans kubectl depuis la version 1.14
- Courbe d'apprentissage rÃ©duite

**MaintenabilitÃ©**
- SÃ©paration claire base/variantes
- Ã‰vite la duplication de code
- Gestion des configurations par environnement

### Structure Kustomize

#### Organisation des fichiers

```
app/
â”œâ”€â”€ base/                   # Configuration de base
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â””â”€â”€ kustomization.yaml
â”œâ”€â”€ overlays/              # Variantes par environnement
â”‚   â”œâ”€â”€ development/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ replica-count.yaml
â”‚   â”‚   â””â”€â”€ config-dev.yaml
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”‚   â””â”€â”€ resources.yaml
â”‚   â””â”€â”€ production/
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â”œâ”€â”€ ingress.yaml
â”‚       â”œâ”€â”€ hpa.yaml
â”‚       â””â”€â”€ resources.yaml
```

### Configuration de base

#### kustomization.yaml (base)

```yaml
# base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Ressources de base
resources:
  - deployment.yaml
  - service.yaml
  - configmap.yaml

# Images Ã  gÃ©rer
images:
  - name: myapp
    newTag: latest

# ConfigMap depuis fichiers
configMapGenerator:
  - name: app-config
    files:
      - config.properties
      - log4j.xml

# Secrets depuis fichiers
secretGenerator:
  - name: app-secrets
    files:
      - database-password.txt
      - api-key.txt

# Labels communs
commonLabels:
  app: myapp
  version: v1.0

# Annotations communes
commonAnnotations:
  maintainer: "team@example.com"
  documentation: "https://docs.example.com/myapp"

# PrÃ©fixe pour toutes les ressources
namePrefix: mycompany-

# Suffixe pour toutes les ressources
nameSuffix: -v1

# Namespace pour toutes les ressources
namespace: default
```

#### Ressources de base

```yaml
# base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        env:
        - name: APP_ENV
          value: "default"
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: database_url
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: api-key
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
# base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP
---
# base/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  database_url: "postgresql://localhost:5432/myapp"
  log_level: "info"
  max_connections: "100"
  config.properties: |
    server.port=8080
    server.servlet.context-path=/api
    logging.level.com.example=INFO
    spring.datasource.url=postgresql://localhost:5432/myapp
```

### Overlays par environnement

#### DÃ©veloppement

```yaml
# overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# RÃ©fÃ©rence Ã  la base
resources:
  - ../../base

# Namespace spÃ©cifique
namespace: development

# Suffixe pour identifier l'environnement
nameSuffix: -dev

# Patches pour l'environnement de dÃ©veloppement
patchesStrategicMerge:
  - replica-count.yaml
  - config-dev.yaml

# Variables d'environnement spÃ©cifiques
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: add
        path: /spec/template/spec/containers/0/env/-
        value:
          name: DEBUG
          value: "true"
      - op: replace
        path: /spec/template/spec/containers/0/env/0/value
        value: "development"

# Images avec tag spÃ©cifique
images:
  - name: myapp
    newTag: dev-latest

# ConfigMap avec des valeurs de dev
configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - log_level=debug
      - max_connections=10
      - database_url=postgresql://dev-db:5432/myapp_dev

# Labels spÃ©cifiques Ã  l'environnement
commonLabels:
  environment: development
  owner: dev-team
```

```yaml
# overlays/development/replica-count.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
```

```yaml
# overlays/development/config-dev.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  config.properties: |
    server.port=8080
    server.servlet.context-path=/api
    logging.level.com.example=DEBUG
    logging.level.org.springframework=DEBUG
    spring.datasource.url=postgresql://dev-db:5432/myapp_dev
    spring.jpa.show-sql=true
    management.endpoints.web.exposure.include=*
```

#### Staging

```yaml
# overlays/staging/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
  - ingress.yaml

namespace: staging
nameSuffix: -staging

patchesStrategicMerge:
  - resources.yaml

images:
  - name: myapp
    newTag: staging-v1.2.0

configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - log_level=info
      - max_connections=50
      - database_url=postgresql://staging-db:5432/myapp_staging

commonLabels:
  environment: staging
  version: v1.2.0

# RÃ©pliques pour staging
replicas:
  - name: myapp
    count: 2
```

```yaml
# overlays/staging/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-staging
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - staging.myapp.example.com
    secretName: myapp-staging-tls
  rules:
  - host: staging.myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
```

```yaml
# overlays/staging/resources.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: myapp
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

#### Production

```yaml
# overlays/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
  - ingress.yaml
  - hpa.yaml
  - pdb.yaml

namespace: production
nameSuffix: -prod

patchesStrategicMerge:
  - resources.yaml
  - security.yaml

images:
  - name: myapp
    newTag: v1.2.0

configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - log_level=warn
      - max_connections=200
      - database_url=postgresql://prod-db:5432/myapp_production

secretGenerator:
  - name: app-secrets
    behavior: merge
    literals:
      - database_password=production_secret_password
      - api_key=prod_api_key_here

commonLabels:
  environment: production
  version: v1.2.0
  criticality: high

replicas:
  - name: myapp
    count: 5

# Patches JSON pour modifications prÃ©cises
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: add
        path: /spec/template/spec/containers/0/env/-
        value:
          name: NEW_RELIC_LICENSE_KEY
          valueFrom:
            secretKeyRef:
              name: monitoring-secrets
              key: newrelic-key
```

```yaml
# overlays/production/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

```yaml
# overlays/production/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
```

### Patches avancÃ©s avec Kustomize

#### Strategic Merge Patches

```yaml
# patch-strategic-merge.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: myapp
        env:
        - name: ADDITIONAL_VAR
          value: "additional_value"
        # Cette variable sera ajoutÃ©e Ã  la liste existante
      - name: sidecar
        image: sidecar:latest
        # Ce conteneur sera ajoutÃ© aux conteneurs existants
```

#### JSON Patches (RFC 6902)

```yaml
# kustomization.yaml avec JSON patches
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: add
        path: /spec/template/spec/containers/0/volumeMounts/-
        value:
          name: cache-volume
          mountPath: /tmp/cache
      - op: add
        path: /spec/template/spec/volumes/-
        value:
          name: cache-volume
          emptyDir: {}
      - op: replace
        path: /spec/replicas
        value: 3
```

#### Patches avec ciblage prÃ©cis

```yaml
# patch-with-selectors.yaml
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: myapp
      namespace: production
    patch: |-
      - op: add
        path: /spec/template/metadata/annotations/prometheus.io~1scrape
        value: "true"
      - op: add
        path: /spec/template/metadata/annotations/prometheus.io~1port
        value: "9090"
```

### GÃ©nÃ©rateurs Kustomize

#### ConfigMap Generator

```yaml
# kustomization.yaml
configMapGenerator:
  - name: app-config
    # Depuis des littÃ©raux
    literals:
      - database_host=localhost
      - database_port=5432
      - debug=false

    # Depuis des fichiers
    files:
      - application.properties
      - log4j2.xml

    # Depuis des variables d'environnement
    envs:
      - app.env

    # Options avancÃ©es
    options:
      # DÃ©sactiver le suffixe de hash
      disableNameSuffixHash: true
      # Labels et annotations
      labels:
        config-type: application
      annotations:
        config-version: "1.0"
```

#### Secret Generator

```yaml
secretGenerator:
  - name: app-secrets
    type: Opaque
    literals:
      - username=admin
      - password=supersecret
    files:
      - tls.crt
      - tls.key
    options:
      disableNameSuffixHash: false
      labels:
        secret-type: application
      annotations:
        secret-version: "1.0"

  # Secret TLS spÃ©cialisÃ©
  - name: tls-secret
    type: kubernetes.io/tls
    files:
      - tls.crt=server.crt
      - tls.key=server.key
```

### Transformateurs Kustomize

#### Images Transformer

```yaml
# Gestion des images
images:
  - name: nginx
    newName: my-registry.com/nginx
    newTag: 1.21.0
  - name: myapp
    newTag: v2.0.0
  - name: postgres
    digest: sha256:b5b2b2c50720...
```

#### Replicas Transformer

```yaml
# Gestion des rÃ©pliques
replicas:
  - name: frontend
    count: 3
  - name: backend
    count: 5
```

#### Namespace Transformer

```yaml
# Application d'un namespace Ã  toutes les ressources
namespace: my-namespace

# Ou via patch pour plus de contrÃ´le
patches:
  - target:
      kind: ".*"
    patch: |-
      - op: replace
        path: /metadata/namespace
        value: specific-namespace
```

### Utilisation de Kustomize

#### Commandes de base

```bash
# AperÃ§u des manifests gÃ©nÃ©rÃ©s
kubectl kustomize overlays/development

# Application directe
kubectl apply -k overlays/development

# Sauvegarde des manifests gÃ©nÃ©rÃ©s
kubectl kustomize overlays/production > production-manifests.yaml

# Validation avant application
kubectl kustomize overlays/staging | kubectl apply --dry-run=client -f -

# Suppression des ressources
kubectl delete -k overlays/development
```

#### IntÃ©gration avec Helm

```yaml
# kustomization.yaml utilisant un Chart Helm
helmCharts:
  - name: prometheus
    repo: https://prometheus-community.github.io/helm-charts
    version: 15.10.1
    releaseName: monitoring
    namespace: monitoring
    valuesInline:
      server:
        persistentVolume:
          size: 20Gi
      alertmanager:
        enabled: true

resources:
  - prometheus-config-override.yaml
```

### Patterns avancÃ©s

#### Multi-tenant avec Kustomize

```
multi-tenant/
â”œâ”€â”€ base/
â”‚   â””â”€â”€ kustomization.yaml
â”œâ”€â”€ tenants/
â”‚   â”œâ”€â”€ tenant-a/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ tenant-specific.yaml
â”‚   â”œâ”€â”€ tenant-b/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ tenant-specific.yaml
â”‚   â””â”€â”€ tenant-c/
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â””â”€â”€ tenant-specific.yaml
â””â”€â”€ environments/
    â”œâ”€â”€ staging/
    â”‚   â””â”€â”€ kustomization.yaml
    â””â”€â”€ production/
        â””â”€â”€ kustomization.yaml
```

```yaml
# environments/production/kustomization.yaml
resources:
  - ../../tenants/tenant-a
  - ../../tenants/tenant-b
  - ../../tenants/tenant-c

namespace: production

commonLabels:
  environment: production
```

#### GitOps avec Kustomize

```yaml
# .github/workflows/deploy.yml
name: Deploy Application
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Setup Kustomize
      run: |
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/

    - name: Update image tag
      run: |
        cd overlays/production
        kustomize edit set image myapp=myapp:${GITHUB_SHA::8}

    - name: Deploy to production
      run: |
        kubectl kustomize overlays/production | kubectl apply -f -
```

---

## 9.4.4 Monitoring (Prometheus, Grafana)

### Introduction au monitoring Kubernetes

Le monitoring est crucial pour maintenir la santÃ© et les performances d'un cluster Kubernetes. **Prometheus** et **Grafana** forment le stack de monitoring de rÃ©fÃ©rence dans l'Ã©cosystÃ¨me cloud-native.

#### Les trois piliers de l'observabilitÃ©

**Metrics (MÃ©triques)**
- DonnÃ©es numÃ©riques temporelles
- CPU, mÃ©moire, requÃªtes par seconde
- CollectÃ©es par Prometheus

**Logs (Journaux)**
- Ã‰vÃ©nements textuels horodatÃ©s
- Messages d'erreur, traces d'exÃ©cution
- GÃ©rÃ©s par ELK/Fluentd (hors scope ici)

**Traces (TraÃ§age)**
- Suivi des requÃªtes distribuÃ©es
- Latence, chemins d'exÃ©cution
- Jaeger, Zipkin (hors scope ici)

### Architecture du monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Applications  â”‚â”€â”€â”€â–¶â”‚   Prometheus    â”‚â”€â”€â”€â–¶â”‚    Grafana      â”‚
â”‚   (metrics)     â”‚    â”‚   (collect)     â”‚    â”‚   (visualize)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node Exporter  â”‚    â”‚  AlertManager   â”‚    â”‚   Dashboard     â”‚
â”‚   (systÃ¨me)     â”‚    â”‚    (alertes)    â”‚    â”‚   (graphs)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation de Prometheus

#### Installation avec Helm (RecommandÃ©e)

```bash
# Ajouter le repository Prometheus
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# CrÃ©er le namespace
kubectl create namespace monitoring

# Installer le stack complet (Prometheus + Grafana + AlertManager)
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values prometheus-values.yaml
```

#### Configuration personnalisÃ©e

```yaml
# prometheus-values.yaml
# Configuration pour kube-prometheus-stack

prometheus:
  prometheusSpec:
    # RÃ©tention des donnÃ©es
    retention: 15d
    retentionSize: 50GB

    # Stockage persistant
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi

    # Ressources
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
      limits:
        memory: 2Gi
        cpu: 1000m

    # RÃ¨gles d'alertes supplÃ©mentaires
    additionalPrometheusRulesMap:
      custom-rules:
        groups:
        - name: custom.rules
          rules:
          - alert: HighPodCPU
            expr: rate(container_cpu_usage_seconds_total[5m]) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} high CPU usage"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has high CPU usage"

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    # Configuration des alertes
    config:
      global:
        smtp_smarthost: 'smtp.example.com:587'
        smtp_from: 'alerts@example.com'

      route:
        group_by: ['alertname', 'cluster']
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 1h
        receiver: 'default-receiver'
        routes:
        - match:
            severity: critical
          receiver: 'critical-receiver'

      receivers:
      - name: 'default-receiver'
        email_configs:
        - to: 'team@example.com'
          subject: '[{{ .Status }}] {{ .GroupLabels.alertname }}'
          body: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}

      - name: 'critical-receiver'
        email_configs:
        - to: 'oncall@example.com'
          subject: '[CRITICAL] {{ .GroupLabels.alertname }}'

grafana:
  # AccÃ¨s admin
  adminPassword: admin123

  # Persistance
  persistence:
    enabled: true
    storageClassName: local-storage
    size: 10Gi

  # Dashboards par dÃ©faut
  defaultDashboardsEnabled: true

  # Dashboards personnalisÃ©s
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'custom-dashboards'
        orgId: 1
        folder: 'Custom'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/custom

  # Sources de donnÃ©es supplÃ©mentaires
  additionalDataSources:
  - name: Loki
    type: loki
    url: http://loki:3100
    access: proxy
    isDefault: false

# Node Exporter pour mÃ©triques systÃ¨me
nodeExporter:
  enabled: true

# kube-state-metrics pour mÃ©triques Kubernetes
kubeStateMetrics:
  enabled: true

# Service Monitors personnalisÃ©s
additionalServiceMonitors:
  - name: myapp-monitor
    selector:
      matchLabels:
        app: myapp
    endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
```

### Configuration de Prometheus

#### Prometheus.yml de base

```yaml
# Configuration Prometheus standalone (si pas Helm)
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"
  - "recording_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Prometheus lui-mÃªme
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # API Server Kubernetes
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
    - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      action: keep
      regex: default;kubernetes;https

  # Kubelet et cAdvisor
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
    - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - action: labelmap
      regex: __meta_kubernetes_node_label_(.+)

  # Pods avec annotation prometheus.io/scrape
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
      target_label: __address__
    - action: labelmap
      regex: __meta_kubernetes_pod_label_(.+)
```

### RÃ¨gles d'alerte Prometheus

#### RÃ¨gles d'alerte de base

```yaml
# alert_rules.yml
groups:
- name: kubernetes-alerts
  rules:

  # NÅ“ud down
  - alert: NodeDown
    expr: up{job="node-exporter"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.instance }} is down"
      description: "Node {{ $labels.instance }} has been down for more than 5 minutes"

  # Utilisation CPU Ã©levÃ©e
  - alert: HighCpuUsage
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is above 80% for 5 minutes on {{ $labels.instance }}"

  # Utilisation mÃ©moire Ã©levÃ©e
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage on {{ $labels.instance }}"
      description: "Memory usage is above 85% on {{ $labels.instance }}"

  # Espace disque faible
  - alert: DiskSpaceLow
    expr: (1 - node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes) * 100 > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Disk space low on {{ $labels.instance }}"
      description: "Disk usage is above 90% on {{ $labels.instance }} mount {{ $labels.mountpoint }}"

  # Pod crashant
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.pod }} is crash looping"
      description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently"

  # Deployment pas prÃªt
  - alert: DeploymentReplicasNotReady
    expr: kube_deployment_spec_replicas != kube_deployment_status_ready_replicas
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Deployment {{ $labels.deployment }} has unready replicas"
      description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} unready replicas"
```

### Configuration de Grafana

#### Dashboards Kubernetes essentiels

Grafana peut Ãªtre configurÃ© avec des dashboards prÃªts Ã  l'emploi :

**Dashboards recommandÃ©s** (IDs Grafana.com) :
- **315** : Kubernetes cluster monitoring
- **747** : Kubernetes Deployment metrics
- **6417** : Kubernetes cluster overview
- **8588** : Node Exporter Full
- **1860** : Node Exporter Full avec alertes

#### Import de dashboards

```bash
# Via l'interface Grafana : + â†’ Import â†’ ID du dashboard

# Ou via API
curl -X POST \
  http://admin:admin123@grafana.monitoring.svc.cluster.local:3000/api/dashboards/db \
  -H 'Content-Type: application/json' \
  -d '{
    "dashboard": {
      "id": null,
      "title": "Custom Kubernetes Dashboard",
      "panels": [...]
    },
    "overwrite": true
  }'
```

#### Dashboard personnalisÃ© pour application

```json
{
  "dashboard": {
    "id": null,
    "title": "MyApp Monitoring",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{job=\"myapp\"}[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"myapp\"}[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket{job=\"myapp\"}[5m]))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds",
            "min": 0
          }
        ]
      }
    ]
  }
}
```

### Exposition des mÃ©triques dans les applications

#### MÃ©triques dans une application Go

```go
// main.go
package main

import (
    "net/http"
    "time"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
    httpRequestsTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "endpoint", "status_code"},
    )

    httpRequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "http_request_duration_seconds",
            Help: "HTTP request duration in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )
)

func init() {
    prometheus.MustRegister(httpRequestsTotal)
    prometheus.MustRegister(httpRequestDuration)
}

func instrumentHandler(handler http.HandlerFunc, endpoint string) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()

        // Wrapper pour capturer le status code
        wrapped := &responseWriter{ResponseWriter: w, statusCode: 200}

        handler(wrapped, r)

        duration := time.Since(start).Seconds()

        httpRequestsTotal.With(prometheus.Labels{
            "method":      r.Method,
            "endpoint":    endpoint,
            "status_code": fmt.Sprintf("%d", wrapped.statusCode),
        }).Inc()

        httpRequestDuration.With(prometheus.Labels{
            "method":   r.Method,
            "endpoint": endpoint,
        }).Observe(duration)
    }
}

type responseWriter struct {
    http.ResponseWriter
    statusCode int
}

func (rw *responseWriter) WriteHeader(code int) {
    rw.statusCode = code
    rw.ResponseWriter.WriteHeader(code)
}

func main() {
    http.HandleFunc("/", instrumentHandler(homeHandler, "/"))
    http.HandleFunc("/api/users", instrumentHandler(usersHandler, "/api/users"))
    http.Handle("/metrics", promhttp.Handler())

    log.Fatal(http.ListenAndServe(":8080", nil))
}

func homeHandler(w http.ResponseWriter, r *http.Request) {
    w.WriteHeader(http.StatusOK)
    fmt.Fprintf(w, "Hello World!")
}

func usersHandler(w http.ResponseWriter, r *http.Request) {
    // Simulation d'une erreur occasionnelle
    if rand.Float32() < 0.1 {
        w.WriteHeader(http.StatusInternalServerError)
        fmt.Fprintf(w, "Internal Server Error")
        return
    }

    w.WriteHeader(http.StatusOK)
    fmt.Fprintf(w, `{"users": ["alice", "bob", "charlie"]}`)
}
```

#### MÃ©triques dans une application Java (Spring Boot)

```java
// Application.java
@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Bean
    public MeterRegistryCustomizer<MeterRegistry> metricsCommonTags() {
        return registry -> registry.config().commonTags("application", "myapp");
    }
}

// UserController.java
@RestController
@RequestMapping("/api/users")
public class UserController {

    private final Counter userRequestsCounter;
    private final Timer userRequestsTimer;
    private final Gauge activeUsersGauge;

    public UserController(MeterRegistry meterRegistry) {
        this.userRequestsCounter = Counter.builder("user_requests_total")
            .description("Total user requests")
            .tag("endpoint", "/api/users")
            .register(meterRegistry);

        this.userRequestsTimer = Timer.builder("user_requests_duration_seconds")
            .description("User request duration")
            .register(meterRegistry);

        this.activeUsersGauge = Gauge.builder("active_users")
            .description("Number of active users")
            .register(meterRegistry, this, UserController::getActiveUserCount);
    }

    @GetMapping
    public ResponseEntity<List<User>> getUsers() {
        return userRequestsTimer.recordCallable(() -> {
            userRequestsCounter.increment();
            // Logique mÃ©tier
            List<User> users = userService.findAll();
            return ResponseEntity.ok(users);
        });
    }

    private Double getActiveUserCount() {
        return (double) userService.getActiveUserCount();
    }
}
```

#### Configuration Kubernetes pour exposition des mÃ©triques

```yaml
# deployment-with-metrics.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
      annotations:
        # Annotations pour Prometheus scraping
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "9090"
    spec:
      containers:
      - name: myapp
        image: myapp:v1.0.0
        ports:
        - name: http
          containerPort: 8080
        - name: metrics
          containerPort: 9090
        # Health checks utilisant les mÃ©triques
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 9090
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 9090
          initialDelaySeconds: 5
          periodSeconds: 5
        env:
        - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE
          value: "health,info,metrics,prometheus"
        - name: MANAGEMENT_SERVER_PORT
          value: "9090"
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  labels:
    app: myapp
spec:
  selector:
    app: myapp
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090
```

### ServiceMonitor pour dÃ©couverte automatique

```yaml
# myapp-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp-monitor
  namespace: monitoring
  labels:
    app: myapp
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    # Relabeling pour ajouter des labels personnalisÃ©s
    relabelings:
    - sourceLabels: [__meta_kubernetes_service_name]
      targetLabel: service
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: kubernetes_namespace
  namespaceSelector:
    matchNames:
    - default
    - production
```

### Alerting avancÃ©

#### RÃ¨gles d'alerte spÃ©cifiques aux applications

```yaml
# application-alerts.yml
groups:
- name: application-alerts
  rules:

  # Taux d'erreur Ã©levÃ©
  - alert: HighErrorRate
    expr: |
      (
        rate(http_requests_total{status_code=~"5.."}[5m])
        /
        rate(http_requests_total[5m])
      ) * 100 > 5
    for: 5m
    labels:
      severity: critical
      team: backend
    annotations:
      summary: "High error rate for {{ $labels.service }}"
      description: "Error rate is {{ $value }}% for service {{ $labels.service }}"
      runbook_url: "https://wiki.example.com/runbooks/high-error-rate"

  # Latence Ã©levÃ©e
  - alert: HighLatency
    expr: |
      histogram_quantile(0.95,
        rate(http_request_duration_seconds_bucket[5m])
      ) > 1
    for: 10m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "High latency for {{ $labels.service }}"
      description: "95th percentile latency is {{ $value }}s for {{ $labels.service }}"

  # Pas de trafic (service potentiellement down)
  - alert: NoTraffic
    expr: |
      rate(http_requests_total[5m]) == 0
    for: 5m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "No traffic to {{ $labels.service }}"
      description: "Service {{ $labels.service }} is not receiving any requests"

  # Utilisation mÃ©moire application Ã©levÃ©e
  - alert: HighApplicationMemory
    expr: |
      (
        container_memory_usage_bytes{pod=~"myapp-.*"}
        /
        container_spec_memory_limit_bytes{pod=~"myapp-.*"}
      ) * 100 > 85
    for: 5m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "High memory usage for pod {{ $labels.pod }}"
      description: "Memory usage is {{ $value }}% for pod {{ $labels.pod }}"
```

#### Configuration AlertManager avancÃ©e

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'password'

# Templates personnalisÃ©s
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routage des alertes
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default-receiver'

  routes:
  # Alertes critiques vers l'Ã©quipe d'astreinte
  - match:
      severity: critical
    receiver: 'oncall-receiver'
    group_wait: 0s
    repeat_interval: 5m
    continue: true

  # Alertes par Ã©quipe
  - match:
      team: backend
    receiver: 'backend-team'
    continue: true

  - match:
      team: frontend
    receiver: 'frontend-team'
    continue: true

  # Alertes de dÃ©veloppement (moins urgentes)
  - match:
      environment: development
    receiver: 'dev-team'
    group_interval: 1h
    repeat_interval: 24h

# Inhibition des alertes (Ã©viter le spam)
inhibit_rules:
  # Si un nÅ“ud est down, ne pas alerter pour les pods sur ce nÅ“ud
  - source_match:
      alertname: NodeDown
    target_match:
      alertname: PodCrashLooping
    equal: ['instance']

  # Si un service est down, ne pas alerter pour la latence
  - source_match:
      alertname: NoTraffic
    target_match:
      alertname: HighLatency
    equal: ['service']

receivers:
- name: 'default-receiver'
  email_configs:
  - to: 'alerts@example.com'
    subject: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}'
    html: '{{ template "email.default.html" . }}'

- name: 'oncall-receiver'
  # Email pour astreinte
  email_configs:
  - to: 'oncall@example.com'
    subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
    html: '{{ template "email.critical.html" . }}'

  # Slack pour notifications immÃ©diates
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts-critical'
    title: 'Critical Alert: {{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    color: 'danger'

  # PagerDuty pour escalade
  pagerduty_configs:
  - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
    description: '{{ .GroupLabels.alertname }}'

- name: 'backend-team'
  email_configs:
  - to: 'backend-team@example.com'
    subject: '[Backend] {{ .GroupLabels.alertname }}'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#backend-alerts'
```

### Dashboards Grafana personnalisÃ©s

#### Dashboard SLI/SLO (Service Level Indicators/Objectives)

```json
{
  "dashboard": {
    "title": "Service SLI/SLO Dashboard",
    "panels": [
      {
        "title": "Availability SLI",
        "type": "stat",
        "targets": [
          {
            "expr": "(\n  (\n    sum(rate(http_requests_total{job=\"myapp\"}[7d])) -\n    sum(rate(http_requests_total{job=\"myapp\",status_code=~\"5..\"}[7d]))\n  ) /\n  sum(rate(http_requests_total{job=\"myapp\"}[7d]))\n) * 100",
            "legendFormat": "Availability %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "min": 0,
            "max": 100,
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 95},
                {"color": "green", "value": 99.9}
              ]
            }
          }
        }
      },
      {
        "title": "Error Budget Remaining",
        "type": "bargauge",
        "targets": [
          {
            "expr": "(\n  1 - (\n    1 - 0.999  # 99.9% SLO\n  ) * 7 * 24 * 3600 / (\n    7 * 24 * 3600 - (\n      sum(increase(http_requests_total{job=\"myapp\",status_code=~\"5..\"}[7d]))\n    )\n  )\n) * 100",
            "legendFormat": "Error Budget %"
          }
        ]
      }
    ]
  }
}
```

### Monitoring des ressources Kubernetes

#### MÃ©triques essentielles Ã  surveiller

```yaml
# kubernetes-resource-alerts.yml
groups:
- name: kubernetes-resources
  rules:

  # Quota de namespace approchant
  - alert: NamespaceQuotaNearLimit
    expr: |
      (
        kube_resourcequota{resource="requests.memory", type="used"}
        /
        kube_resourcequota{resource="requests.memory", type="hard"}
      ) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Namespace {{ $labels.namespace }} memory quota near limit"
      description: "Memory quota usage is {{ $value }}% in namespace {{ $labels.namespace }}"

  # NÅ“ud avec trop de pods
  - alert: NodeHighPodCount
    expr: |
      (
        kube_node_status_allocatable{resource="pods"}
        -
        kube_node_status_capacity{resource="pods"}
      ) < 10
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Node {{ $labels.node }} is running out of pod capacity"
      description: "Node {{ $labels.node }} has less than 10 pods available"

  # PersistentVolume bientÃ´t plein
  - alert: PVCAlmostFull
    expr: |
      (
        kubelet_volume_stats_used_bytes
        /
        kubelet_volume_stats_capacity_bytes
      ) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} almost full"
      description: "PVC usage is {{ $value }}% in namespace {{ $labels.namespace }}"
```

### Backup et restauration des configurations

#### Script de backup Prometheus/Grafana

```bash
#!/bin/bash
# backup-monitoring.sh

BACKUP_DIR="/backup/monitoring/$(date +%Y%m%d-%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "Backing up monitoring stack..."

# Backup des ConfigMaps Prometheus
kubectl get configmap -n monitoring -o yaml > "$BACKUP_DIR/prometheus-configs.yaml"

# Backup des rÃ¨gles d'alertes
kubectl get prometheusrule -n monitoring -o yaml > "$BACKUP_DIR/prometheus-rules.yaml"

# Backup des dashboards Grafana
kubectl get configmap -n monitoring -l grafana_dashboard=1 -o yaml > "$BACKUP_DIR/grafana-dashboards.yaml"

# Backup des donnÃ©es Grafana (si accessible)
POD=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n monitoring "$POD" -- tar czf - /var/lib/grafana > "$BACKUP_DIR/grafana-data.tar.gz"

# Backup des donnÃ©es Prometheus (snapshot)
PROMETHEUS_POD=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n monitoring "$PROMETHEUS_POD" -- \
  curl -X POST http://localhost:9090/api/v1/admin/tsdb/snapshot

echo "Backup completed in $BACKUP_DIR"
tar czf "$BACKUP_DIR.tar.gz" -C "$(dirname $BACKUP_DIR)" "$(basename $BACKUP_DIR)"
rm -rf "$BACKUP_DIR"
echo "Backup archive: $BACKUP_DIR.tar.gz"
```

### Optimisation des performances

#### Configuration Prometheus pour haute charge

```yaml
# prometheus-performance.yaml
prometheus:
  prometheusSpec:
    # ParamÃ¨tres de performance
    scrapeInterval: 30s
    evaluationInterval: 30s

    # Optimisations mÃ©moire
    resources:
      requests:
        memory: 4Gi
        cpu: 1000m
      limits:
        memory: 8Gi
        cpu: 2000m

    # Configuration TSDB
    tsdb:
      # RÃ©tention plus aggressive pour les sÃ©ries peu utilisÃ©es
      retentionSize: "100GB"

      # Compression plus agressive
      compression: "snappy"

      # Optimisations WAL
      walCompression: true

    # Limitations pour Ã©viter la surcharge
    query:
      maxConcurrency: 20
      maxSamples: 50000000
      timeout: 2m

    # Optimisations rÃ©seau
    scrapeTimeout: 10s

    # RÃ¨gles de dÃ©couverte de services optimisÃ©es
    serviceDiscoveryConfigs:
      - kubernetes_sd_configs:
        - role: endpoints
          # Limitation du taux de dÃ©couverte
          refresh_interval: 60s
```

#### MÃ©triques de performance Ã  surveiller

```yaml
# monitoring-performance-alerts.yml
groups:
- name: monitoring-performance
  rules:

  # Prometheus ingestion rate Ã©levÃ©e
  - alert: PrometheusHighIngestionRate
    expr: rate(prometheus_samples_ingested_total[5m]) > 100000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus high sample ingestion rate"
      description: "Prometheus is ingesting samples at {{ $value }} samples/sec"

  # Grafana rÃ©ponse lente
  - alert: GrafanaSlowQueries
    expr: histogram_quantile(0.99, rate(grafana_http_request_duration_seconds_bucket[5m])) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Grafana slow query response time"
      description: "99th percentile response time is {{ $value }}s"
```

---

## Conclusion de la section 9.4

Cette section a explorÃ© les outils essentiels de l'Ã©cosystÃ¨me Kubernetes qui transforment un cluster de base en plateforme d'exploitation moderne et observable.

### RÃ©capitulatif des outils couverts

#### Helm - Gestionnaire de paquets
âœ… **MaÃ®trisÃ©** : Installation, crÃ©ation de charts, gestion des releases
âœ… **Patterns avancÃ©s** : Hooks, dÃ©pendances, tests intÃ©grÃ©s
âœ… **Production** : Versionning, rollbacks, configuration multi-environnements

#### kubectl avancÃ©
âœ… **EfficacitÃ©** : Contextes, JSONPath, patches en place
âœ… **Debugging** : Logs, Ã©vÃ©nements, mÃ©triques
âœ… **ExtensibilitÃ©** : Plugins krew, scripts d'automatisation

#### Kustomize
âœ… **Configuration** : Base + overlays, gestion multi-environnements
âœ… **FlexibilitÃ©** : Patches, gÃ©nÃ©rateurs, transformateurs
âœ… **GitOps ready** : Configuration declarative, versioning

#### Monitoring avec Prometheus/Grafana
âœ… **ObservabilitÃ© complÃ¨te** : MÃ©triques, alertes, dashboards
âœ… **Applications** : Instrumentation, SLI/SLO
âœ… **Production** : Performance, backup, optimisation

### Synergie des outils

Ces outils fonctionnent en synergie pour crÃ©er un Ã©cosystÃ¨me cohÃ©rent :

```
Helm â†’ DÃ©ploie des applications packagÃ©es
  â†“
kubectl â†’ GÃ¨re et debug le cluster
  â†“
Kustomize â†’ Adapte aux environnements
  â†“
Prometheus/Grafana â†’ Monitore et alerte
```

### Bonnes pratiques retenues

**Organisation**
- Structure claire des charts Helm avec tests
- Configuration Kustomize par environnement
- Dashboards et alertes par Ã©quipe

**SÃ©curitÃ©**
- Secrets dans les generators Kustomize
- RBAC pour l'accÃ¨s aux mÃ©triques
- Chiffrement des communications

**Performance**
- MÃ©triques applicatives exposÃ©es correctement
- Alertes basÃ©es sur les SLI/SLO
- Optimisation Prometheus pour haute charge

### Vers la section suivante

Vous Ãªtes maintenant Ã©quipÃ© pour :
- **Section 9.5** : GitOps et CI/CD avec Kubernetes
- IntÃ©grer ces outils dans des pipelines automatisÃ©s
- ImplÃ©menter des pratiques DevOps avancÃ©es
- GÃ©rer des dÃ©ploiements zero-downtime

La maÃ®trise de ces outils de l'Ã©cosystÃ¨me Kubernetes vous donne une base solide pour exploiter toute la puissance de l'orchestration de conteneurs en production.

â­ï¸
