🔝 Retour au [Sommaire](/SOMMAIRE.md)

# 9.4 Outils d'écosystème K8s

*Module 9 - Kubernetes et orchestration | Niveau : Avancé*

## Vue d'ensemble

L'écosystème Kubernetes s'enrichit constamment d'outils qui simplifient et améliorent l'expérience de développement et d'exploitation. Cette section explore les outils essentiels qui transforment Kubernetes d'une plateforme complexe en un environnement productif et observable.

**Pourquoi ces outils sont-ils essentiels ?**
- **Productivité** : Automatisation des tâches répétitives
- **Gestion simplifiée** : Abstraction de la complexité
- **Observabilité** : Visibilité complète sur les applications
- **Standardisation** : Bonnes pratiques intégrées

### L'écosystème CNCF

La Cloud Native Computing Foundation (CNCF) maintient un landscape d'outils organisé par catégories :

```
┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
│   Packaging     │  │   Management    │  │  Observability  │
│     (Helm)      │  │   (kubectl)     │  │ (Prometheus)    │
└─────────────────┘  └─────────────────┘  └─────────────────┘
┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
│ Configuration   │  │    Security     │  │   Networking    │
│  (Kustomize)    │  │   (Falco)       │  │   (Istio)       │
└─────────────────┘  └─────────────────┘  └─────────────────┘
```

### Outils couverts dans cette section

| Outil | Catégorie | Fonction principale | Niveau |
|-------|-----------|-------------------|--------|
| **Helm** | Package Manager | Déploiement d'applications | Essentiel |
| **kubectl** | CLI | Gestion du cluster | Fondamental |
| **Kustomize** | Configuration | Gestion des variants | Avancé |
| **Prometheus** | Monitoring | Collecte de métriques | Production |
| **Grafana** | Visualization | Dashboards et alertes | Production |

---

## 9.4.1 Helm (gestionnaire de paquets)

### Introduction à Helm

**Helm** est le gestionnaire de paquets de Kubernetes, comparable à `apt` pour Debian ou `npm` pour Node.js. Il simplifie l'installation, la mise à jour et la gestion des applications complexes.

#### Concepts fondamentaux

**Chart (Graphique)**
- **Template d'application** : Ensemble de fichiers YAML paramétrables
- **Réutilisable** : Peut être installé plusieurs fois avec des configurations différentes
- **Versionné** : Gestion des versions et des dépendances

**Release (Livraison)**
- **Instance déployée** : Une installation d'un chart dans un cluster
- **Nom unique** : Identifie l'installation dans un namespace
- **Historique** : Suivi des versions et rollback possible

**Repository (Dépôt)**
- **Collection de charts** : Hébergement centralisé
- **Public ou privé** : Artifact Hub ou repos d'entreprise
- **Versionné** : Gestion des versions de charts

### Architecture Helm

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Helm Client    │───▶│  Kubernetes     │───▶│   Application   │
│     (CLI)       │    │   API Server    │    │   Resources     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │
         ▼                       ▼
┌─────────────────┐    ┌─────────────────┐
│ Chart Repository│    │  Release Store  │
│  (Artifact Hub) │    │  (Secrets/CM)   │
└─────────────────┘    └─────────────────┘
```

### Installation de Helm

#### Installation sur Debian

```bash
# Méthode 1: Script officiel (Recommandé)
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Méthode 2: Téléchargement manuel
curl -LO https://get.helm.sh/helm-v3.13.0-linux-amd64.tar.gz
tar -zxvf helm-v3.13.0-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm

# Méthode 3: Depuis les sources (développeurs)
git clone https://github.com/helm/helm.git
cd helm
make build
sudo cp bin/helm /usr/local/bin/

# Vérifier l'installation
helm version
helm help
```

#### Configuration initiale

```bash
# Ajouter des repositories populaires
helm repo add stable https://charts.helm.sh/stable
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts

# Mettre à jour la liste des charts
helm repo update

# Lister les repos configurés
helm repo list

# Rechercher des charts
helm search repo nginx
helm search hub wordpress
```

### Utilisation de base de Helm

#### Installation d'applications

```bash
# Installation simple
helm install my-nginx bitnami/nginx

# Installation avec nom personnalisé
helm install web-server bitnami/nginx --namespace production --create-namespace

# Installation avec valeurs personnalisées
helm install my-db bitnami/postgresql \
  --set auth.postgresPassword=secretpassword \
  --set primary.persistence.size=20Gi

# Installation depuis un fichier de valeurs
helm install my-app bitnami/wordpress -f custom-values.yaml
```

#### Gestion des releases

```bash
# Lister les installations
helm list
helm list --all-namespaces

# Voir le statut d'une installation
helm status my-nginx

# Historique des déploiements
helm history my-nginx

# Mise à jour d'une installation
helm upgrade my-nginx bitnami/nginx --set service.type=LoadBalancer

# Rollback vers une version précédente
helm rollback my-nginx 1

# Désinstallation
helm uninstall my-nginx
```

### Création de Charts personnalisés

#### Structure d'un Chart

```bash
# Créer un nouveau chart
helm create my-app

# Structure générée:
my-app/
├── Chart.yaml          # Métadonnées du chart
├── values.yaml         # Valeurs par défaut
├── charts/            # Dépendances
├── templates/         # Templates Kubernetes
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── ingress.yaml
│   ├── _helpers.tpl   # Helpers et fonctions
│   └── NOTES.txt      # Instructions post-installation
└── .helmignore        # Fichiers à ignorer
```

#### Chart.yaml - Métadonnées

```yaml
# Chart.yaml
apiVersion: v2
name: my-app
description: Une application web simple
type: application
version: 0.1.0          # Version du chart
appVersion: "1.0.0"     # Version de l'application

# Informations complémentaires
keywords:
  - web
  - nginx
  - application
home: https://example.com
sources:
  - https://github.com/example/my-app
maintainers:
  - name: John Doe
    email: john@example.com
    url: https://example.com

# Dépendances
dependencies:
  - name: postgresql
    version: "12.1.0"
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
```

#### values.yaml - Configuration par défaut

```yaml
# values.yaml
# Configuration par défaut de l'application

# Réplicas et image
replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  tag: "1.20"

# Configuration du service
service:
  type: ClusterIP
  port: 80

# Configuration Ingress
ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

# Ressources
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

# Autoscaling
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80

# Sécurité
securityContext:
  runAsNonRoot: true
  runAsUser: 1001

# Stockage persistant
persistence:
  enabled: false
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 1Gi

# Configuration de l'application
config:
  database:
    host: localhost
    port: 5432
    name: myapp
  cache:
    enabled: true
    ttl: 3600

# Base de données PostgreSQL (dépendance)
postgresql:
  enabled: true
  auth:
    postgresPassword: "changeme"
    database: "myapp"
  primary:
    persistence:
      enabled: true
      size: 8Gi
```

#### Templates avec Go templating

```yaml
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "my-app.fullname" . }}
  labels:
    {{- include "my-app.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "my-app.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      labels:
        {{- include "my-app.selectorLabels" . | nindent 8 }}
    spec:
      securityContext:
        {{- toYaml .Values.securityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: DATABASE_HOST
              value: {{ .Values.config.database.host | quote }}
            - name: DATABASE_PORT
              value: {{ .Values.config.database.port | quote }}
            - name: DATABASE_NAME
              value: {{ .Values.config.database.name | quote }}
            {{- if .Values.postgresql.enabled }}
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "my-app.postgresql.secretName" . }}
                  key: postgres-password
            {{- end }}
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          {{- if .Values.persistence.enabled }}
          volumeMounts:
            - name: data
              mountPath: /data
          {{- end }}
      {{- if .Values.persistence.enabled }}
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: {{ include "my-app.fullname" . }}-data
      {{- end }}
```

#### Helpers et fonctions (_helpers.tpl)

```yaml
# templates/_helpers.tpl
{{/*
Expand the name of the chart.
*/}}
{{- define "my-app.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "my-app.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "my-app.labels" -}}
helm.sh/chart: {{ include "my-app.chart" . }}
{{ include "my-app.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "my-app.selectorLabels" -}}
app.kubernetes.io/name: {{ include "my-app.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
PostgreSQL secret name
*/}}
{{- define "my-app.postgresql.secretName" -}}
{{- if .Values.postgresql.enabled }}
{{- printf "%s-postgresql" (include "my-app.fullname" .) }}
{{- else }}
{{- .Values.externalDatabase.existingSecret | default (printf "%s-external-db" (include "my-app.fullname" .)) }}
{{- end }}
{{- end }}
```

### Tests et validation des Charts

#### Tests Helm intégrés

```yaml
# templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "my-app.fullname" . }}-test"
  labels:
    {{- include "my-app.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  restartPolicy: Never
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['{{ include "my-app.fullname" . }}:{{ .Values.service.port }}']
```

```bash
# Exécuter les tests
helm test my-app

# Voir les logs des tests
kubectl logs my-app-test

# Nettoyer les pods de test
helm test my-app --cleanup
```

#### Validation avec helm lint

```bash
# Valider la syntaxe du chart
helm lint my-app/

# Rendu à sec (dry-run)
helm install my-app ./my-app --dry-run --debug

# Template avec valeurs spécifiques
helm template my-app ./my-app --values production-values.yaml
```

### Charts avancés et patterns

#### Hooks Helm

```yaml
# templates/pre-install-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ include "my-app.fullname" . }}-pre-install"
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pre-install
        image: postgres:13
        command:
        - /bin/bash
        - -c
        - |
          echo "Running pre-install tasks..."
          # Initialisation de la base de données
          psql -h $DB_HOST -U postgres -c "CREATE DATABASE IF NOT EXISTS myapp;"
```

#### Charts avec sous-charts

```yaml
# Chart.yaml avec dépendances
dependencies:
  - name: redis
    version: "17.3.7"
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
  - name: postgresql
    version: "12.1.0"
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
```

```bash
# Mettre à jour les dépendances
helm dependency update

# Installation avec dépendances
helm install my-stack ./my-app
```

---

## 9.4.2 Kubectl avancé

### Maîtrise de kubectl

**kubectl** est l'interface en ligne de commande de Kubernetes. Au-delà des commandes de base, il offre des fonctionnalités avancées pour l'administration et le dépannage.

### Configuration et contextes

#### Gestion des contextes

```bash
# Lister les contextes disponibles
kubectl config get-contexts

# Voir la configuration actuelle
kubectl config view

# Changer de contexte
kubectl config use-context production-cluster

# Créer un nouveau contexte
kubectl config set-context dev-context \
  --cluster=development \
  --user=dev-user \
  --namespace=development

# Configurer le namespace par défaut
kubectl config set-context --current --namespace=production
```

#### Configuration multi-cluster

```yaml
# ~/.kube/config
apiVersion: v1
kind: Config
current-context: production

clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTi...
    server: https://prod-k8s.example.com:6443
  name: production
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTi...
    server: https://dev-k8s.example.com:6443
  name: development

users:
- name: admin@production
  user:
    client-certificate-data: LS0tLS1CRUdJTi...
    client-key-data: LS0tLS1CRUdJTi...
- name: dev@development
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii...

contexts:
- context:
    cluster: production
    user: admin@production
    namespace: default
  name: production
- context:
    cluster: development
    user: dev@development
    namespace: dev
  name: development
```

### Commandes avancées de kubectl

#### Requêtes et filtrage JSONPath

```bash
# Extraire des informations spécifiques avec JSONPath
kubectl get pods -o jsonpath='{.items[*].metadata.name}'

# Afficher les images utilisées
kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'

# Nœuds avec leurs versions
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.nodeInfo.kubeletVersion}{"\n"}{end}'

# Pods avec leurs adresses IP
kubectl get pods -o wide --no-headers | awk '{print $1"\t"$6}'

# Services avec leurs ports
kubectl get svc -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.ports[*].port}{"\n"}{end}'
```

#### Tri et formatage avancés

```bash
# Trier les pods par âge
kubectl get pods --sort-by=.metadata.creationTimestamp

# Trier les nœuds par nom
kubectl get nodes --sort-by=.metadata.name

# Affichage personnalisé avec colonnes
kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName

# Afficher les événements récents
kubectl get events --sort-by=.metadata.creationTimestamp --field-selector type=Warning
```

#### Labels et sélecteurs avancés

```bash
# Sélection par labels multiples
kubectl get pods -l app=nginx,version=v1.0

# Sélection par inégalité
kubectl get pods -l environment!=production

# Sélection avec opérateurs set
kubectl get pods -l 'environment in (staging,development)'
kubectl get pods -l 'tier notin (frontend,backend)'

# Ajouter des labels à plusieurs ressources
kubectl label pods -l app=nginx version=v2.0

# Supprimer un label
kubectl label pod my-pod version-
```

### Patch et mise à jour en place

#### Types de patch

```bash
# Strategic merge patch (par défaut)
kubectl patch deployment nginx -p '{"spec":{"replicas":3}}'

# JSON merge patch
kubectl patch deployment nginx --type='merge' -p '{"spec":{"replicas":3}}'

# JSON patch (RFC 6902)
kubectl patch deployment nginx --type='json' -p='[{"op": "replace", "path": "/spec/replicas", "value": 3}]'
```

#### Exemples de patch courants

```bash
# Changer l'image d'un deployment
kubectl patch deployment nginx -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","image":"nginx:1.21"}]}}}}'

# Ajouter une variable d'environnement
kubectl patch deployment myapp -p '{"spec":{"template":{"spec":{"containers":[{"name":"myapp","env":[{"name":"DEBUG","value":"true"}]}]}}}}'

# Mettre à jour les ressources
kubectl patch deployment myapp -p '{"spec":{"template":{"spec":{"containers":[{"name":"myapp","resources":{"requests":{"memory":"256Mi","cpu":"200m"}}}]}}}}'

# Ajouter une annotation
kubectl patch service nginx -p '{"metadata":{"annotations":{"nginx.ingress.kubernetes.io/rewrite-target":"/"}}}'
```

### Debug et troubleshooting

#### Inspection détaillée des ressources

```bash
# Describe avec tous les détails
kubectl describe pod my-pod | grep -A 10 -B 10 "Error"

# Voir les événements d'un pod spécifique
kubectl get events --field-selector involvedObject.name=my-pod

# Logs avancés
kubectl logs -f deployment/nginx --all-containers=true --since=1h
kubectl logs -l app=nginx --prefix=true --since=10m

# Logs précédents (conteneur crashé)
kubectl logs my-pod --previous

# Exécution de commandes dans les pods
kubectl exec -it my-pod -- /bin/bash
kubectl exec my-pod -- ps aux
kubectl exec my-pod -- netstat -tulpn
```

#### Top et métriques

```bash
# Utilisation des ressources des nœuds
kubectl top nodes

# Utilisation des ressources des pods
kubectl top pods --all-namespaces --sort-by=memory
kubectl top pods --containers --sort-by=cpu

# Métriques détaillées (si metrics-server est installé)
kubectl top pod my-pod --containers
```

### Kubectl plugins et extensions

#### Installation de krew (gestionnaire de plugins)

```bash
# Installation de krew
(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

# Ajouter krew au PATH
echo 'export PATH="${PATH}:${HOME}/.krew/bin"' >> ~/.bashrc
source ~/.bashrc
```

#### Plugins utiles

```bash
# Installer des plugins populaires
kubectl krew install ctx     # Changement rapide de contexte
kubectl krew install ns      # Changement rapide de namespace
kubectl krew install tree    # Vue arborescente des ressources
kubectl krew install neat    # Nettoyage des manifests kubectl
kubectl krew install whoami  # Informations utilisateur actuel

# Utilisation des plugins
kubectl ctx                  # Lister et changer de contexte
kubectl ns production        # Changer de namespace
kubectl tree deployment nginx  # Vue arborescente du deployment
kubectl neat pod my-pod      # Manifest nettoyé sans metadata
kubectl whoami              # Informations sur l'utilisateur
```

### Automatisation avec kubectl

#### Scripts d'administration

```bash
#!/bin/bash
# health-check.sh - Vérification de santé du cluster

echo "=== Cluster Health Check ==="

# Vérifier les nœuds
echo "Nodes status:"
kubectl get nodes --no-headers | while read node status roles age version; do
  if [[ "$status" != "Ready" ]]; then
    echo "❌ Node $node is $status"
  else
    echo "✅ Node $node is ready"
  fi
done

# Vérifier les pods système
echo -e "\nSystem pods:"
kubectl get pods -n kube-system --no-headers | while read name ready status restarts age; do
  if [[ "$status" != "Running" && "$status" != "Completed" ]]; then
    echo "❌ Pod $name is $status"
  fi
done

# Vérifier les services critiques
echo -e "\nCritical services:"
services=("kube-dns" "metrics-server" "ingress-nginx-controller")
for svc in "${services[@]}"; do
  if kubectl get svc "$svc" -A &>/dev/null; then
    echo "✅ Service $svc is present"
  else
    echo "❌ Service $svc is missing"
  fi
done

echo -e "\n=== Health Check Complete ==="
```

#### Déploiement conditionnel

```bash
#!/bin/bash
# conditional-deploy.sh

ENVIRONMENT=${1:-staging}
APP_NAME=${2:-myapp}

# Vérifier si l'environnement existe
if ! kubectl get namespace "$ENVIRONMENT" &>/dev/null; then
  echo "Creating namespace $ENVIRONMENT"
  kubectl create namespace "$ENVIRONMENT"
fi

# Déployer selon l'environnement
case $ENVIRONMENT in
  production)
    kubectl apply -f manifests/production/ -n "$ENVIRONMENT"
    kubectl scale deployment "$APP_NAME" --replicas=3 -n "$ENVIRONMENT"
    ;;
  staging)
    kubectl apply -f manifests/staging/ -n "$ENVIRONMENT"
    kubectl scale deployment "$APP_NAME" --replicas=2 -n "$ENVIRONMENT"
    ;;
  development)
    kubectl apply -f manifests/development/ -n "$ENVIRONMENT"
    kubectl scale deployment "$APP_NAME" --replicas=1 -n "$ENVIRONMENT"
    ;;
  *)
    echo "Unknown environment: $ENVIRONMENT"
    exit 1
    ;;
esac

# Attendre que le déploiement soit prêt
kubectl rollout status deployment/"$APP_NAME" -n "$ENVIRONMENT"
echo "Deployment complete!"
```

---

## 9.4.3 Kustomize

### Introduction à Kustomize

**Kustomize** est un outil de gestion de configuration pour Kubernetes qui permet de personnaliser les manifests YAML sans les modifier directement.

#### Philosophie Kustomize

**Base + Overlays**
- **Base** : Configuration commune partagée
- **Overlays** : Variantes spécifiques par environnement
- **Patches** : Modifications ciblées
- **Sans templating** : YAML natif, pas de langage de template

#### Avantages de Kustomize

**Simplicité**
- YAML standard, pas de syntaxe additionnelle
- Intégré dans kubectl depuis la version 1.14
- Courbe d'apprentissage réduite

**Maintenabilité**
- Séparation claire base/variantes
- Évite la duplication de code
- Gestion des configurations par environnement

### Structure Kustomize

#### Organisation des fichiers

```
app/
├── base/                   # Configuration de base
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── configmap.yaml
│   └── kustomization.yaml
├── overlays/              # Variantes par environnement
│   ├── development/
│   │   ├── kustomization.yaml
│   │   ├── replica-count.yaml
│   │   └── config-dev.yaml
│   ├── staging/
│   │   ├── kustomization.yaml
│   │   ├── ingress.yaml
│   │   └── resources.yaml
│   └── production/
│       ├── kustomization.yaml
│       ├── ingress.yaml
│       ├── hpa.yaml
│       └── resources.yaml
```

### Configuration de base

#### kustomization.yaml (base)

```yaml
# base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Ressources de base
resources:
  - deployment.yaml
  - service.yaml
  - configmap.yaml

# Images à gérer
images:
  - name: myapp
    newTag: latest

# ConfigMap depuis fichiers
configMapGenerator:
  - name: app-config
    files:
      - config.properties
      - log4j.xml

# Secrets depuis fichiers
secretGenerator:
  - name: app-secrets
    files:
      - database-password.txt
      - api-key.txt

# Labels communs
commonLabels:
  app: myapp
  version: v1.0

# Annotations communes
commonAnnotations:
  maintainer: "team@example.com"
  documentation: "https://docs.example.com/myapp"

# Préfixe pour toutes les ressources
namePrefix: mycompany-

# Suffixe pour toutes les ressources
nameSuffix: -v1

# Namespace pour toutes les ressources
namespace: default
```

#### Ressources de base

```yaml
# base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        env:
        - name: APP_ENV
          value: "default"
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: database_url
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: api-key
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
# base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP
---
# base/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  database_url: "postgresql://localhost:5432/myapp"
  log_level: "info"
  max_connections: "100"
  config.properties: |
    server.port=8080
    server.servlet.context-path=/api
    logging.level.com.example=INFO
    spring.datasource.url=postgresql://localhost:5432/myapp
```

### Overlays par environnement

#### Développement

```yaml
# overlays/development/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Référence à la base
resources:
  - ../../base

# Namespace spécifique
namespace: development

# Suffixe pour identifier l'environnement
nameSuffix: -dev

# Patches pour l'environnement de développement
patchesStrategicMerge:
  - replica-count.yaml
  - config-dev.yaml

# Variables d'environnement spécifiques
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: add
        path: /spec/template/spec/containers/0/env/-
        value:
          name: DEBUG
          value: "true"
      - op: replace
        path: /spec/template/spec/containers/0/env/0/value
        value: "development"

# Images avec tag spécifique
images:
  - name: myapp
    newTag: dev-latest

# ConfigMap avec des valeurs de dev
configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - log_level=debug
      - max_connections=10
      - database_url=postgresql://dev-db:5432/myapp_dev

# Labels spécifiques à l'environnement
commonLabels:
  environment: development
  owner: dev-team
```

```yaml
# overlays/development/replica-count.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
```

```yaml
# overlays/development/config-dev.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  config.properties: |
    server.port=8080
    server.servlet.context-path=/api
    logging.level.com.example=DEBUG
    logging.level.org.springframework=DEBUG
    spring.datasource.url=postgresql://dev-db:5432/myapp_dev
    spring.jpa.show-sql=true
    management.endpoints.web.exposure.include=*
```

#### Staging

```yaml
# overlays/staging/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
  - ingress.yaml

namespace: staging
nameSuffix: -staging

patchesStrategicMerge:
  - resources.yaml

images:
  - name: myapp
    newTag: staging-v1.2.0

configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - log_level=info
      - max_connections=50
      - database_url=postgresql://staging-db:5432/myapp_staging

commonLabels:
  environment: staging
  version: v1.2.0

# Répliques pour staging
replicas:
  - name: myapp
    count: 2
```

```yaml
# overlays/staging/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-staging
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - staging.myapp.example.com
    secretName: myapp-staging-tls
  rules:
  - host: staging.myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: myapp-service
            port:
              number: 80
```

```yaml
# overlays/staging/resources.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: myapp
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
```

#### Production

```yaml
# overlays/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - ../../base
  - ingress.yaml
  - hpa.yaml
  - pdb.yaml

namespace: production
nameSuffix: -prod

patchesStrategicMerge:
  - resources.yaml
  - security.yaml

images:
  - name: myapp
    newTag: v1.2.0

configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - log_level=warn
      - max_connections=200
      - database_url=postgresql://prod-db:5432/myapp_production

secretGenerator:
  - name: app-secrets
    behavior: merge
    literals:
      - database_password=production_secret_password
      - api_key=prod_api_key_here

commonLabels:
  environment: production
  version: v1.2.0
  criticality: high

replicas:
  - name: myapp
    count: 5

# Patches JSON pour modifications précises
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: add
        path: /spec/template/spec/containers/0/env/-
        value:
          name: NEW_RELIC_LICENSE_KEY
          valueFrom:
            secretKeyRef:
              name: monitoring-secrets
              key: newrelic-key
```

```yaml
# overlays/production/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

```yaml
# overlays/production/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
```

### Patches avancés avec Kustomize

#### Strategic Merge Patches

```yaml
# patch-strategic-merge.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  template:
    spec:
      containers:
      - name: myapp
        env:
        - name: ADDITIONAL_VAR
          value: "additional_value"
        # Cette variable sera ajoutée à la liste existante
      - name: sidecar
        image: sidecar:latest
        # Ce conteneur sera ajouté aux conteneurs existants
```

#### JSON Patches (RFC 6902)

```yaml
# kustomization.yaml avec JSON patches
patches:
  - target:
      kind: Deployment
      name: myapp
    patch: |-
      - op: add
        path: /spec/template/spec/containers/0/volumeMounts/-
        value:
          name: cache-volume
          mountPath: /tmp/cache
      - op: add
        path: /spec/template/spec/volumes/-
        value:
          name: cache-volume
          emptyDir: {}
      - op: replace
        path: /spec/replicas
        value: 3
```

#### Patches avec ciblage précis

```yaml
# patch-with-selectors.yaml
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: myapp
      namespace: production
    patch: |-
      - op: add
        path: /spec/template/metadata/annotations/prometheus.io~1scrape
        value: "true"
      - op: add
        path: /spec/template/metadata/annotations/prometheus.io~1port
        value: "9090"
```

### Générateurs Kustomize

#### ConfigMap Generator

```yaml
# kustomization.yaml
configMapGenerator:
  - name: app-config
    # Depuis des littéraux
    literals:
      - database_host=localhost
      - database_port=5432
      - debug=false

    # Depuis des fichiers
    files:
      - application.properties
      - log4j2.xml

    # Depuis des variables d'environnement
    envs:
      - app.env

    # Options avancées
    options:
      # Désactiver le suffixe de hash
      disableNameSuffixHash: true
      # Labels et annotations
      labels:
        config-type: application
      annotations:
        config-version: "1.0"
```

#### Secret Generator

```yaml
secretGenerator:
  - name: app-secrets
    type: Opaque
    literals:
      - username=admin
      - password=supersecret
    files:
      - tls.crt
      - tls.key
    options:
      disableNameSuffixHash: false
      labels:
        secret-type: application
      annotations:
        secret-version: "1.0"

  # Secret TLS spécialisé
  - name: tls-secret
    type: kubernetes.io/tls
    files:
      - tls.crt=server.crt
      - tls.key=server.key
```

### Transformateurs Kustomize

#### Images Transformer

```yaml
# Gestion des images
images:
  - name: nginx
    newName: my-registry.com/nginx
    newTag: 1.21.0
  - name: myapp
    newTag: v2.0.0
  - name: postgres
    digest: sha256:b5b2b2c50720...
```

#### Replicas Transformer

```yaml
# Gestion des répliques
replicas:
  - name: frontend
    count: 3
  - name: backend
    count: 5
```

#### Namespace Transformer

```yaml
# Application d'un namespace à toutes les ressources
namespace: my-namespace

# Ou via patch pour plus de contrôle
patches:
  - target:
      kind: ".*"
    patch: |-
      - op: replace
        path: /metadata/namespace
        value: specific-namespace
```

### Utilisation de Kustomize

#### Commandes de base

```bash
# Aperçu des manifests générés
kubectl kustomize overlays/development

# Application directe
kubectl apply -k overlays/development

# Sauvegarde des manifests générés
kubectl kustomize overlays/production > production-manifests.yaml

# Validation avant application
kubectl kustomize overlays/staging | kubectl apply --dry-run=client -f -

# Suppression des ressources
kubectl delete -k overlays/development
```

#### Intégration avec Helm

```yaml
# kustomization.yaml utilisant un Chart Helm
helmCharts:
  - name: prometheus
    repo: https://prometheus-community.github.io/helm-charts
    version: 15.10.1
    releaseName: monitoring
    namespace: monitoring
    valuesInline:
      server:
        persistentVolume:
          size: 20Gi
      alertmanager:
        enabled: true

resources:
  - prometheus-config-override.yaml
```

### Patterns avancés

#### Multi-tenant avec Kustomize

```
multi-tenant/
├── base/
│   └── kustomization.yaml
├── tenants/
│   ├── tenant-a/
│   │   ├── kustomization.yaml
│   │   └── tenant-specific.yaml
│   ├── tenant-b/
│   │   ├── kustomization.yaml
│   │   └── tenant-specific.yaml
│   └── tenant-c/
│       ├── kustomization.yaml
│       └── tenant-specific.yaml
└── environments/
    ├── staging/
    │   └── kustomization.yaml
    └── production/
        └── kustomization.yaml
```

```yaml
# environments/production/kustomization.yaml
resources:
  - ../../tenants/tenant-a
  - ../../tenants/tenant-b
  - ../../tenants/tenant-c

namespace: production

commonLabels:
  environment: production
```

#### GitOps avec Kustomize

```yaml
# .github/workflows/deploy.yml
name: Deploy Application
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Setup Kustomize
      run: |
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/

    - name: Update image tag
      run: |
        cd overlays/production
        kustomize edit set image myapp=myapp:${GITHUB_SHA::8}

    - name: Deploy to production
      run: |
        kubectl kustomize overlays/production | kubectl apply -f -
```

---

## 9.4.4 Monitoring (Prometheus, Grafana)

### Introduction au monitoring Kubernetes

Le monitoring est crucial pour maintenir la santé et les performances d'un cluster Kubernetes. **Prometheus** et **Grafana** forment le stack de monitoring de référence dans l'écosystème cloud-native.

#### Les trois piliers de l'observabilité

**Metrics (Métriques)**
- Données numériques temporelles
- CPU, mémoire, requêtes par seconde
- Collectées par Prometheus

**Logs (Journaux)**
- Événements textuels horodatés
- Messages d'erreur, traces d'exécution
- Gérés par ELK/Fluentd (hors scope ici)

**Traces (Traçage)**
- Suivi des requêtes distribuées
- Latence, chemins d'exécution
- Jaeger, Zipkin (hors scope ici)

### Architecture du monitoring

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Applications  │───▶│   Prometheus    │───▶│    Grafana      │
│   (metrics)     │    │   (collect)     │    │   (visualize)   │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Node Exporter  │    │  AlertManager   │    │   Dashboard     │
│   (système)     │    │    (alertes)    │    │   (graphs)      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Installation de Prometheus

#### Installation avec Helm (Recommandée)

```bash
# Ajouter le repository Prometheus
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Créer le namespace
kubectl create namespace monitoring

# Installer le stack complet (Prometheus + Grafana + AlertManager)
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values prometheus-values.yaml
```

#### Configuration personnalisée

```yaml
# prometheus-values.yaml
# Configuration pour kube-prometheus-stack

prometheus:
  prometheusSpec:
    # Rétention des données
    retention: 15d
    retentionSize: 50GB

    # Stockage persistant
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi

    # Ressources
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
      limits:
        memory: 2Gi
        cpu: 1000m

    # Règles d'alertes supplémentaires
    additionalPrometheusRulesMap:
      custom-rules:
        groups:
        - name: custom.rules
          rules:
          - alert: HighPodCPU
            expr: rate(container_cpu_usage_seconds_total[5m]) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} high CPU usage"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has high CPU usage"

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    # Configuration des alertes
    config:
      global:
        smtp_smarthost: 'smtp.example.com:587'
        smtp_from: 'alerts@example.com'

      route:
        group_by: ['alertname', 'cluster']
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 1h
        receiver: 'default-receiver'
        routes:
        - match:
            severity: critical
          receiver: 'critical-receiver'

      receivers:
      - name: 'default-receiver'
        email_configs:
        - to: 'team@example.com'
          subject: '[{{ .Status }}] {{ .GroupLabels.alertname }}'
          body: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}

      - name: 'critical-receiver'
        email_configs:
        - to: 'oncall@example.com'
          subject: '[CRITICAL] {{ .GroupLabels.alertname }}'

grafana:
  # Accès admin
  adminPassword: admin123

  # Persistance
  persistence:
    enabled: true
    storageClassName: local-storage
    size: 10Gi

  # Dashboards par défaut
  defaultDashboardsEnabled: true

  # Dashboards personnalisés
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'custom-dashboards'
        orgId: 1
        folder: 'Custom'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/custom

  # Sources de données supplémentaires
  additionalDataSources:
  - name: Loki
    type: loki
    url: http://loki:3100
    access: proxy
    isDefault: false

# Node Exporter pour métriques système
nodeExporter:
  enabled: true

# kube-state-metrics pour métriques Kubernetes
kubeStateMetrics:
  enabled: true

# Service Monitors personnalisés
additionalServiceMonitors:
  - name: myapp-monitor
    selector:
      matchLabels:
        app: myapp
    endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
```

### Configuration de Prometheus

#### Prometheus.yml de base

```yaml
# Configuration Prometheus standalone (si pas Helm)
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"
  - "recording_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Prometheus lui-même
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # API Server Kubernetes
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
    - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      action: keep
      regex: default;kubernetes;https

  # Kubelet et cAdvisor
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
    - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - action: labelmap
      regex: __meta_kubernetes_node_label_(.+)

  # Pods avec annotation prometheus.io/scrape
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
      target_label: __address__
    - action: labelmap
      regex: __meta_kubernetes_pod_label_(.+)
```

### Règles d'alerte Prometheus

#### Règles d'alerte de base

```yaml
# alert_rules.yml
groups:
- name: kubernetes-alerts
  rules:

  # Nœud down
  - alert: NodeDown
    expr: up{job="node-exporter"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.instance }} is down"
      description: "Node {{ $labels.instance }} has been down for more than 5 minutes"

  # Utilisation CPU élevée
  - alert: HighCpuUsage
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is above 80% for 5 minutes on {{ $labels.instance }}"

  # Utilisation mémoire élevée
  - alert: HighMemoryUsage
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage on {{ $labels.instance }}"
      description: "Memory usage is above 85% on {{ $labels.instance }}"

  # Espace disque faible
  - alert: DiskSpaceLow
    expr: (1 - node_filesystem_avail_bytes{fstype!="tmpfs"} / node_filesystem_size_bytes) * 100 > 90
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Disk space low on {{ $labels.instance }}"
      description: "Disk usage is above 90% on {{ $labels.instance }} mount {{ $labels.mountpoint }}"

  # Pod crashant
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.pod }} is crash looping"
      description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently"

  # Deployment pas prêt
  - alert: DeploymentReplicasNotReady
    expr: kube_deployment_spec_replicas != kube_deployment_status_ready_replicas
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Deployment {{ $labels.deployment }} has unready replicas"
      description: "Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has {{ $value }} unready replicas"
```

### Configuration de Grafana

#### Dashboards Kubernetes essentiels

Grafana peut être configuré avec des dashboards prêts à l'emploi :

**Dashboards recommandés** (IDs Grafana.com) :
- **315** : Kubernetes cluster monitoring
- **747** : Kubernetes Deployment metrics
- **6417** : Kubernetes cluster overview
- **8588** : Node Exporter Full
- **1860** : Node Exporter Full avec alertes

#### Import de dashboards

```bash
# Via l'interface Grafana : + → Import → ID du dashboard

# Ou via API
curl -X POST \
  http://admin:admin123@grafana.monitoring.svc.cluster.local:3000/api/dashboards/db \
  -H 'Content-Type: application/json' \
  -d '{
    "dashboard": {
      "id": null,
      "title": "Custom Kubernetes Dashboard",
      "panels": [...]
    },
    "overwrite": true
  }'
```

#### Dashboard personnalisé pour application

```json
{
  "dashboard": {
    "id": null,
    "title": "MyApp Monitoring",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{job=\"myapp\"}[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"myapp\"}[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket{job=\"myapp\"}[5m]))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds",
            "min": 0
          }
        ]
      }
    ]
  }
}
```

### Exposition des métriques dans les applications

#### Métriques dans une application Go

```go
// main.go
package main

import (
    "net/http"
    "time"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
    httpRequestsTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "endpoint", "status_code"},
    )

    httpRequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "http_request_duration_seconds",
            Help: "HTTP request duration in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )
)

func init() {
    prometheus.MustRegister(httpRequestsTotal)
    prometheus.MustRegister(httpRequestDuration)
}

func instrumentHandler(handler http.HandlerFunc, endpoint string) http.HandlerFunc {
    return func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()

        // Wrapper pour capturer le status code
        wrapped := &responseWriter{ResponseWriter: w, statusCode: 200}

        handler(wrapped, r)

        duration := time.Since(start).Seconds()

        httpRequestsTotal.With(prometheus.Labels{
            "method":      r.Method,
            "endpoint":    endpoint,
            "status_code": fmt.Sprintf("%d", wrapped.statusCode),
        }).Inc()

        httpRequestDuration.With(prometheus.Labels{
            "method":   r.Method,
            "endpoint": endpoint,
        }).Observe(duration)
    }
}

type responseWriter struct {
    http.ResponseWriter
    statusCode int
}

func (rw *responseWriter) WriteHeader(code int) {
    rw.statusCode = code
    rw.ResponseWriter.WriteHeader(code)
}

func main() {
    http.HandleFunc("/", instrumentHandler(homeHandler, "/"))
    http.HandleFunc("/api/users", instrumentHandler(usersHandler, "/api/users"))
    http.Handle("/metrics", promhttp.Handler())

    log.Fatal(http.ListenAndServe(":8080", nil))
}

func homeHandler(w http.ResponseWriter, r *http.Request) {
    w.WriteHeader(http.StatusOK)
    fmt.Fprintf(w, "Hello World!")
}

func usersHandler(w http.ResponseWriter, r *http.Request) {
    // Simulation d'une erreur occasionnelle
    if rand.Float32() < 0.1 {
        w.WriteHeader(http.StatusInternalServerError)
        fmt.Fprintf(w, "Internal Server Error")
        return
    }

    w.WriteHeader(http.StatusOK)
    fmt.Fprintf(w, `{"users": ["alice", "bob", "charlie"]}`)
}
```

#### Métriques dans une application Java (Spring Boot)

```java
// Application.java
@SpringBootApplication
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

    @Bean
    public MeterRegistryCustomizer<MeterRegistry> metricsCommonTags() {
        return registry -> registry.config().commonTags("application", "myapp");
    }
}

// UserController.java
@RestController
@RequestMapping("/api/users")
public class UserController {

    private final Counter userRequestsCounter;
    private final Timer userRequestsTimer;
    private final Gauge activeUsersGauge;

    public UserController(MeterRegistry meterRegistry) {
        this.userRequestsCounter = Counter.builder("user_requests_total")
            .description("Total user requests")
            .tag("endpoint", "/api/users")
            .register(meterRegistry);

        this.userRequestsTimer = Timer.builder("user_requests_duration_seconds")
            .description("User request duration")
            .register(meterRegistry);

        this.activeUsersGauge = Gauge.builder("active_users")
            .description("Number of active users")
            .register(meterRegistry, this, UserController::getActiveUserCount);
    }

    @GetMapping
    public ResponseEntity<List<User>> getUsers() {
        return userRequestsTimer.recordCallable(() -> {
            userRequestsCounter.increment();
            // Logique métier
            List<User> users = userService.findAll();
            return ResponseEntity.ok(users);
        });
    }

    private Double getActiveUserCount() {
        return (double) userService.getActiveUserCount();
    }
}
```

#### Configuration Kubernetes pour exposition des métriques

```yaml
# deployment-with-metrics.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
      annotations:
        # Annotations pour Prometheus scraping
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "9090"
    spec:
      containers:
      - name: myapp
        image: myapp:v1.0.0
        ports:
        - name: http
          containerPort: 8080
        - name: metrics
          containerPort: 9090
        # Health checks utilisant les métriques
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 9090
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 9090
          initialDelaySeconds: 5
          periodSeconds: 5
        env:
        - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE
          value: "health,info,metrics,prometheus"
        - name: MANAGEMENT_SERVER_PORT
          value: "9090"
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  labels:
    app: myapp
spec:
  selector:
    app: myapp
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: metrics
    port: 9090
    targetPort: 9090
```

### ServiceMonitor pour découverte automatique

```yaml
# myapp-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp-monitor
  namespace: monitoring
  labels:
    app: myapp
    release: kube-prometheus-stack
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    # Relabeling pour ajouter des labels personnalisés
    relabelings:
    - sourceLabels: [__meta_kubernetes_service_name]
      targetLabel: service
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: kubernetes_namespace
  namespaceSelector:
    matchNames:
    - default
    - production
```

### Alerting avancé

#### Règles d'alerte spécifiques aux applications

```yaml
# application-alerts.yml
groups:
- name: application-alerts
  rules:

  # Taux d'erreur élevé
  - alert: HighErrorRate
    expr: |
      (
        rate(http_requests_total{status_code=~"5.."}[5m])
        /
        rate(http_requests_total[5m])
      ) * 100 > 5
    for: 5m
    labels:
      severity: critical
      team: backend
    annotations:
      summary: "High error rate for {{ $labels.service }}"
      description: "Error rate is {{ $value }}% for service {{ $labels.service }}"
      runbook_url: "https://wiki.example.com/runbooks/high-error-rate"

  # Latence élevée
  - alert: HighLatency
    expr: |
      histogram_quantile(0.95,
        rate(http_request_duration_seconds_bucket[5m])
      ) > 1
    for: 10m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "High latency for {{ $labels.service }}"
      description: "95th percentile latency is {{ $value }}s for {{ $labels.service }}"

  # Pas de trafic (service potentiellement down)
  - alert: NoTraffic
    expr: |
      rate(http_requests_total[5m]) == 0
    for: 5m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "No traffic to {{ $labels.service }}"
      description: "Service {{ $labels.service }} is not receiving any requests"

  # Utilisation mémoire application élevée
  - alert: HighApplicationMemory
    expr: |
      (
        container_memory_usage_bytes{pod=~"myapp-.*"}
        /
        container_spec_memory_limit_bytes{pod=~"myapp-.*"}
      ) * 100 > 85
    for: 5m
    labels:
      severity: warning
      team: backend
    annotations:
      summary: "High memory usage for pod {{ $labels.pod }}"
      description: "Memory usage is {{ $value }}% for pod {{ $labels.pod }}"
```

#### Configuration AlertManager avancée

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'password'

# Templates personnalisés
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routage des alertes
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default-receiver'

  routes:
  # Alertes critiques vers l'équipe d'astreinte
  - match:
      severity: critical
    receiver: 'oncall-receiver'
    group_wait: 0s
    repeat_interval: 5m
    continue: true

  # Alertes par équipe
  - match:
      team: backend
    receiver: 'backend-team'
    continue: true

  - match:
      team: frontend
    receiver: 'frontend-team'
    continue: true

  # Alertes de développement (moins urgentes)
  - match:
      environment: development
    receiver: 'dev-team'
    group_interval: 1h
    repeat_interval: 24h

# Inhibition des alertes (éviter le spam)
inhibit_rules:
  # Si un nœud est down, ne pas alerter pour les pods sur ce nœud
  - source_match:
      alertname: NodeDown
    target_match:
      alertname: PodCrashLooping
    equal: ['instance']

  # Si un service est down, ne pas alerter pour la latence
  - source_match:
      alertname: NoTraffic
    target_match:
      alertname: HighLatency
    equal: ['service']

receivers:
- name: 'default-receiver'
  email_configs:
  - to: 'alerts@example.com'
    subject: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}'
    html: '{{ template "email.default.html" . }}'

- name: 'oncall-receiver'
  # Email pour astreinte
  email_configs:
  - to: 'oncall@example.com'
    subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
    html: '{{ template "email.critical.html" . }}'

  # Slack pour notifications immédiates
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts-critical'
    title: 'Critical Alert: {{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    color: 'danger'

  # PagerDuty pour escalade
  pagerduty_configs:
  - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
    description: '{{ .GroupLabels.alertname }}'

- name: 'backend-team'
  email_configs:
  - to: 'backend-team@example.com'
    subject: '[Backend] {{ .GroupLabels.alertname }}'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#backend-alerts'
```

### Dashboards Grafana personnalisés

#### Dashboard SLI/SLO (Service Level Indicators/Objectives)

```json
{
  "dashboard": {
    "title": "Service SLI/SLO Dashboard",
    "panels": [
      {
        "title": "Availability SLI",
        "type": "stat",
        "targets": [
          {
            "expr": "(\n  (\n    sum(rate(http_requests_total{job=\"myapp\"}[7d])) -\n    sum(rate(http_requests_total{job=\"myapp\",status_code=~\"5..\"}[7d]))\n  ) /\n  sum(rate(http_requests_total{job=\"myapp\"}[7d]))\n) * 100",
            "legendFormat": "Availability %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "min": 0,
            "max": 100,
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 95},
                {"color": "green", "value": 99.9}
              ]
            }
          }
        }
      },
      {
        "title": "Error Budget Remaining",
        "type": "bargauge",
        "targets": [
          {
            "expr": "(\n  1 - (\n    1 - 0.999  # 99.9% SLO\n  ) * 7 * 24 * 3600 / (\n    7 * 24 * 3600 - (\n      sum(increase(http_requests_total{job=\"myapp\",status_code=~\"5..\"}[7d]))\n    )\n  )\n) * 100",
            "legendFormat": "Error Budget %"
          }
        ]
      }
    ]
  }
}
```

### Monitoring des ressources Kubernetes

#### Métriques essentielles à surveiller

```yaml
# kubernetes-resource-alerts.yml
groups:
- name: kubernetes-resources
  rules:

  # Quota de namespace approchant
  - alert: NamespaceQuotaNearLimit
    expr: |
      (
        kube_resourcequota{resource="requests.memory", type="used"}
        /
        kube_resourcequota{resource="requests.memory", type="hard"}
      ) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Namespace {{ $labels.namespace }} memory quota near limit"
      description: "Memory quota usage is {{ $value }}% in namespace {{ $labels.namespace }}"

  # Nœud avec trop de pods
  - alert: NodeHighPodCount
    expr: |
      (
        kube_node_status_allocatable{resource="pods"}
        -
        kube_node_status_capacity{resource="pods"}
      ) < 10
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Node {{ $labels.node }} is running out of pod capacity"
      description: "Node {{ $labels.node }} has less than 10 pods available"

  # PersistentVolume bientôt plein
  - alert: PVCAlmostFull
    expr: |
      (
        kubelet_volume_stats_used_bytes
        /
        kubelet_volume_stats_capacity_bytes
      ) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} almost full"
      description: "PVC usage is {{ $value }}% in namespace {{ $labels.namespace }}"
```

### Backup et restauration des configurations

#### Script de backup Prometheus/Grafana

```bash
#!/bin/bash
# backup-monitoring.sh

BACKUP_DIR="/backup/monitoring/$(date +%Y%m%d-%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "Backing up monitoring stack..."

# Backup des ConfigMaps Prometheus
kubectl get configmap -n monitoring -o yaml > "$BACKUP_DIR/prometheus-configs.yaml"

# Backup des règles d'alertes
kubectl get prometheusrule -n monitoring -o yaml > "$BACKUP_DIR/prometheus-rules.yaml"

# Backup des dashboards Grafana
kubectl get configmap -n monitoring -l grafana_dashboard=1 -o yaml > "$BACKUP_DIR/grafana-dashboards.yaml"

# Backup des données Grafana (si accessible)
POD=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n monitoring "$POD" -- tar czf - /var/lib/grafana > "$BACKUP_DIR/grafana-data.tar.gz"

# Backup des données Prometheus (snapshot)
PROMETHEUS_POD=$(kubectl get pod -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n monitoring "$PROMETHEUS_POD" -- \
  curl -X POST http://localhost:9090/api/v1/admin/tsdb/snapshot

echo "Backup completed in $BACKUP_DIR"
tar czf "$BACKUP_DIR.tar.gz" -C "$(dirname $BACKUP_DIR)" "$(basename $BACKUP_DIR)"
rm -rf "$BACKUP_DIR"
echo "Backup archive: $BACKUP_DIR.tar.gz"
```

### Optimisation des performances

#### Configuration Prometheus pour haute charge

```yaml
# prometheus-performance.yaml
prometheus:
  prometheusSpec:
    # Paramètres de performance
    scrapeInterval: 30s
    evaluationInterval: 30s

    # Optimisations mémoire
    resources:
      requests:
        memory: 4Gi
        cpu: 1000m
      limits:
        memory: 8Gi
        cpu: 2000m

    # Configuration TSDB
    tsdb:
      # Rétention plus aggressive pour les séries peu utilisées
      retentionSize: "100GB"

      # Compression plus agressive
      compression: "snappy"

      # Optimisations WAL
      walCompression: true

    # Limitations pour éviter la surcharge
    query:
      maxConcurrency: 20
      maxSamples: 50000000
      timeout: 2m

    # Optimisations réseau
    scrapeTimeout: 10s

    # Règles de découverte de services optimisées
    serviceDiscoveryConfigs:
      - kubernetes_sd_configs:
        - role: endpoints
          # Limitation du taux de découverte
          refresh_interval: 60s
```

#### Métriques de performance à surveiller

```yaml
# monitoring-performance-alerts.yml
groups:
- name: monitoring-performance
  rules:

  # Prometheus ingestion rate élevée
  - alert: PrometheusHighIngestionRate
    expr: rate(prometheus_samples_ingested_total[5m]) > 100000
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus high sample ingestion rate"
      description: "Prometheus is ingesting samples at {{ $value }} samples/sec"

  # Grafana réponse lente
  - alert: GrafanaSlowQueries
    expr: histogram_quantile(0.99, rate(grafana_http_request_duration_seconds_bucket[5m])) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Grafana slow query response time"
      description: "99th percentile response time is {{ $value }}s"
```

---

## Conclusion de la section 9.4

Cette section a exploré les outils essentiels de l'écosystème Kubernetes qui transforment un cluster de base en plateforme d'exploitation moderne et observable.

### Récapitulatif des outils couverts

#### Helm - Gestionnaire de paquets
✅ **Maîtrisé** : Installation, création de charts, gestion des releases
✅ **Patterns avancés** : Hooks, dépendances, tests intégrés
✅ **Production** : Versionning, rollbacks, configuration multi-environnements

#### kubectl avancé
✅ **Efficacité** : Contextes, JSONPath, patches en place
✅ **Debugging** : Logs, événements, métriques
✅ **Extensibilité** : Plugins krew, scripts d'automatisation

#### Kustomize
✅ **Configuration** : Base + overlays, gestion multi-environnements
✅ **Flexibilité** : Patches, générateurs, transformateurs
✅ **GitOps ready** : Configuration declarative, versioning

#### Monitoring avec Prometheus/Grafana
✅ **Observabilité complète** : Métriques, alertes, dashboards
✅ **Applications** : Instrumentation, SLI/SLO
✅ **Production** : Performance, backup, optimisation

### Synergie des outils

Ces outils fonctionnent en synergie pour créer un écosystème cohérent :

```
Helm → Déploie des applications packagées
  ↓
kubectl → Gère et debug le cluster
  ↓
Kustomize → Adapte aux environnements
  ↓
Prometheus/Grafana → Monitore et alerte
```

### Bonnes pratiques retenues

**Organisation**
- Structure claire des charts Helm avec tests
- Configuration Kustomize par environnement
- Dashboards et alertes par équipe

**Sécurité**
- Secrets dans les generators Kustomize
- RBAC pour l'accès aux métriques
- Chiffrement des communications

**Performance**
- Métriques applicatives exposées correctement
- Alertes basées sur les SLI/SLO
- Optimisation Prometheus pour haute charge

### Vers la section suivante

Vous êtes maintenant équipé pour :
- **Section 9.5** : GitOps et CI/CD avec Kubernetes
- Intégrer ces outils dans des pipelines automatisés
- Implémenter des pratiques DevOps avancées
- Gérer des déploiements zero-downtime

La maîtrise de ces outils de l'écosystème Kubernetes vous donne une base solide pour exploiter toute la puissance de l'orchestration de conteneurs en production.

⏭️
