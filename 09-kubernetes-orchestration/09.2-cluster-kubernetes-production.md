üîù Retour au [Sommaire](/SOMMAIRE.md)

# 9.2 Cluster Kubernetes production

*Module 9 - Kubernetes et orchestration | Niveau : Avanc√©*

## Vue d'ensemble

Apr√®s avoir appris les bases de Kubernetes avec une installation simple, cette section vous guide dans la cr√©ation d'un cluster Kubernetes adapt√© √† un environnement de production. Un cluster de production n√©cessite des consid√©rations avanc√©es en termes de haute disponibilit√©, de r√©seau, de stockage et d'exposition des services.

**Diff√©rences cl√©s production vs d√©veloppement :**
- **Haute disponibilit√©** : Control plane redondant, pas de point de d√©faillance unique
- **S√©curit√© renforc√©e** : Isolation r√©seau, chiffrement, authentification forte
- **Performances optimis√©es** : R√©seaux rapides, stockage performant, monitoring
- **Scalabilit√©** : Capacit√© √† g√©rer des centaines de n≈ìuds et milliers de pods
- **Observabilit√©** : Monitoring, logging, alerting complets

---

## 9.2.1 Configuration multi-n≈ìuds HA

### Concepts de haute disponibilit√©

La **haute disponibilit√© (HA)** garantit que votre cluster reste op√©rationnel m√™me en cas de d√©faillance de composants individuels.

#### Composants critiques √† rendre redondants :

**Control Plane HA**
- Plusieurs masters (g√©n√©ralement 3 ou 5)
- Load balancer devant l'API Server
- etcd en cluster (nombre impair de n≈ìuds)
- R√©plication automatique des composants

**Worker Nodes HA**
- R√©partition des pods sur plusieurs n≈ìuds
- Zones de disponibilit√© diff√©rentes
- Auto-scaling des n≈ìuds

### Architecture HA recommand√©e

```
                    Load Balancer
                    (HAProxy/NGINX)
                         |
        +----------------+----------------+
        |                |                |
   Master 1         Master 2         Master 3
   (Control         (Control         (Control
    Plane)           Plane)           Plane)
        |                |                |
    +---+---+        +---+---+        +---+---+
    | etcd1 |        | etcd2 |        | etcd3 |
    +-------+        +-------+        +-------+
                         |
        +----------------+----------------+
        |                |                |
   Worker 1         Worker 2         Worker N
   (Pods)           (Pods)           (Pods)
```

### Pr√©paration des n≈ìuds

#### Configuration r√©seau

Pour un cluster HA, configurez des adresses IP statiques :

```bash
# Sur chaque n≈ìud, configurer /etc/netplan/00-installer-config.yaml
network:
  version: 2
  ethernets:
    enp0s3:
      dhcp4: false
      addresses:
        - 192.168.1.10/24  # Master 1
        # - 192.168.1.11/24  # Master 2
        # - 192.168.1.12/24  # Master 3
        # - 192.168.1.20/24  # Worker 1
        # - 192.168.1.21/24  # Worker 2
      gateway4: 192.168.1.1
      nameservers:
        addresses:
          - 8.8.8.8
          - 1.1.1.1

# Appliquer la configuration
sudo netplan apply
```

#### Configuration des hostnames

```bash
# Sur chaque n≈ìud, d√©finir le hostname
sudo hostnamectl set-hostname k8s-master1  # Adapter selon le n≈ìud

# Configurer /etc/hosts sur tous les n≈ìuds
cat <<EOF | sudo tee -a /etc/hosts
192.168.1.10 k8s-master1
192.168.1.11 k8s-master2
192.168.1.12 k8s-master3
192.168.1.20 k8s-worker1
192.168.1.21 k8s-worker2
192.168.1.100 k8s-lb
EOF
```

### Installation du Load Balancer

Un load balancer est essentiel pour distribuer le trafic entre les masters.

#### Option 1 : HAProxy (Recommand√©)

```bash
# Sur le serveur load balancer (ou n≈ìud d√©di√©)
sudo apt update && sudo apt install -y haproxy

# Configuration /etc/haproxy/haproxy.cfg
sudo tee /etc/haproxy/haproxy.cfg <<EOF
global
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
    errorfile 408 /etc/haproxy/errors/408.http
    errorfile 500 /etc/haproxy/errors/500.http
    errorfile 502 /etc/haproxy/errors/502.http
    errorfile 503 /etc/haproxy/errors/503.http
    errorfile 504 /etc/haproxy/errors/504.http

# API Server Load Balancer
frontend k8s-api
    bind *:6443
    mode tcp
    default_backend k8s-masters

backend k8s-masters
    mode tcp
    balance roundrobin
    option tcp-check
    server k8s-master1 192.168.1.10:6443 check
    server k8s-master2 192.168.1.11:6443 check
    server k8s-master3 192.168.1.12:6443 check

# Stats interface (optionnel)
listen stats
    bind *:8080
    stats enable
    stats uri /stats
    stats refresh 30s
EOF

# Red√©marrer HAProxy
sudo systemctl restart haproxy
sudo systemctl enable haproxy
```

#### Option 2 : NGINX (Alternative)

```bash
# Installation
sudo apt install -y nginx

# Configuration /etc/nginx/nginx.conf
events {
    worker_connections 1024;
}

stream {
    upstream k8s_masters {
        server 192.168.1.10:6443;
        server 192.168.1.11:6443;
        server 192.168.1.12:6443;
    }

    server {
        listen 6443;
        proxy_pass k8s_masters;
        proxy_timeout 3s;
        proxy_responses 1;
    }
}

# Red√©marrer NGINX
sudo systemctl restart nginx
sudo systemctl enable nginx
```

### Initialisation du cluster HA

#### √âtape 1 : Premier master

```bash
# Sur le premier master (k8s-master1)
sudo kubeadm init \
  --control-plane-endpoint "k8s-lb:6443" \
  --upload-certs \
  --pod-network-cidr=10.244.0.0/16 \
  --service-cidr=10.96.0.0/12

# Sauvegarder les tokens affich√©s !
# - Token pour joindre les masters
# - Token pour joindre les workers
# - Certificate key pour les masters
```

#### √âtape 2 : Configuration kubectl

```bash
# Sur le premier master
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# V√©rifier l'installation
kubectl get nodes
kubectl cluster-info
```

#### √âtape 3 : Ajout des masters suppl√©mentaires

```bash
# Sur k8s-master2 et k8s-master3
sudo kubeadm join k8s-lb:6443 \
  --token <TOKEN> \
  --discovery-token-ca-cert-hash sha256:<HASH> \
  --control-plane \
  --certificate-key <CERTIFICATE-KEY>

# Configurer kubectl sur chaque master
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### √âtape 4 : Ajout des workers

```bash
# Sur chaque worker
sudo kubeadm join k8s-lb:6443 \
  --token <TOKEN> \
  --discovery-token-ca-cert-hash sha256:<HASH>
```

### V√©rification de la HA

```bash
# V√©rifier les n≈ìuds
kubectl get nodes

# V√©rifier les composants du control plane
kubectl get pods -n kube-system

# Tester la r√©silience
# 1. Arr√™ter un master et v√©rifier que le cluster fonctionne
# 2. Tester l'API via le load balancer
curl -k https://k8s-lb:6443/version

# V√©rifier etcd cluster
kubectl exec -n kube-system etcd-k8s-master1 -- \
  etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health --cluster
```

---

## 9.2.2 Networking (CNI, Flannel, Calico)

### Concepts de r√©seau Kubernetes

Le r√©seau Kubernetes doit respecter ces exigences fondamentales :
- **Tous les pods** peuvent communiquer entre eux sans NAT
- **Tous les n≈ìuds** peuvent communiquer avec tous les pods sans NAT
- **L'IP d'un pod** est la m√™me vue de l'int√©rieur et de l'ext√©rieur

#### Container Network Interface (CNI)

Le **CNI** est une sp√©cification qui d√©finit comment les plugins r√©seau configurent la connectivit√© des conteneurs.

**Responsabilit√©s du CNI :**
- Attribution d'adresses IP aux pods
- Configuration des interfaces r√©seau
- Routage entre les pods
- Application des politiques r√©seau

### Comparaison des solutions CNI

| Plugin | Complexit√© | Performance | Fonctionnalit√©s | Production |
|--------|------------|-------------|-----------------|------------|
| **Flannel** | Simple | Bonne | Basiques | PME |
| **Calico** | Mod√©r√©e | Excellente | Avanc√©es | Entreprise |
| **Weave** | Mod√©r√©e | Bonne | Chiffrement | Sp√©cialis√© |
| **Cilium** | √âlev√©e | Excellente | eBPF/S√©curit√© | Expert |

### Installation de Flannel

Flannel est simple et adapt√© pour d√©buter avec Kubernetes.

#### Caract√©ristiques de Flannel :
- **Backend VXLAN** : Encapsulation r√©seau overlay
- **Configuration simple** : Peu de param√®tres √† ajuster
- **L√©ger** : Faible consommation de ressources
- **Limit√©** : Pas de Network Policies natives

#### Installation :

```bash
# Appliquer le manifest Flannel
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

# V√©rifier le d√©ploiement
kubectl get pods -n kube-flannel

# V√©rifier la configuration r√©seau
kubectl get nodes -o wide
```

#### Configuration personnalis√©e de Flannel :

```yaml
# flannel-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-flannel-cfg
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
```

### Installation de Calico (Recommand√© pour production)

Calico offre des fonctionnalit√©s avanc√©es et d'excellentes performances.

#### Avantages de Calico :
- **Network Policies** : S√©curit√© fine au niveau r√©seau
- **Performance** : Routage natif, pas d'overlay par d√©faut
- **Scalabilit√©** : Supporte des milliers de n≈ìuds
- **Observabilit√©** : Int√©gration monitoring avanc√©e

#### Installation standard :

```bash
# T√©l√©charger l'op√©rateur Calico
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml

# Configuration personnalis√©e
cat <<EOF | kubectl apply -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
---
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
EOF

# V√©rifier l'installation
kubectl get pods -n calico-system
watch kubectl get pods -n calico-system
```

#### Configuration avanc√©e de Calico :

```yaml
# calico-custom.yaml
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # Configuration des pools IP
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: IPIP
      natOutgoing: Enabled
      nodeSelector: all()

  # Configuration des n≈ìuds
  nodeMetricsPort: 9091
  typhaMetricsPort: 9093

  # Ressources par d√©faut
  componentResources:
  - componentName: Node
    resourceRequirements:
      requests:
        cpu: 250m
        memory: 64Mi
      limits:
        cpu: 500m
        memory: 128Mi
```

### Network Policies avec Calico

Les Network Policies contr√¥lent le trafic r√©seau entre les pods.

#### Exemple de politique basique :

```yaml
# network-policy-example.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-access
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 80
```

### Diagnostic r√©seau

```bash
# Outils de diagnostic Calico
kubectl exec -ti -n calico-system calico-node-xxxxx -- calicoctl node status

# Test de connectivit√©
kubectl run test-pod --image=busybox --restart=Never -- sleep 3600
kubectl exec test-pod -- ping <pod-ip>

# V√©rification des routes
kubectl exec -ti -n calico-system calico-node-xxxxx -- ip route

# Logs des composants r√©seau
kubectl logs -n calico-system -l k8s-app=calico-node
```

---

## 9.2.3 Storage (PV, PVC, StorageClass)

### Concepts de stockage Kubernetes

Le stockage dans Kubernetes est g√©r√© par trois abstractions principales :

#### Persistent Volume (PV)
- **Ressource cluster** : Stockage provisionn√© par l'administrateur
- **Cycle de vie ind√©pendant** : Existe au-del√† de la vie des pods
- **Types vari√©s** : NFS, iSCSI, cloud storage, etc.

#### Persistent Volume Claim (PVC)
- **Demande d'utilisateur** : Sp√©cification des besoins de stockage
- **Binding automatique** : Kubernetes associe PVC et PV compatibles
- **Abstraction** : L'utilisateur ne conna√Æt pas les d√©tails du stockage

#### StorageClass
- **Provisioning dynamique** : Cr√©ation automatique de PV selon la demande
- **Param√®tres configurables** : Type, performance, r√©plication
- **Flexibilit√©** : Diff√©rentes classes pour diff√©rents besoins

### Architecture du stockage

```
Application Pod
      |
      v
 Persistent Volume Claim (PVC)
      |
      v
 Persistent Volume (PV)
      |
      v
Physical Storage (NFS, iSCSI, Cloud)
```

### StorageClass - Provisioning dynamique

#### StorageClass avec stockage local

```yaml
# local-storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
allowVolumeExpansion: true
---
# Pr√©paration des volumes locaux sur chaque n≈ìud
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/local-storage/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - k8s-worker1
```

#### Pr√©paration du stockage local sur les n≈ìuds

```bash
# Sur chaque worker node
sudo mkdir -p /mnt/local-storage/vol{1..3}
sudo chmod 777 /mnt/local-storage/vol{1..3}

# Cr√©er des volumes de test (optionnel)
sudo mkdir -p /mnt/local-storage/vol1/data
echo "Volume local 1" | sudo tee /mnt/local-storage/vol1/data/info.txt
```

#### StorageClass avec NFS

```yaml
# nfs-storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
provisioner: nfs.csi.k8s.io
parameters:
  server: 192.168.1.200
  share: /srv/nfs/k8s
  # Optionnel: sous-r√©pertoire par PVC
  subDir: ${pvc.metadata.namespace}-${pvc.metadata.name}
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
```

#### Installation du driver CSI NFS

```bash
# Installer le driver NFS CSI
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/rbac-csi-nfs-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/csi-nfs-driverinfo.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/csi-nfs-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/csi-nfs-node.yaml

# V√©rifier l'installation
kubectl get pods -n kube-system -l app=csi-nfs-controller
kubectl get pods -n kube-system -l app=csi-nfs-node
```

### Persistent Volume Claims

#### PVC basique

```yaml
# basic-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: web-storage
  namespace: production
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 5Gi
```

#### PVC avec modes d'acc√®s diff√©rents

```yaml
# shared-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-storage
spec:
  accessModes:
    - ReadWriteMany  # Acc√®s concurrent
  storageClassName: nfs-storage
  resources:
    requests:
      storage: 20Gi
---
# read-only-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: config-storage
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: nfs-storage
  resources:
    requests:
      storage: 1Gi
```

### Utilisation des volumes dans les pods

#### Deployment avec volume persistant

```yaml
# app-with-storage.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 80
        volumeMounts:
        - name: web-storage
          mountPath: /usr/share/nginx/html
        - name: config-storage
          mountPath: /etc/nginx/conf.d
          readOnly: true
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
      volumes:
      - name: web-storage
        persistentVolumeClaim:
          claimName: web-storage
      - name: config-storage
        persistentVolumeClaim:
          claimName: config-storage
```

### StatefulSet avec stockage

Les StatefulSets utilisent des templates de PVC pour garantir le stockage persistant.

```yaml
# statefulset-storage.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  serviceName: database-svc
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "secretpassword"
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
  # Template de PVC pour chaque r√©plique
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: local-storage
      resources:
        requests:
          storage: 10Gi
```

### Monitoring et maintenance du stockage

```bash
# V√©rifier les PV et PVC
kubectl get pv
kubectl get pvc -A

# D√©tails d'un volume
kubectl describe pv <pv-name>
kubectl describe pvc <pvc-name> -n <namespace>

# V√©rifier l'utilisation des volumes
kubectl exec -it <pod-name> -- df -h

# Nettoyer les ressources
kubectl delete pvc <pvc-name> -n <namespace>
# Note: Le PV sera nettoy√© selon sa reclaimPolicy
```

---

## 9.2.4 Ingress Controllers

### Concepts Ingress

Un **Ingress** expose les services HTTP et HTTPS depuis l'ext√©rieur du cluster vers les services internes.

#### Probl√®mes r√©solus par Ingress :
- **Exposition multiple** : Un point d'entr√©e pour plusieurs services
- **Routage intelligent** : Par nom d'h√¥te, path, headers
- **SSL/TLS** : Terminaison SSL centralis√©e
- **Load balancing** : R√©partition de charge avanc√©e

#### Architecture Ingress

```
Internet
    |
[Ingress Controller]
    |
[Ingress Rules]
    |
[Services] --> [Pods]
```

### Comparaison des Ingress Controllers

| Controller | Complexit√© | Fonctionnalit√©s | Performance | Cas d'usage |
|------------|------------|-----------------|-------------|-------------|
| **NGINX** | Mod√©r√©e | Compl√®tes | Excellente | G√©n√©ral |
| **Traefik** | Simple | Bonnes | Bonne | PME/Dev |
| **HAProxy** | √âlev√©e | Avanc√©es | Excellente | Enterprise |
| **Istio** | Tr√®s √©lev√©e | Service Mesh | Excellente | Microservices |

### Installation de NGINX Ingress Controller

NGINX Ingress est le plus populaire et convient √† la majorit√© des cas d'usage.

#### Installation via manifest

```bash
# Installation du controller NGINX
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/baremetal/deploy.yaml

# V√©rifier l'installation
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# Le service sera de type NodePort par d√©faut sur bare metal
kubectl get svc ingress-nginx-controller -n ingress-nginx
```

#### Configuration pour LoadBalancer (cloud)

```yaml
# nginx-ingress-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: LoadBalancer  # Sera NodePort sur bare metal
  loadBalancerIP: 192.168.1.100  # IP statique si disponible
  ports:
  - name: http
    port: 80
    targetPort: 80
    protocol: TCP
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/component: controller
```

#### Configuration personnalis√©e du controller

```yaml
# nginx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
data:
  # Optimisation des performances
  worker-processes: "auto"
  worker-connections: "1024"

  # Configuration SSL
  ssl-protocols: "TLSv1.2 TLSv1.3"
  ssl-ciphers: "ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384"

  # Timeouts
  proxy-connect-timeout: "60"
  proxy-send-timeout: "60"
  proxy-read-timeout: "60"

  # Logs
  log-format-upstream: '$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id'
```

### Cr√©ation de ressources Ingress

#### Ingress simple

```yaml
# simple-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

#### Ingress avec SSL/TLS

```yaml
# tls-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: secure-web-ingress
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - secure.example.com
    secretName: tls-secret
  rules:
  - host: secure.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

#### Cr√©ation du certificat TLS

```bash
# G√©n√©rer un certificat auto-sign√© (dev/test)
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout tls.key -out tls.crt \
  -subj "/CN=secure.example.com"

# Cr√©er le secret TLS
kubectl create secret tls tls-secret \
  --key tls.key --cert tls.crt \
  -n production
```

#### Ingress multi-services avec path-based routing

```yaml
# multi-service-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-service-ingress
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /v1(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: api-v1-service
            port:
              number: 80
      - path: /v2(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: api-v2-service
            port:
              number: 80
      - path: /admin(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 8080
      - path: /()(.*)
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

### Annotations avanc√©es NGINX Ingress

#### Gestion du trafic et s√©curit√©

```yaml
# advanced-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: advanced-ingress
  namespace: production
  annotations:
    # Authentification basique
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"

    # Rate limiting
    nginx.ingress.kubernetes.io/rate-limit-connections: "10"
    nginx.ingress.kubernetes.io/rate-limit-rps: "5"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"

    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://frontend.example.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "DNT,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization"

    # Timeouts personnalis√©s
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"

    # Headers personnalis√©s
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "X-Content-Type-Options: nosniff";
      more_set_headers "X-XSS-Protection: 1; mode=block";
spec:
  ingressClassName: nginx
  rules:
  - host: secure-api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: secure-api-service
            port:
              number: 80
```

#### Cr√©ation du secret d'authentification basique

```bash
# Cr√©er un fichier d'authentification
htpasswd -c auth admin
# Entrer le mot de passe quand demand√©

# Cr√©er le secret Kubernetes
kubectl create secret generic basic-auth \
  --from-file=auth \
  -n production
```

### Load Balancing et Health Checks

#### Configuration avanc√©e du load balancing

```yaml
# loadbalancing-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: loadbalanced-ingress
  namespace: production
  annotations:
    # Algorithme de load balancing
    nginx.ingress.kubernetes.io/upstream-hash-by: "$remote_addr"

    # Session affinity
    nginx.ingress.kubernetes.io/affinity: "cookie"
    nginx.ingress.kubernetes.io/affinity-mode: "persistent"
    nginx.ingress.kubernetes.io/session-cookie-name: "INGRESSCOOKIE"
    nginx.ingress.kubernetes.io/session-cookie-max-age: "86400"

    # Health checks personnalis√©s
    nginx.ingress.kubernetes.io/upstream-vhost: "health.internal"
    nginx.ingress.kubernetes.io/custom-http-errors: "404,503"
    nginx.ingress.kubernetes.io/default-backend: "default-backend"
spec:
  ingressClassName: nginx
  rules:
  - host: balanced.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

### Ingress avec Traefik (Alternative)

Traefik est une alternative moderne avec configuration automatique.

#### Installation de Traefik

```yaml
# traefik-deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: traefik-ingress-controller
rules:
- apiGroups: [""]
  resources: ["services", "endpoints", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions", "networking.k8s.io"]
  resources: ["ingresses", "ingressclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions", "networking.k8s.io"]
  resources: ["ingresses/status"]
  verbs: ["update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: traefik-ingress-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-ingress-controller
subjects:
- kind: ServiceAccount
  name: traefik-ingress-controller
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traefik
  namespace: kube-system
  labels:
    app: traefik
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traefik
  template:
    metadata:
      labels:
        app: traefik
    spec:
      serviceAccountName: traefik-ingress-controller
      containers:
      - name: traefik
        image: traefik:v2.10
        args:
        - --api.insecure=true
        - --providers.kubernetesingress=true
        - --entrypoints.web.address=:80
        - --entrypoints.websecure.address=:443
        ports:
        - name: web
          containerPort: 80
        - name: websecure
          containerPort: 443
        - name: admin
          containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: traefik
  namespace: kube-system
spec:
  selector:
    app: traefik
  ports:
  - name: web
    port: 80
    targetPort: 80
  - name: websecure
    port: 443
    targetPort: 443
  - name: admin
    port: 8080
    targetPort: 8080
  type: NodePort
```

#### Ingress avec Traefik

```yaml
# traefik-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: traefik-ingress
  namespace: production
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: "default-auth@kubernetescrd"
    traefik.ingress.kubernetes.io/router.tls: "true"
spec:
  rules:
  - host: traefik.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
  tls:
  - hosts:
    - traefik.example.com
    secretName: traefik-tls
```

### Certificats SSL automatiques avec cert-manager

cert-manager automatise la gestion des certificats SSL/TLS.

#### Installation de cert-manager

```bash
# Installer cert-manager
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# V√©rifier l'installation
kubectl get pods -n cert-manager
```

#### Configuration de Let's Encrypt

```yaml
# letsencrypt-issuer.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    # Serveur Let's Encrypt production
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod-key
    solvers:
    - http01:
        ingress:
          class: nginx
---
# Issuer de test (rate limit plus √©lev√©)
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-staging-key
    solvers:
    - http01:
        ingress:
          class: nginx
```

#### Ingress avec certificat automatique

```yaml
# auto-tls-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auto-tls-ingress
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - secure.example.com
    - www.secure.example.com
    secretName: auto-tls-secret
  rules:
  - host: secure.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
  - host: www.secure.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

### Monitoring et observabilit√© des Ingress

#### M√©triques NGINX Ingress

```yaml
# nginx-monitoring.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
data:
  enable-prometheus-metrics: "true"
  prometheus-port: "10254"
---
# ServiceMonitor pour Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-ingress
  namespace: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
  endpoints:
  - port: prometheus
    interval: 30s
    path: /metrics
```

#### Dashboard de monitoring

```yaml
# ingress-dashboard-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-metrics
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
spec:
  ports:
  - name: prometheus
    port: 10254
    targetPort: 10254
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/component: controller
```

### D√©pannage des Ingress

#### Commandes de diagnostic

```bash
# V√©rifier les Ingress Controllers
kubectl get pods -n ingress-nginx
kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller

# V√©rifier les Ingress rules
kubectl get ingress -A
kubectl describe ingress <ingress-name> -n <namespace>

# V√©rifier les services backend
kubectl get svc -n <namespace>
kubectl get endpoints <service-name> -n <namespace>

# Tester la connectivit√©
kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80
curl -H "Host: myapp.example.com" http://localhost:8080

# V√©rifier les certificats
kubectl get certificate -A
kubectl describe certificate <cert-name> -n <namespace>

# Logs de cert-manager
kubectl logs -n cert-manager -l app=cert-manager
```

#### Probl√®mes courants et solutions

**1. Ingress ne route pas le trafic**
```bash
# V√©rifier que l'Ingress Controller est d√©marr√©
kubectl get pods -n ingress-nginx

# V√©rifier la configuration
kubectl get ingress <ingress-name> -o yaml

# V√©rifier les endpoints
kubectl get endpoints <service-name>
```

**2. Certificat SSL non g√©n√©r√©**
```bash
# V√©rifier l'issuer
kubectl get clusterissuer

# V√©rifier la certificate request
kubectl get certificaterequest -A

# V√©rifier les challenges ACME
kubectl get challenges -A
```

**3. 502/503 Backend errors**
```bash
# V√©rifier que les pods sont running
kubectl get pods -n <namespace>

# V√©rifier les health checks
kubectl describe pod <pod-name> -n <namespace>

# Tester directement le service
kubectl port-forward svc/<service-name> 8080:80 -n <namespace>
```

### Patterns avanc√©s d'Ingress

#### Canary Deployments avec Ingress

```yaml
# main-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: main-app
  namespace: production
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-stable
            port:
              number: 80
---
# canary-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: canary-app
  namespace: production
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"
    # Ou par header: nginx.ingress.kubernetes.io/canary-by-header: "canary"
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-canary
            port:
              number: 80
```

#### Blue/Green avec Ingress

```yaml
# blue-green-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: production
  annotations:
    # Switcher entre "blue" et "green"
    nginx.ingress.kubernetes.io/configuration-snippet: |
      set $service_name "app-blue";
      if ($http_version = "green") {
        set $service_name "app-green";
      }
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-blue  # Service actif
            port:
              number: 80
```

### S√©curit√© avanc√©e des Ingress

#### WAF (Web Application Firewall)

```yaml
# waf-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: protected-app
  namespace: production
  annotations:
    # Protection contre les attaques communes
    nginx.ingress.kubernetes.io/configuration-snippet: |
      # Bloquer les user agents malveillants
      if ($http_user_agent ~* "sqlmap|nikto|wpscan|nessus") {
        return 403;
      }

      # Limiter la taille des headers
      if ($http_content_length > 10m) {
        return 413;
      }

      # Protection XSS
      add_header X-Frame-Options "SAMEORIGIN";
      add_header X-XSS-Protection "1; mode=block";
      add_header X-Content-Type-Options "nosniff";

    # Rate limiting strict
    nginx.ingress.kubernetes.io/rate-limit-connections: "5"
    nginx.ingress.kubernetes.io/rate-limit-rps: "2"
spec:
  ingressClassName: nginx
  rules:
  - host: secure-app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: secure-service
            port:
              number: 80
```

### Performance et optimisation

#### Configuration optimis√©e pour haute charge

```yaml
# high-performance-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
data:
  # Optimisations worker
  worker-processes: "auto"
  worker-connections: "16384"
  worker-rlimit-nofile: "65535"

  # Buffers optimis√©s
  proxy-buffer-size: "128k"
  proxy-buffers-number: "4"
  proxy-busy-buffers-size: "256k"

  # Compression
  use-gzip: "true"
  gzip-level: "6"
  gzip-types: "text/plain text/css text/xml text/javascript application/javascript application/xml+rss application/json"

  # Keep-alive
  upstream-keepalive-connections: "32"
  upstream-keepalive-time: "60s"
  upstream-keepalive-timeout: "60s"

  # Caching
  proxy-cache-valid: "200 302 10m"
  proxy-cache-valid-404: "1m"

  # Logs optimis√©s
  access-log-off: "false"
  error-log-level: "warn"
```

---

## Conclusion de la section 9.2

Cette section vous a pr√©sent√© les aspects essentiels pour d√©ployer Kubernetes en production sur Debian :

### Points cl√©s abord√©s

**Configuration HA (9.2.1)**
- Architecture multi-masters avec load balancer
- etcd en cluster pour la r√©silience des donn√©es
- Proc√©dures de d√©ploiement et de v√©rification

**Networking (9.2.2)**
- Comparaison CNI : Flannel vs Calico
- Configuration avanc√©e du r√©seau
- Network Policies pour la s√©curit√©

**Storage (9.2.3)**
- Gestion du stockage persistant avec PV/PVC/StorageClass
- Support NFS et stockage local
- StatefulSets pour les applications stateful

**Ingress Controllers (9.2.4)**
- NGINX Ingress pour l'exposition des services
- Gestion automatique des certificats SSL
- Patterns avanc√©s (canary, blue/green)

### Bonnes pratiques retenues

‚úÖ **Haute disponibilit√©**
- Toujours d√©ployer 3 masters minimum
- Utiliser un load balancer externe
- Distribuer sur diff√©rentes zones

‚úÖ **S√©curit√©**
- Activer les Network Policies
- Chiffrer le trafic avec TLS
- Limiter les acc√®s avec RBAC

‚úÖ **Performance**
- Choisir le bon CNI selon les besoins
- Optimiser la configuration des Ingress
- Monitorer les m√©triques

‚úÖ **Op√©rationnel**
- Automatiser la gestion des certificats
- Centraliser les logs et m√©triques
- Documenter les proc√©dures

### Prochaines √©tapes

Vous √™tes maintenant pr√™t √† explorer :
- **Section 9.3** : Distributions Kubernetes sp√©cialis√©es (K3s, MicroK8s)
- **Section 9.4** : Outils de l'√©cosyst√®me (Helm, kubectl avanc√©)
- **Section 9.5** : GitOps et CI/CD avec Kubernetes

Un cluster Kubernetes de production n√©cessite de la vigilance et une maintenance r√©guli√®re, mais offre une plateforme robuste et scalable pour vos applications modernes.

‚è≠Ô∏è
