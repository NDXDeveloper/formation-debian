üîù Retour au [Sommaire](/SOMMAIRE.md)

# 12.2 RAID, stockage et persistance

## Introduction

Le stockage constitue le fondement de toute strat√©gie de haute disponibilit√© et de sauvegarde. Cette section explore les technologies qui permettent de prot√©ger vos donn√©es contre les pannes mat√©rielles, d'optimiser les performances et de garantir la persistance dans les environnements modernes, des serveurs traditionnels aux clusters Kubernetes.

Nous aborderons les concepts de base jusqu'aux impl√©mentations avanc√©es, en veillant √† ce que chaque notion soit accessible aux d√©butants tout en fournissant les d√©tails techniques n√©cessaires pour une mise en pratique efficace.

---

## 12.2.1 Configuration RAID logiciel

### Comprendre le RAID

**RAID** (Redundant Array of Independent Disks) permet de combiner plusieurs disques physiques en une unit√© logique pour am√©liorer la performance, la fiabilit√©, ou les deux.

### Types de RAID essentiels

**RAID 0 - R√©partition sans redondance**
- **Principe** : Les donn√©es sont r√©parties sur plusieurs disques
- **Avantages** : Performance maximale (vitesse √ó nombre de disques)
- **Inconv√©nients** : Aucune protection (panne d'un disque = perte totale)
- **Usage** : Cache temporaire, donn√©es non critiques

**RAID 1 - Miroir**
- **Principe** : Duplication identique sur tous les disques
- **Avantages** : Protection compl√®te, lecture rapide
- **Inconv√©nients** : Capacit√© divis√©e par 2, √©criture normale
- **Usage** : Syst√®me d'exploitation, donn√©es critiques

**RAID 5 - R√©partition avec parit√©**
- **Principe** : Donn√©es + parit√© r√©parties sur tous les disques
- **Avantages** : Protection contre 1 panne, bon ratio capacit√©/protection
- **Inconv√©nients** : Performance d'√©criture r√©duite, reconstruction lente
- **Usage** : Stockage de fichiers, applications standard

**RAID 6 - Double parit√©**
- **Principe** : Comme RAID 5 mais avec 2 informations de parit√©
- **Avantages** : Protection contre 2 pannes simultan√©es
- **Inconv√©nients** : Performance d'√©criture encore plus r√©duite
- **Usage** : Gros volumes, donn√©es tr√®s importantes

**RAID 10 (1+0) - Miroir de stripes**
- **Principe** : Combinaison de RAID 1 et RAID 0
- **Avantages** : Excellentes performances + protection
- **Inconv√©nients** : Co√ªt √©lev√© (50% de capacit√© utile)
- **Usage** : Bases de donn√©es, applications critiques

### Configuration pratique avec mdadm

**Installation des outils :**

```bash
# Installation sur Debian
sudo apt update
sudo apt install mdadm

# V√©rification des disques disponibles
sudo fdisk -l
lsblk
```

**Cr√©ation d'un RAID 1 :**

```bash
# Pr√©paration des disques (exemple avec /dev/sdb et /dev/sdc)
sudo fdisk /dev/sdb
# Cr√©er une partition de type 'Linux RAID' (fd)

sudo fdisk /dev/sdc
# M√™me op√©ration

# Cr√©ation du RAID 1
sudo mdadm --create /dev/md0 \
    --level=1 \
    --raid-devices=2 \
    /dev/sdb1 /dev/sdc1

# V√©rification
cat /proc/mdstat
sudo mdadm --detail /dev/md0
```

**Cr√©ation d'un RAID 5 :**

```bash
# RAID 5 avec 3 disques (minimum)
sudo mdadm --create /dev/md1 \
    --level=5 \
    --raid-devices=3 \
    /dev/sdd1 /dev/sde1 /dev/sdf1

# Surveillance de la construction
watch cat /proc/mdstat
```

**Configuration persistante :**

```bash
# G√©n√©ration du fichier de configuration
sudo mdadm --detail --scan >> /etc/mdadm/mdadm.conf

# Mise √† jour de l'initramfs
sudo update-initramfs -u

# Exemple de contenu mdadm.conf
echo "ARRAY /dev/md0 metadata=1.2 name=hostname:0 UUID=12345678-1234-1234-1234-123456789abc" | sudo tee -a /etc/mdadm/mdadm.conf
```

### Formatage et montage

```bash
# Formatage du RAID
sudo mkfs.ext4 /dev/md0

# Cr√©ation du point de montage
sudo mkdir /mnt/raid1

# Montage temporaire
sudo mount /dev/md0 /mnt/raid1

# Montage permanent dans /etc/fstab
echo "/dev/md0 /mnt/raid1 ext4 defaults 0 2" | sudo tee -a /etc/fstab
```

### Gestion et maintenance

**Surveillance :**

```bash
# √âtat d√©taill√©
sudo mdadm --detail /dev/md0

# Statut de tous les RAIDs
cat /proc/mdstat

# Information sur un disque sp√©cifique
sudo mdadm --examine /dev/sdb1
```

**Simulation de panne et remplacement :**

```bash
# Marquer un disque comme d√©faillant (test)
sudo mdadm --manage /dev/md0 --fail /dev/sdb1

# Retirer le disque d√©faillant
sudo mdadm --manage /dev/md0 --remove /dev/sdb1

# Ajouter un nouveau disque
sudo mdadm --manage /dev/md0 --add /dev/sdg1

# Surveiller la reconstruction
watch cat /proc/mdstat
```

**Arr√™t et d√©marrage :**

```bash
# Arr√™t propre d'un RAID
sudo umount /mnt/raid1
sudo mdadm --stop /dev/md0

# Assemblage au d√©marrage
sudo mdadm --assemble /dev/md0 /dev/sdb1 /dev/sdc1

# Assemblage automatique
sudo mdadm --assemble --scan
```

---

## 12.2.2 LVM (Logical Volume Manager)

### Concepts fondamentaux

LVM ajoute une couche d'abstraction entre les disques physiques et les syst√®mes de fichiers, offrant flexibilit√© et fonctionnalit√©s avanc√©es.

**Hi√©rarchie LVM :**
1. **PV (Physical Volume)** : Disque ou partition physique
2. **VG (Volume Group)** : Groupe de PV formant un pool de stockage
3. **LV (Logical Volume)** : Volume logique cr√©√© dans un VG

```
Disques physiques -> PV -> VG -> LV -> Syst√®me de fichiers
```

### Installation et configuration de base

```bash
# Installation des outils LVM
sudo apt install lvm2

# V√©rification des disques
sudo fdisk -l
```

**Cr√©ation √©tape par √©tape :**

```bash
# 1. Cr√©er des Physical Volumes
sudo pvcreate /dev/sdb /dev/sdc
# Ou sur des partitions
sudo pvcreate /dev/sdb1 /dev/sdc1

# V√©rification
sudo pvdisplay
sudo pvscan

# 2. Cr√©er un Volume Group
sudo vgcreate vg-data /dev/sdb /dev/sdc

# V√©rification
sudo vgdisplay
sudo vgscan

# 3. Cr√©er des Logical Volumes
sudo lvcreate -L 10G -n lv-web vg-data
sudo lvcreate -L 20G -n lv-database vg-data
sudo lvcreate -l 100%FREE -n lv-backup vg-data  # Utilise l'espace restant

# V√©rification
sudo lvdisplay
sudo lvscan
```

### Formatage et montage des LV

```bash
# Formatage
sudo mkfs.ext4 /dev/vg-data/lv-web
sudo mkfs.ext4 /dev/vg-data/lv-database

# Cr√©ation des points de montage
sudo mkdir -p /var/www /var/lib/mysql-lvm

# Montage
sudo mount /dev/vg-data/lv-web /var/www
sudo mount /dev/vg-data/lv-database /var/lib/mysql-lvm

# Configuration permanente dans /etc/fstab
echo "/dev/vg-data/lv-web /var/www ext4 defaults 0 2" | sudo tee -a /etc/fstab
echo "/dev/vg-data/lv-database /var/lib/mysql-lvm ext4 defaults 0 2" | sudo tee -a /etc/fstab
```

### Gestion dynamique des volumes

**Redimensionnement :**

```bash
# Agrandir un Logical Volume
sudo lvextend -L +5G /dev/vg-data/lv-web
# Ou sp√©cifier la taille finale
sudo lvextend -L 15G /dev/vg-data/lv-web

# Redimensionner le syst√®me de fichiers
sudo resize2fs /dev/vg-data/lv-web

# Op√©ration en une commande (avec -r)
sudo lvextend -L +5G -r /dev/vg-data/lv-web

# R√©duire un volume (ATTENTION : risque de perte de donn√©es)
# 1. D√©monter le volume
sudo umount /var/www
# 2. V√©rifier le syst√®me de fichiers
sudo e2fsck -f /dev/vg-data/lv-web
# 3. R√©duire le syst√®me de fichiers
sudo resize2fs /dev/vg-data/lv-web 8G
# 4. R√©duire le LV
sudo lvreduce -L 8G /dev/vg-data/lv-web
# 5. Remonter
sudo mount /dev/vg-data/lv-web /var/www
```

**Ajout de disques au Volume Group :**

```bash
# Pr√©parer le nouveau disque
sudo pvcreate /dev/sdd

# L'ajouter au VG existant
sudo vgextend vg-data /dev/sdd

# V√©rifier l'espace disponible
sudo vgdisplay vg-data
```

### Snapshots LVM

Les snapshots permettent des sauvegardes coh√©rentes sans arr√™t de service :

```bash
# Cr√©er un snapshot
sudo lvcreate -L 2G -s -n lv-database-snap /dev/vg-data/lv-database

# Monter le snapshot (lecture seule recommand√©e)
sudo mkdir /mnt/database-backup
sudo mount -o ro /dev/vg-data/lv-database-snap /mnt/database-backup

# Effectuer la sauvegarde depuis le snapshot
sudo tar -czf /backup/database-$(date +%Y%m%d).tar.gz -C /mnt/database-backup .

# Nettoyer apr√®s sauvegarde
sudo umount /mnt/database-backup
sudo lvremove /dev/vg-data/lv-database-snap
```

**Script de sauvegarde automatis√©e avec snapshot :**

```bash
#!/bin/bash
# Script de sauvegarde avec snapshot LVM

LV_PATH="/dev/vg-data/lv-database"
SNAP_NAME="lv-database-snap"
SNAP_SIZE="2G"
MOUNT_POINT="/mnt/snapshot-backup"
BACKUP_DIR="/backup"
DATE=$(date +%Y%m%d-%H%M%S)

# Cr√©ation du snapshot
echo "Cr√©ation du snapshot..."
sudo lvcreate -L $SNAP_SIZE -s -n $SNAP_NAME $LV_PATH

# Montage
sudo mkdir -p $MOUNT_POINT
sudo mount -o ro /dev/vg-data/$SNAP_NAME $MOUNT_POINT

# Sauvegarde
echo "Sauvegarde en cours..."
sudo tar -czf $BACKUP_DIR/database-backup-$DATE.tar.gz -C $MOUNT_POINT .

# Nettoyage
sudo umount $MOUNT_POINT
sudo lvremove -f /dev/vg-data/$SNAP_NAME

echo "Sauvegarde termin√©e : $BACKUP_DIR/database-backup-$DATE.tar.gz"
```

---

## 12.2.3 Stockage Kubernetes (CSI drivers)

### Introduction au stockage Kubernetes

Kubernetes s√©pare le stockage en plusieurs concepts :
- **Volume** : Espace de stockage temporaire li√© au pod
- **PersistentVolume (PV)** : Ressource de stockage dans le cluster
- **PersistentVolumeClaim (PVC)** : Demande de stockage par une application
- **StorageClass** : Template pour cr√©er dynamiquement des PV

### Container Storage Interface (CSI)

CSI standardise l'interface entre Kubernetes et les syst√®mes de stockage.

**Avantages des CSI drivers :**
- Ind√©pendance du fournisseur
- Fonctionnalit√©s avanc√©es (snapshots, clonage)
- D√©ploiement et mise √† jour simplifi√©s
- Support multi-plateformes

### Configuration de base avec CSI

**Exemple avec le driver local-path (pour tests) :**

```yaml
# Installation du provisioner local-path
apiVersion: v1
kind: Namespace
metadata:
  name: local-path-storage

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
```

**D√©ploiement du driver :**

```bash
# Installation via kubectl
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.24/deploy/local-path-storage.yaml

# V√©rification
kubectl get storageclass
kubectl get pods -n local-path-storage
```

### PersistentVolumes et PersistentVolumeClaims

**Cr√©ation d'un PVC :**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path
  resources:
    requests:
      storage: 10Gi
```

**Utilisation dans un pod :**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "password"
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        ports:
        - containerPort: 5432
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: database-pvc
```

### CSI avec Ceph (Rook)

Rook simplifie le d√©ploiement de Ceph dans Kubernetes :

```yaml
# Installation de l'op√©rateur Rook
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph

---
# StorageClass pour Ceph RBD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  clusterID: rook-ceph
  pool: replicapool
  imageFormat: "2"
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
reclaimPolicy: Delete
allowVolumeExpansion: true
```

### Snapshots de volumes

**VolumeSnapshotClass :**

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-snapclass
driver: rook-ceph.rbd.csi.ceph.com
deletionPolicy: Delete
parameters:
  clusterID: rook-ceph
  csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/snapshotter-secret-namespace: rook-ceph
```

**Cr√©ation d'un snapshot :**

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: database-snapshot
spec:
  volumeSnapshotClassName: csi-snapclass
  source:
    persistentVolumeClaimName: database-pvc
```

**Restauration depuis un snapshot :**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-restored-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: rook-ceph-block
  resources:
    requests:
      storage: 10Gi
  dataSource:
    name: database-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
```

### Monitoring du stockage

**M√©triques importantes :**

```yaml
# ServiceMonitor pour Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: csi-metrics
spec:
  selector:
    matchLabels:
      app: csi-driver
  endpoints:
  - port: metrics
    path: /metrics
```

**Exemple d'alertes :**

```yaml
# R√®gles d'alerte pour le stockage
groups:
- name: storage.rules
  rules:
  - alert: PVCStorageRunningFull
    expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PVC storage running full"
      description: "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is running full ({{ $value | humanizePercentage }} available)"

  - alert: PVCStorageFull
    expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.05
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "PVC storage full"
      description: "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is full ({{ $value | humanizePercentage }} available)"
```

---

## 12.2.4 Surveillance des disques

### Outils de monitoring syst√®me

**smartmontools - Surveillance SMART :**

```bash
# Installation
sudo apt install smartmontools

# V√©rification de la compatibilit√© SMART
sudo smartctl -i /dev/sda

# Test de sant√© global
sudo smartctl -H /dev/sda

# Informations d√©taill√©es
sudo smartctl -a /dev/sda

# Test court
sudo smartctl -t short /dev/sda

# Test long (peut prendre plusieurs heures)
sudo smartctl -t long /dev/sda

# R√©sultats des tests
sudo smartctl -l selftest /dev/sda
```

**Configuration de surveillance automatique :**

```bash
# Configuration dans /etc/smartd.conf
echo "DEVICESCAN -a -o on -S on -n standby,q -s (S/../.././02|L/../../6/03) -W 4,35,40" | sudo tee -a /etc/smartd.conf

# Red√©marrage du service
sudo systemctl restart smartd
sudo systemctl enable smartd
```

**Script de surveillance personnalis√© :**

```bash
#!/bin/bash
# Script de surveillance des disques

LOG_FILE="/var/log/disk-health.log"
EMAIL="admin@example.com"

check_disk_health() {
    local disk=$1
    local health=$(sudo smartctl -H $disk | grep "SMART overall-health" | awk '{print $6}')

    echo "$(date): Checking $disk - Health: $health" >> $LOG_FILE

    if [ "$health" != "PASSED" ]; then
        echo "ALERTE: Disque $disk en panne!" | mail -s "Alerte disque" $EMAIL
        return 1
    fi

    # V√©rification de la temp√©rature
    local temp=$(sudo smartctl -a $disk | grep "Temperature_Celsius" | awk '{print $10}')
    if [ "$temp" -gt 50 ]; then
        echo "ATTENTION: Disque $disk chaud ($temp¬∞C)" | mail -s "Temp√©rature disque" $EMAIL
    fi

    return 0
}

# V√©rification de tous les disques
for disk in $(lsblk -d -o NAME | grep -E "sd[a-z]|nvme[0-9]" | sed 's/^/\/dev\//'); do
    check_disk_health $disk
done
```

### Surveillance des performances I/O

**iostat - Statistiques I/O :**

```bash
# Installation (fait partie de sysstat)
sudo apt install sysstat

# Surveillance en temps r√©el (toutes les 5 secondes)
iostat -x 5

# Affichage par device avec statistiques √©tendues
iostat -d -x 1

# Monitoring sp√©cifique d'un disque
iostat -x /dev/sda 2
```

**iotop - Processus consommateurs I/O :**

```bash
# Installation
sudo apt install iotop

# Surveillance en temps r√©el
sudo iotop

# Tri par I/O total
sudo iotop -o

# Affichage cumulatif
sudo iotop -a
```

### Surveillance RAID

**Monitoring mdadm :**

```bash
# Script de surveillance RAID
#!/bin/bash

RAID_STATUS_FILE="/proc/mdstat"
LOG_FILE="/var/log/raid-monitor.log"
EMAIL="admin@example.com"

check_raid_status() {
    local raid_device=$1

    # V√©rification de l'√©tat
    local status=$(sudo mdadm --detail $raid_device | grep "State :" | awk '{print $3}')

    echo "$(date): RAID $raid_device - Status: $status" >> $LOG_FILE

    case $status in
        "clean")
            echo "OK: RAID $raid_device est sain"
            ;;
        "degraded")
            echo "ALERTE: RAID $raid_device d√©grad√©!" | mail -s "RAID d√©grad√©" $EMAIL
            ;;
        "failed")
            echo "CRITIQUE: RAID $raid_device en √©chec!" | mail -s "RAID en √©chec" $EMAIL
            ;;
        *)
            echo "ATTENTION: RAID $raid_device √©tat inconnu: $status" | mail -s "RAID √©tat inconnu" $EMAIL
            ;;
    esac
}

# V√©rification de tous les RAIDs actifs
for raid in $(grep md /proc/mdstat | awk '{print "/dev/" $1}'); do
    check_raid_status $raid
done
```

### Surveillance LVM

```bash
#!/bin/bash
# Script de surveillance LVM

check_lvm_health() {
    # V√©rification des Physical Volumes
    echo "=== Physical Volumes ==="
    sudo pvs -o pv_name,vg_name,pv_size,pv_free,pv_attr --noheadings | while read pv; do
        echo "PV: $pv"
    done

    # V√©rification des Volume Groups
    echo "=== Volume Groups ==="
    sudo vgs -o vg_name,vg_size,vg_free,vg_attr --noheadings | while read vg; do
        vg_name=$(echo $vg | awk '{print $1}')
        vg_free_pct=$(sudo vgs --noheadings -o vg_free_percent $vg_name | tr -d ' %')

        echo "VG: $vg"

        if [ "$vg_free_pct" -lt 10 ]; then
            echo "ATTENTION: VG $vg_name presque plein (${vg_free_pct}% libre)"
        fi
    done

    # V√©rification des Logical Volumes
    echo "=== Logical Volumes ==="
    sudo lvs -o lv_name,vg_name,lv_size,lv_attr --noheadings | while read lv; do
        echo "LV: $lv"
    done
}

check_lvm_health
```

### Surveillance dans Kubernetes

**DaemonSet pour monitoring des n≈ìuds :**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: disk-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      name: disk-monitor
  template:
    metadata:
      labels:
        name: disk-monitor
    spec:
      hostPID: true
      hostNetwork: true
      containers:
      - name: disk-monitor
        image: debian:11
        command:
        - /bin/bash
        - -c
        - |
          apt-get update && apt-get install -y smartmontools sysstat
          while true; do
            echo "=== $(date) ==="
            smartctl -H /dev/sda || echo "SMART check failed"
            iostat -x 1 1
            sleep 300
          done
        securityContext:
          privileged: true
        volumeMounts:
        - name: dev
          mountPath: /dev
        - name: proc
          mountPath: /host-proc
          readOnly: true
        - name: sys
          mountPath: /host-sys
          readOnly: true
      volumes:
      - name: dev
        hostPath:
          path: /dev
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      tolerations:
      - effect: NoSchedule
        operator: Exists
```

---

## 12.2.5 R√©cup√©ration de donn√©es

### Pr√©vention et pr√©paration

**Bonnes pratiques pr√©ventives :**
- Sauvegardes r√©guli√®res et test√©es
- Surveillance proactive des disques
- Documentation des configurations
- Tests de r√©cup√©ration planifi√©s

### Outils de r√©cup√©ration Linux

**ddrescue - R√©cup√©ration de disques d√©faillants :**

```bash
# Installation
sudo apt install gddrescue

# Clonage d'un disque d√©faillant
sudo ddrescue /dev/sda /dev/sdb logfile.log

# R√©cup√©ration avec multiple passes
sudo ddrescue --no-scrape /dev/sda /dev/sdb logfile.log
sudo ddrescue --direct --max-retries=3 /dev/sda /dev/sdb logfile.log

# Visualisation du progr√®s
sudo ddrescue --verbose /dev/sda /dev/sdb logfile.log
```

**testdisk - R√©cup√©ration de partitions :**

```bash
# Installation
sudo apt install testdisk

# Lancement interactif
sudo testdisk

# Processus typique :
# 1. S√©lectionner le disque
# 2. Choisir le type de table de partition
# 3. Analyser pour trouver les partitions perdues
# 4. √âcrire les modifications
```

**photorec - R√©cup√©ration de fichiers :**

```bash
# R√©cup√©ration de fichiers (inclus avec testdisk)
sudo photorec

# R√©cup√©ration en ligne de commande
sudo photorec /log /dev/sda
```

### R√©cup√©ration de syst√®mes de fichiers

**fsck - R√©paration de syst√®me de fichiers :**

```bash
# V√©rification et r√©paration ext4
sudo umount /dev/sda1
sudo fsck.ext4 -f /dev/sda1  # Force check
sudo fsck.ext4 -f -y /dev/sda1  # R√©paration automatique

# Pour XFS
sudo umount /dev/sda1
sudo xfs_repair /dev/sda1

# Pour BTRFS
sudo umount /dev/sda1
sudo btrfs check --repair /dev/sda1
```

**R√©cup√©ration LVM :**

```bash
# Reconstruction des m√©tadonn√©es LVM
sudo vgscan
sudo vgchange -ay

# R√©cup√©ration d'un VG √† partir des PV
sudo vgcfgrestore vg-data

# Activation forc√©e
sudo vgchange -ay --partial vg-data
```

### R√©cup√©ration de bases de donn√©es

**MySQL/MariaDB :**

```bash
# Mode de r√©cup√©ration InnoDB
sudo systemctl stop mysql

# Ajouter dans /etc/mysql/my.cnf sous [mysqld]
# innodb_force_recovery = 1

sudo systemctl start mysql

# Export des donn√©es r√©cup√©rables
mysqldump --all-databases > recovery-dump.sql

# Remettre innodb_force_recovery = 0 et red√©marrer
```

**PostgreSQL :**

```bash
# R√©cup√©ration avec pg_resetwal (ex pg_resetxlog)
sudo systemctl stop postgresql

# Reset du WAL (ATTENTION: perte possible de donn√©es)
sudo -u postgres pg_resetwal /var/lib/postgresql/13/main

sudo systemctl start postgresql

# V√©rification de la coh√©rence
sudo -u postgres vacuumdb --all --analyze
```

### R√©cup√©ration dans Kubernetes

**Restauration depuis Velero :**

```bash
# Lister les sauvegardes disponibles
velero backup get

# Restauration compl√®te
velero restore create restore-20240115 --from-backup backup-20240115

# Restauration s√©lective
velero restore create restore-namespace --from-backup backup-20240115 --include-namespaces production

# Suivi de la restauration
velero restore describe restore-20240115
velero restore logs restore-20240115
```

**R√©cup√©ration ETCD :**

```bash
# Arr√™t du cluster
sudo systemctl stop kubelet
sudo systemctl stop kube-apiserver
sudo systemctl stop kube-controller-manager
sudo systemctl stop kube-scheduler

# Sauvegarde des donn√©es existantes (par pr√©caution)
sudo mv /var/lib/etcd /var/lib/etcd.backup.$(date +%Y%m%d)

# Restauration depuis un snapshot
ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot.db \
  --data-dir=/var/lib/etcd \
  --name=master-node \
  --initial-cluster=master-node=https://10.0.0.10:2380 \
  --initial-cluster-token=etcd-cluster-1 \
  --initial-advertise-peer-urls=https://10.0.0.10:2380

# Correction des permissions
sudo chown -R etcd:etcd /var/lib/etcd

# Red√©marrage des services
sudo systemctl start kube-apiserver
sudo systemctl start kube-controller-manager
sudo systemctl start kube-scheduler
sudo systemctl start kubelet

# V√©rification
kubectl get nodes
kubectl get pods --all-namespaces
```

### R√©cup√©ration de volumes persistants

**R√©cup√©ration CSI avec snapshots :**

```bash
# Lister les snapshots disponibles
kubectl get volumesnapshots

# Cr√©er un nouveau PVC depuis un snapshot
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: recovered-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 10Gi
  dataSource:
    name: database-snapshot-20240115
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
EOF

# V√©rification de la r√©cup√©ration
kubectl get pvc recovered-pvc
kubectl describe pvc recovered-pvc
```

**R√©cup√©ration manuelle de volumes :**

```bash
# Si les m√©tadonn√©es Kubernetes sont perdues mais les donn√©es physiques existent
# 1. Identifier le volume physique
sudo lvs
sudo mount /dev/vg-data/lost-volume /mnt/recovery

# 2. Cr√©er un nouveau PV manuellement
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: recovered-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  local:
    path: /mnt/recovery
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
EOF

# 3. Cr√©er un PVC pour r√©clamer ce PV
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: recovered-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  resources:
    requests:
      storage: 10Gi
  volumeName: recovered-pv
EOF
```

### Proc√©dures d'urgence et disaster recovery

**Plan de r√©cup√©ration d'urgence :**

```bash
#!/bin/bash
# Script de disaster recovery

set -e

LOG_FILE="/var/log/disaster-recovery-$(date +%Y%m%d-%H%M%S).log"
BACKUP_LOCATION="/backup/disaster-recovery"
RECOVERY_DIR="/recovery"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

log "=== D√âBUT DE LA PROC√âDURE DE DISASTER RECOVERY ==="

# 1. √âvaluation des dommages
log "√âvaluation des syst√®mes..."

# V√©rification des disques
for disk in /dev/sd{a..z}; do
    if [ -e $disk ]; then
        if sudo smartctl -H $disk > /dev/null 2>&1; then
            log "‚úì Disque $disk : op√©rationnel"
        else
            log "‚úó Disque $disk : d√©faillant ou inaccessible"
        fi
    fi
done

# V√©rification des RAIDs
if [ -f /proc/mdstat ]; then
    while read line; do
        if echo $line | grep -q "md[0-9]"; then
            raid_device=$(echo $line | awk '{print $1}')
            status=$(echo $line | grep -o '\[U*_*\]' || echo "unknown")
            log "RAID /dev/$raid_device : $status"
        fi
    done < /proc/mdstat
fi

# V√©rification LVM
log "V√©rification LVM..."
sudo vgscan 2>&1 | tee -a $LOG_FILE
sudo lvscan 2>&1 | tee -a $LOG_FILE

# 2. R√©cup√©ration des donn√©es critiques
log "Tentative de r√©cup√©ration des donn√©es critiques..."

mkdir -p $RECOVERY_DIR

# R√©cup√©ration des configurations syst√®me
if [ -d /etc ]; then
    log "Sauvegarde de /etc..."
    sudo tar -czf $RECOVERY_DIR/etc-recovery-$(date +%Y%m%d).tar.gz /etc/ 2>/dev/null || log "Erreur lors de la sauvegarde /etc"
fi

# R√©cup√©ration des donn√©es utilisateurs
if [ -d /home ]; then
    log "Sauvegarde de /home..."
    sudo tar -czf $RECOVERY_DIR/home-recovery-$(date +%Y%m%d).tar.gz /home/ 2>/dev/null || log "Erreur lors de la sauvegarde /home"
fi

# 3. Reconstruction du syst√®me
log "Pr√©paration de la reconstruction..."

# Script de reconstruction automatique
cat > $RECOVERY_DIR/rebuild-system.sh << 'EOF'
#!/bin/bash
# Script de reconstruction du syst√®me

echo "=== RECONSTRUCTION DU SYST√àME ==="

# 1. Reconfiguration RAID si n√©cessaire
echo "V√©rification et reconstruction RAID..."
# (Commandes sp√©cifiques selon la configuration RAID d√©couverte)

# 2. Reconfiguration LVM
echo "Reconfiguration LVM..."
sudo vgchange -ay
sudo lvchange -ay --all

# 3. Montage des syst√®mes de fichiers
echo "Montage des syst√®mes de fichiers..."
sudo mount -a

# 4. Restauration des services critiques
echo "Red√©marrage des services..."
sudo systemctl start docker
sudo systemctl start kubelet

echo "Reconstruction termin√©e"
EOF

chmod +x $RECOVERY_DIR/rebuild-system.sh

log "=== PROC√âDURE DE DISASTER RECOVERY TERMIN√âE ==="
log "Logs disponibles dans : $LOG_FILE"
log "Donn√©es de r√©cup√©ration dans : $RECOVERY_DIR"
log "Script de reconstruction : $RECOVERY_DIR/rebuild-system.sh"
```

### Tests de r√©cup√©ration automatis√©s

**Script de test de r√©cup√©ration :**

```bash
#!/bin/bash
# Test automatis√© de proc√©dures de r√©cup√©ration

TEST_LOG="/var/log/recovery-test-$(date +%Y%m%d).log"
TEST_DIR="/tmp/recovery-test"
BACKUP_DIR="/backup"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $TEST_LOG
}

cleanup() {
    log "Nettoyage des tests..."
    sudo umount $TEST_DIR/mount 2>/dev/null || true
    sudo lvremove -f /dev/test-vg/test-lv 2>/dev/null || true
    sudo vgremove -f test-vg 2>/dev/null || true
    sudo pvremove -f /dev/loop0 2>/dev/null || true
    sudo losetup -d /dev/loop0 2>/dev/null || true
    rm -rf $TEST_DIR
}

trap cleanup EXIT

log "=== D√âBUT DES TESTS DE R√âCUP√âRATION ==="

# 1. Test de r√©cup√©ration LVM
log "Test de r√©cup√©ration LVM..."

mkdir -p $TEST_DIR

# Cr√©ation d'un disque virtuel pour test
dd if=/dev/zero of=$TEST_DIR/test-disk.img bs=1M count=100 2>/dev/null
sudo losetup /dev/loop0 $TEST_DIR/test-disk.img

# Configuration LVM de test
sudo pvcreate /dev/loop0
sudo vgcreate test-vg /dev/loop0
sudo lvcreate -L 50M -n test-lv test-vg

# Formatage et montage
sudo mkfs.ext4 /dev/test-vg/test-lv
mkdir -p $TEST_DIR/mount
sudo mount /dev/test-vg/test-lv $TEST_DIR/mount

# Cr√©ation de donn√©es de test
echo "Test data $(date)" | sudo tee $TEST_DIR/mount/test-file

# Simulation de snapshot
sudo lvcreate -L 10M -s -n test-lv-snap /dev/test-vg/test-lv

# Test de sauvegarde depuis snapshot
mkdir -p $TEST_DIR/snapshot-mount
sudo mount -o ro /dev/test-vg/test-lv-snap $TEST_DIR/snapshot-mount
sudo tar -czf $TEST_DIR/snapshot-backup.tar.gz -C $TEST_DIR/snapshot-mount .
sudo umount $TEST_DIR/snapshot-mount

# Validation
if [ -f $TEST_DIR/snapshot-backup.tar.gz ]; then
    log "‚úì Test snapshot LVM r√©ussi"
else
    log "‚úó Test snapshot LVM √©chou√©"
fi

# 2. Test de r√©cup√©ration de base de donn√©es
log "Test de r√©cup√©ration base de donn√©es..."

# Simulation d'une base de donn√©es corrompue
if command -v mysql >/dev/null 2>&1; then
    mysql -e "CREATE DATABASE test_recovery;" 2>/dev/null || true
    mysql test_recovery -e "CREATE TABLE test (id INT, data VARCHAR(50));"
    mysql test_recovery -e "INSERT INTO test VALUES (1, 'test data');"

    # Sauvegarde
    mysqldump test_recovery > $TEST_DIR/test_recovery.sql

    # Suppression et restauration
    mysql -e "DROP DATABASE test_recovery;"
    mysql -e "CREATE DATABASE test_recovery;"
    mysql test_recovery < $TEST_DIR/test_recovery.sql

    # V√©rification
    RESULT=$(mysql test_recovery -sN -e "SELECT data FROM test WHERE id=1;")
    if [ "$RESULT" = "test data" ]; then
        log "‚úì Test r√©cup√©ration MySQL r√©ussi"
    else
        log "‚úó Test r√©cup√©ration MySQL √©chou√©"
    fi

    mysql -e "DROP DATABASE test_recovery;" 2>/dev/null || true
fi

# 3. Test de r√©cup√©ration Kubernetes (si disponible)
if command -v kubectl >/dev/null 2>&1; then
    log "Test de r√©cup√©ration Kubernetes..."

    # Cr√©ation d'un namespace de test
    kubectl create namespace recovery-test 2>/dev/null || true

    # D√©ploiement de test
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: recovery-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: test
        image: nginx:alpine
        ports:
        - containerPort: 80
EOF

    # Attendre le d√©ploiement
    sleep 10

    # V√©rification
    if kubectl get pods -n recovery-test | grep -q "Running"; then
        log "‚úì Test d√©ploiement Kubernetes r√©ussi"
    else
        log "‚úó Test d√©ploiement Kubernetes √©chou√©"
    fi

    # Nettoyage
    kubectl delete namespace recovery-test 2>/dev/null || true
fi

log "=== TESTS DE R√âCUP√âRATION TERMIN√âS ==="
log "R√©sultats d√©taill√©s dans : $TEST_LOG"
```

### Documentation de r√©cup√©ration

**Template de runbook de r√©cup√©ration :**

```markdown
# RUNBOOK - PROC√âDURE DE R√âCUP√âRATION D'URGENCE

## Informations g√©n√©rales
- **Service** : [Nom du service]
- **Criticit√©** : [Critique/Important/Standard]
- **RTO** : [Temps de r√©cup√©ration objectif]
- **RPO** : [Point de r√©cup√©ration objectif]
- **Responsable** : [Nom et contact]

## Pr√©requis
- [ ] Acc√®s root/admin aux syst√®mes
- [ ] Acc√®s aux sauvegardes
- [ ] Outils de r√©cup√©ration install√©s
- [ ] Communication avec l'√©quipe

## Proc√©dure de diagnostic

### 1. √âvaluation initiale
```bash
# V√©rification des syst√®mes critiques
systemctl status [service-critique]
df -h
mount | grep [points-montage-critiques]
```

### 2. V√©rification du stockage
```bash
# RAID
cat /proc/mdstat
mdadm --detail /dev/md[X]

# LVM
vgs
lvs
pvs

# Disques
smartctl -H /dev/sd[X]
```

## Proc√©dure de r√©cup√©ration

### Sc√©nario 1 : Panne de disque simple
1. **Identification du disque d√©faillant**
   ```bash
   cat /proc/mdstat
   smartctl -H /dev/sd[X]
   ```

2. **Remplacement √† chaud (si support√©)**
   ```bash
   mdadm --manage /dev/md[X] --fail /dev/sd[X]
   mdadm --manage /dev/md[X] --remove /dev/sd[X]
   # Remplacer physiquement le disque
   mdadm --manage /dev/md[X] --add /dev/sd[Y]
   ```

### Sc√©nario 2 : Corruption du syst√®me de fichiers
1. **D√©montage et v√©rification**
   ```bash
   umount /mount/point
   fsck -f /dev/device
   ```

2. **R√©cup√©ration depuis sauvegarde si n√©cessaire**
   ```bash
   tar -xzf /backup/latest.tar.gz -C /mount/point/
   ```

### Sc√©nario 3 : Perte compl√®te du syst√®me
1. **Installation syst√®me minimal**
2. **Restauration depuis sauvegarde compl√®te**
3. **Reconfiguration des services**

## Validation post-r√©cup√©ration

### Tests fonctionnels
- [ ] Services d√©marr√©s
- [ ] Connectivit√© r√©seau
- [ ] Acc√®s aux donn√©es
- [ ] Performance acceptable

### Tests applicatifs
- [ ] Interface utilisateur accessible
- [ ] Fonctionnalit√©s critiques op√©rationnelles
- [ ] Int√©grit√© des donn√©es v√©rifi√©e

## Communication

### Notifications √† envoyer
- [ ] √âquipe technique
- [ ] Management
- [ ] Utilisateurs finaux
- [ ] Clients (si applicable)

### Template de communication
```
Objet : [R√âSOLU/EN COURS] Incident [ID] - [Service]

Le service [nom] a √©t√© restaur√© suite √† l'incident [description].
- D√©but d'incident : [heure]
- Fin d'incident : [heure]
- Dur√©e d'indisponibilit√© : [dur√©e]
- Cause racine : [cause]
- Actions correctives : [actions]
```

## Post-mortem

### Points √† analyser
- [ ] Cause racine de l'incident
- [ ] Efficacit√© de la r√©ponse
- [ ] Am√©liorations possibles
- [ ] Mise √† jour des proc√©dures

### Actions correctives
- [ ] [Action 1] - Responsable : [nom] - Date : [date]
- [ ] [Action 2] - Responsable : [nom] - Date : [date]
```

---

## Conclusion

La r√©cup√©ration de donn√©es et la gestion des pannes de stockage n√©cessitent une approche m√©thodique et des outils appropri√©s. Les points cl√©s √† retenir :

### Pr√©paration
- **Sauvegardes r√©guli√®res** et test√©es
- **Surveillance proactive** des syst√®mes de stockage
- **Documentation** des proc√©dures et configurations
- **Formation** des √©quipes aux proc√©dures d'urgence

### Outils essentiels
- **ddrescue** pour la r√©cup√©ration de disques d√©faillants
- **testdisk/photorec** pour la r√©cup√©ration de partitions et fichiers
- **fsck** pour la r√©paration des syst√®mes de fichiers
- **Velero** pour la sauvegarde/restauration Kubernetes

### Bonnes pratiques
- **Tests r√©guliers** des proc√©dures de r√©cup√©ration
- **Automatisation** des t√¢ches r√©p√©titives
- **Monitoring** continu des m√©triques de sant√©
- **Documentation** √† jour des configurations

### Strat√©gies modernes
- **Snapshots** pour des sauvegardes coh√©rentes
- **R√©plication** multi-sites pour la continuit√©
- **Infrastructure as Code** pour la reproductibilit√©
- **Observabilit√©** pour la d√©tection pr√©coce des probl√®mes

Une strat√©gie de r√©cup√©ration efficace combine outils traditionnels √©prouv√©s et technologies cloud-native modernes, avec une emphase sur la pr√©vention, la d√©tection pr√©coce et la r√©cup√©ration automatis√©e.

‚è≠Ô∏è
