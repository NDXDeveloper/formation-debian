üîù Retour au [Sommaire](/SOMMAIRE.md)

# 12.3 Haute disponibilit√© moderne

## Introduction

La haute disponibilit√© (HA) est l'art de concevoir des syst√®mes qui restent op√©rationnels m√™me en cas de pannes partielles. Dans un monde o√π les interruptions de service co√ªtent cher en r√©putation et en revenus, ma√Ætriser les techniques de haute disponibilit√© devient essentiel.

Cette section explore les technologies et strat√©gies modernes pour √©liminer les points de d√©faillance unique (SPOF - Single Point of Failure) et atteindre des niveaux de disponibilit√© de 99.9% et plus, en combinant approches traditionnelles et solutions cloud-native.

**Concepts cl√©s :**
- **Redondance** : √âlimination des points de d√©faillance unique
- **Basculement automatique** : Transition transparente vers des ressources de secours
- **√âquilibrage de charge** : R√©partition du trafic sur plusieurs instances
- **R√©plication** : Synchronisation des donn√©es et √©tats entre n≈ìuds

---

## 12.3.1 Clustering avec Pacemaker

### Introduction √† Pacemaker

Pacemaker est un gestionnaire de ressources de cluster qui orchestre les services dans un environnement haute disponibilit√©. Il d√©cide quand et o√π d√©marrer, arr√™ter ou d√©placer les services en fonction de l'√©tat du cluster.

**Architecture Pacemaker :**
- **CRM (Cluster Resource Manager)** : Cerveau du cluster qui prend les d√©cisions
- **CIB (Cluster Information Base)** : Base de donn√©es de configuration distribu√©e
- **PE (Policy Engine)** : Moteur qui d√©termine les actions √† effectuer
- **TE (Transition Engine)** : Ex√©cute les transitions d'√©tat

### Installation et configuration de base

**Installation sur Debian :**

```bash
# Installation des composants essentiels
sudo apt update
sudo apt install pacemaker corosync pcs fence-agents-all

# Activation des services
sudo systemctl enable pacemaker
sudo systemctl enable corosync
sudo systemctl enable pcsd
```

**Configuration r√©seau pr√©alable :**

```bash
# Configuration /etc/hosts sur tous les n≈ìuds
echo "192.168.1.10 node1.cluster.local node1" | sudo tee -a /etc/hosts
echo "192.168.1.11 node2.cluster.local node2" | sudo tee -a /etc/hosts
echo "192.168.1.12 node3.cluster.local node3" | sudo tee -a /etc/hosts

# Test de connectivit√©
ping -c 3 node2.cluster.local
ping -c 3 node3.cluster.local
```

### Configuration Corosync

Corosync fournit la couche de communication du cluster :

```bash
# Configuration /etc/corosync/corosync.conf
sudo tee /etc/corosync/corosync.conf << 'EOF'
totem {
    version: 2
    cluster_name: production-cluster
    secauth: on
    threads: 0
    interface {
        ringnumber: 0
        bindnetaddr: 192.168.1.0
        mcastaddr: 239.255.1.1
        mcastport: 5405
        ttl: 1
    }
}

logging {
    fileline: off
    to_logfile: yes
    to_syslog: yes
    logfile: /var/log/cluster/corosync.log
    debug: off
    timestamp: on
    logger_subsys {
        subsys: QUORUM
        debug: off
    }
}

quorum {
    provider: corosync_votequorum
    two_node: 0
}

nodelist {
    node {
        ring0_addr: 192.168.1.10
        nodeid: 1
        name: node1
    }
    node {
        ring0_addr: 192.168.1.11
        nodeid: 2
        name: node2
    }
    node {
        ring0_addr: 192.168.1.12
        nodeid: 3
        name: node3
    }
}
EOF

# Copier la configuration sur tous les n≈ìuds
scp /etc/corosync/corosync.conf node2:/etc/corosync/
scp /etc/corosync/corosync.conf node3:/etc/corosync/

# G√©n√©ration et distribution de la cl√© d'authentification
sudo corosync-keygen
sudo scp /etc/corosync/authkey node2:/etc/corosync/
sudo scp /etc/corosync/authkey node3:/etc/corosync/
```

### D√©marrage et configuration du cluster

```bash
# D√©marrage des services sur tous les n≈ìuds
sudo systemctl start corosync
sudo systemctl start pacemaker

# V√©rification de l'√©tat du cluster
sudo crm status

# Configuration initiale du cluster
sudo crm configure property stonith-enabled=false
sudo crm configure property no-quorum-policy=ignore
sudo crm configure rsc_defaults resource-stickiness=100
```

### Configuration d'une ressource simple

**Service web hautement disponible :**

```bash
# Cr√©ation d'une ressource IP virtuelle
sudo crm configure primitive vip ocf:heartbeat:IPaddr2 \
    params ip=192.168.1.100 cidr_netmask=24 \
    op monitor interval=30s

# Cr√©ation d'une ressource service web
sudo crm configure primitive webserver ocf:heartbeat:apache \
    params configfile=/etc/apache2/apache2.conf \
    op monitor interval=1min

# Cr√©ation d'un groupe pour lier les ressources
sudo crm configure group web-group vip webserver

# V√©rification de la configuration
sudo crm configure show
```

### Contraintes et politiques

```bash
# Contrainte de localisation (pr√©f√©rence de n≈ìud)
sudo crm configure location web-prefer-node1 web-group 50: node1

# Contrainte de colocation (ressources ensemble)
sudo crm configure colocation web-with-vip inf: webserver vip

# Contrainte d'ordre (s√©quence de d√©marrage)
sudo crm configure order vip-before-web mandatory: vip webserver

# Politique de basculement automatique
sudo crm configure primitive web-monitor ocf:heartbeat:anything \
    params binfile="/usr/bin/curl" cmdline_options="-f http://192.168.1.100/" \
    op monitor interval=10s timeout=20s
```

### Gestion op√©rationnelle

```bash
# Surveillance continue du cluster
sudo crm status

# D√©placer manuellement une ressource
sudo crm resource move web-group node2

# Nettoyer les erreurs d'une ressource
sudo crm resource cleanup webserver

# Mise en maintenance d'un n≈ìud
sudo crm node standby node1

# Sortie de maintenance
sudo crm node online node1

# Test de basculement
sudo crm resource ban web-group node1
sudo crm resource clear web-group node1
```

**Script de monitoring du cluster :**

```bash
#!/bin/bash
# Script de surveillance Pacemaker

LOGFILE="/var/log/cluster-monitor.log"
EMAIL="admin@example.com"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOGFILE
}

check_cluster_status() {
    # V√©rification de l'√©tat g√©n√©ral
    if ! sudo crm status >/dev/null 2>&1; then
        log "ERREUR: Cluster Pacemaker inaccessible"
        echo "Le cluster Pacemaker ne r√©pond pas" | mail -s "Alerte Cluster" $EMAIL
        return 1
    fi

    # V√©rification des n≈ìuds
    OFFLINE_NODES=$(sudo crm status | grep -i "offline" | wc -l)
    if [ $OFFLINE_NODES -gt 0 ]; then
        log "ATTENTION: $OFFLINE_NODES n≈ìud(s) hors ligne"
        sudo crm status | grep -i "offline" | mail -s "N≈ìuds hors ligne" $EMAIL
    fi

    # V√©rification des ressources
    FAILED_RESOURCES=$(sudo crm status | grep -i "failed" | wc -l)
    if [ $FAILED_RESOURCES -gt 0 ]; then
        log "CRITIQUE: $FAILED_RESOURCES ressource(s) en √©chec"
        sudo crm status | grep -i "failed" | mail -s "Ressources en √©chec" $EMAIL
    fi

    log "Cluster status: OK"
}

# Ex√©cution de la v√©rification
check_cluster_status
```

---

## 12.3.2 Load balancing (HAProxy, NGINX)

### HAProxy - √âquilibrage de charge avanc√©

HAProxy est une solution de load balancing haute performance qui supporte de nombreux algorithmes de r√©partition et fonctionnalit√©s avanc√©es.

**Installation et configuration de base :**

```bash
# Installation
sudo apt install haproxy

# Activation du service
sudo systemctl enable haproxy
```

**Configuration /etc/haproxy/haproxy.cfg :**

```bash
# Configuration HAProxy compl√®te
sudo tee /etc/haproxy/haproxy.cfg << 'EOF'
global
    log 127.0.0.1:514 local0
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    mode http
    log global
    option httplog
    option dontlognull
    option log-health-checks
    option forwardfor
    option http-server-close
    timeout connect 5000
    timeout client 50000
    timeout server 50000
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
    errorfile 408 /etc/haproxy/errors/408.http
    errorfile 500 /etc/haproxy/errors/500.http
    errorfile 502 /etc/haproxy/errors/502.http
    errorfile 503 /etc/haproxy/errors/503.http
    errorfile 504 /etc/haproxy/errors/504.http

# Interface de statistiques
frontend stats
    bind *:8080
    stats enable
    stats uri /stats
    stats refresh 30s
    stats admin if TRUE

# Frontend pour le trafic web
frontend web-frontend
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/server.pem
    redirect scheme https if !{ ssl_fc }

    # ACL pour routage intelligent
    acl is_api path_beg /api/
    acl is_admin path_beg /admin/

    use_backend api-servers if is_api
    use_backend admin-servers if is_admin
    default_backend web-servers

# Backend pour serveurs web
backend web-servers
    balance roundrobin
    option httpchk GET /health

    server web1 192.168.1.20:80 check
    server web2 192.168.1.21:80 check
    server web3 192.168.1.22:80 check backup

# Backend pour API
backend api-servers
    balance leastconn
    option httpchk GET /api/health

    server api1 192.168.1.30:8080 check
    server api2 192.168.1.31:8080 check

# Backend pour administration
backend admin-servers
    balance source
    option httpchk GET /admin/ping

    server admin1 192.168.1.40:8080 check
EOF

# Test de la configuration
sudo haproxy -f /etc/haproxy/haproxy.cfg -c

# Red√©marrage
sudo systemctl restart haproxy
```

### Algorithmes de load balancing

```bash
# Configuration avec diff√©rents algorithmes

# Round Robin (par d√©faut)
backend web-roundrobin
    balance roundrobin
    server web1 192.168.1.20:80 check
    server web2 192.168.1.21:80 check

# Least Connections (moins de connexions actives)
backend api-leastconn
    balance leastconn
    server api1 192.168.1.30:8080 check
    server api2 192.168.1.31:8080 check

# Source (bas√© sur l'IP client)
backend session-sticky
    balance source
    server app1 192.168.1.40:8080 check
    server app2 192.168.1.41:8080 check

# URI (bas√© sur l'URI de la requ√™te)
backend content-based
    balance uri
    hash-type consistent
    server content1 192.168.1.50:80 check
    server content2 192.168.1.51:80 check

# Weighted Round Robin (avec poids)
backend weighted-servers
    balance roundrobin
    server powerful 192.168.1.60:80 check weight 3
    server standard 192.168.1.61:80 check weight 1
```

### Health checks avanc√©s

```bash
# Configuration des health checks sophistiqu√©s
backend database-servers
    balance leastconn
    option mysql-check user haproxy_check

    server db1 192.168.1.70:3306 check
    server db2 192.168.1.71:3306 check backup

    # Health check HTTP personnalis√©
    option httpchk POST /health-check
    http-check expect status 200
    http-check expect string "OK"

# Health check avec en-t√™tes personnalis√©s
backend api-with-auth
    option httpchk GET /health
    http-check send-state
    http-check expect status 200

    server api1 192.168.1.80:8080 check inter 5s rise 2 fall 3
    server api2 192.168.1.81:8080 check inter 5s rise 2 fall 3
```

### Configuration SSL/TLS

```bash
# Configuration SSL terminaison
frontend https-frontend
    bind *:443 ssl crt /etc/ssl/certs/server.pem

    # Redirection HTTP vers HTTPS
    redirect scheme https code 301 if !{ ssl_fc }

    # Headers de s√©curit√©
    http-response set-header Strict-Transport-Security "max-age=31536000; includeSubDomains"
    http-response set-header X-Frame-Options "SAMEORIGIN"
    http-response set-header X-Content-Type-Options "nosniff"

    default_backend secure-servers

# SSL passthrough (pour applications g√©rant SSL)
frontend ssl-passthrough
    mode tcp
    bind *:8443
    default_backend ssl-backend

backend ssl-backend
    mode tcp
    balance roundrobin
    server app1 192.168.1.90:8443 check
    server app2 192.168.1.91:8443 check
```

### NGINX - Load balancer moderne

NGINX offre des fonctionnalit√©s de load balancing int√©gr√©es avec des performances exceptionnelles.

**Configuration NGINX load balancer :**

```bash
# Installation
sudo apt install nginx

# Configuration /etc/nginx/conf.d/load-balancer.conf
sudo tee /etc/nginx/conf.d/load-balancer.conf << 'EOF'
# D√©finition des upstream
upstream web-backend {
    least_conn;
    server 192.168.1.20:80 max_fails=3 fail_timeout=30s;
    server 192.168.1.21:80 max_fails=3 fail_timeout=30s;
    server 192.168.1.22:80 backup;
}

upstream api-backend {
    ip_hash;
    server 192.168.1.30:8080 weight=3;
    server 192.168.1.31:8080 weight=1;

    # Health check (NGINX Plus)
    # health_check uri=/health match=server_ok;
}

# Configuration du load balancer
server {
    listen 80;
    listen 443 ssl http2;
    server_name example.com;

    # Configuration SSL
    ssl_certificate /etc/ssl/certs/server.crt;
    ssl_certificate_key /etc/ssl/private/server.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;

    # Logs personnalis√©s
    access_log /var/log/nginx/lb-access.log combined;
    error_log /var/log/nginx/lb-error.log;

    # Headers pour les backends
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;

    # Route vers API
    location /api/ {
        proxy_pass http://api-backend;
        proxy_timeout 60s;
        proxy_read_timeout 60s;
        proxy_connect_timeout 60s;
    }

    # Route par d√©faut
    location / {
        proxy_pass http://web-backend;

        # Health check local
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
        proxy_next_upstream_tries 3;
        proxy_next_upstream_timeout 10s;
    }

    # Page de statut
    location /nginx-status {
        stub_status on;
        access_log off;
        allow 127.0.0.1;
        allow 192.168.1.0/24;
        deny all;
    }
}

# Health check externe (script personnalis√©)
server {
    listen 8080;
    server_name localhost;

    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
EOF

# Test de la configuration
sudo nginx -t

# Rechargement
sudo systemctl reload nginx
```

### Load balancing avanc√© avec NGINX

```bash
# Configuration avanc√©e avec sticky sessions
sudo tee /etc/nginx/conf.d/advanced-lb.conf << 'EOF'
# Map pour sticky sessions
map $cookie_sessionid $backend_pool {
    ~.{32} backend_a;
    default backend_b;
}

upstream backend_a {
    server 192.168.1.100:8080;
    server 192.168.1.101:8080;
}

upstream backend_b {
    server 192.168.1.102:8080;
    server 192.168.1.103:8080;
}

# Rate limiting
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=web:10m rate=50r/s;

server {
    listen 80;

    # Application du rate limiting
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        proxy_pass http://api-backend;
    }

    location / {
        limit_req zone=web burst=100 nodelay;

        # Sticky sessions bas√©es sur cookie
        proxy_pass http://$backend_pool;

        # D√©finir le cookie de session si absent
        add_header Set-Cookie "sessionid=$request_id; Path=/; HttpOnly" always;
    }
}
EOF
```

### Monitoring des load balancers

**Script de monitoring HAProxy :**

```bash
#!/bin/bash
# Monitoring HAProxy

HAPROXY_STATS_URL="http://localhost:8080/stats;csv"
LOG_FILE="/var/log/haproxy-monitor.log"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

check_haproxy_health() {
    # R√©cup√©ration des stats HAProxy
    STATS=$(curl -s $HAPROXY_STATS_URL)

    if [ $? -ne 0 ]; then
        log "ERREUR: Impossible de r√©cup√©rer les statistiques HAProxy"
        return 1
    fi

    # Analyse des backends en √©chec
    FAILED_SERVERS=$(echo "$STATS" | awk -F',' '$18=="DOWN" {print $1"/"$2}')

    if [ -n "$FAILED_SERVERS" ]; then
        log "ALERTE: Serveurs en √©chec: $FAILED_SERVERS"
        echo "Serveurs HAProxy en √©chec: $FAILED_SERVERS" | mail -s "HAProxy Alert" admin@example.com
    fi

    # V√©rification de la charge
    echo "$STATS" | awk -F',' '
    $1 !~ /^#/ && $2 !~ /FRONTEND|BACKEND/ {
        if ($5 > 100) {
            print "ATTENTION: Serveur " $1"/"$2 " - Sessions: " $5
        }
    }'
}

check_haproxy_health
```

**Monitoring NGINX :**

```bash
#!/bin/bash
# Monitoring NGINX

NGINX_STATUS_URL="http://localhost:8080/nginx-status"
LOG_FILE="/var/log/nginx-monitor.log"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

check_nginx_health() {
    # V√©rification du statut NGINX
    STATUS=$(curl -s $NGINX_STATUS_URL)

    if [ $? -ne 0 ]; then
        log "ERREUR: NGINX inaccessible"
        return 1
    fi

    # Extraction des m√©triques
    ACTIVE_CONNECTIONS=$(echo "$STATUS" | awk '/Active connections/ {print $3}')
    REQUESTS_PER_SEC=$(echo "$STATUS" | awk 'NR==3 {print ($3-prev)/interval}' prev=0 interval=60)

    log "Connexions actives: $ACTIVE_CONNECTIONS"

    # Alerte si trop de connexions
    if [ "$ACTIVE_CONNECTIONS" -gt 1000 ]; then
        log "ALERTE: Trop de connexions actives ($ACTIVE_CONNECTIONS)"
        echo "NGINX: $ACTIVE_CONNECTIONS connexions actives" | mail -s "NGINX Alert" admin@example.com
    fi
}

check_nginx_health
```

---

## 12.3.3 Kubernetes HA (control plane)

### Architecture haute disponibilit√© Kubernetes

Un cluster Kubernetes HA n√©cessite plusieurs composants redondants :

- **API Server** : Multiple instances derri√®re un load balancer
- **etcd** : Cluster avec nombre impair de n≈ìuds (3, 5, 7)
- **Controller Manager** : Avec √©lection de leader
- **Scheduler** : Avec √©lection de leader
- **Load Balancer** : Pour l'API Server

### Configuration d'un cluster HA avec kubeadm

**Pr√©requis pour 3 masters + LB :**

```bash
# Sur le load balancer (HAProxy)
sudo tee /etc/haproxy/haproxy.cfg << 'EOF'
global
    log stdout local0
    daemon

defaults
    mode tcp
    log global
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend k8s-api
    bind *:6443
    default_backend k8s-api-backend

backend k8s-api-backend
    balance roundrobin
    option tcp-check

    server master1 192.168.1.10:6443 check
    server master2 192.168.1.11:6443 check
    server master3 192.168.1.12:6443 check
EOF

sudo systemctl restart haproxy
```

**Initialisation du premier master :**

```bash
# Configuration kubeadm
sudo tee /etc/kubernetes/kubeadm-config.yaml << 'EOF'
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.10
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock

---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
controlPlaneEndpoint: "192.168.1.100:6443"  # IP du load balancer
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
etcd:
  external:
    endpoints:
    - https://192.168.1.10:2379
    - https://192.168.1.11:2379
    - https://192.168.1.12:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key

---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: systemd
EOF

# Initialisation du cluster
sudo kubeadm init --config=/etc/kubernetes/kubeadm-config.yaml --upload-certs
```

**Ajout des masters suppl√©mentaires :**

```bash
# Sur master2 et master3
sudo kubeadm join 192.168.1.100:6443 \
    --token <token> \
    --discovery-token-ca-cert-hash sha256:<hash> \
    --control-plane \
    --certificate-key <certificate-key>
```

### Configuration etcd en cluster

**Installation et configuration etcd :**

```bash
# Installation etcd sur les 3 n≈ìuds masters
sudo apt install etcd-server etcd-client

# Configuration /etc/default/etcd sur master1
sudo tee /etc/default/etcd << 'EOF'
ETCD_NAME="master1"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_CLIENT_URLS="https://192.168.1.10:2379,https://127.0.0.1:2379"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.1.10:2379"
ETCD_LISTEN_PEER_URLS="https://192.168.1.10:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.1.10:2380"
ETCD_INITIAL_CLUSTER="master1=https://192.168.1.10:2380,master2=https://192.168.1.11:2380,master3=https://192.168.1.12:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-etcd-cluster"

# TLS Configuration
ETCD_CERT_FILE="/etc/kubernetes/pki/etcd/server.crt"
ETCD_KEY_FILE="/etc/kubernetes/pki/etcd/server.key"
ETCD_TRUSTED_CA_FILE="/etc/kubernetes/pki/etcd/ca.crt"
ETCD_PEER_CERT_FILE="/etc/kubernetes/pki/etcd/peer.crt"
ETCD_PEER_KEY_FILE="/etc/kubernetes/pki/etcd/peer.key"
ETCD_PEER_TRUSTED_CA_FILE="/etc/kubernetes/pki/etcd/ca.crt"
EOF

# Adaptez pour master2 et master3 avec leurs IPs respectives
```

### Surveillance du control plane

**Script de monitoring Kubernetes HA :**

```bash
#!/bin/bash
# Monitoring Kubernetes HA (suite)

KUBECONFIG="/etc/kubernetes/admin.conf"
LOG_FILE="/var/log/k8s-ha-monitor.log"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

check_api_servers() {
    log "=== V√©rification API Servers ==="

    # Test de chaque API server
    for endpoint in 192.168.1.10:6443 192.168.1.11:6443 192.168.1.12:6443; do
        if curl -k -s https://$endpoint/healthz > /dev/null; then
            log "‚úì API Server $endpoint: OK"
        else
            log "‚úó API Server $endpoint: √âCHEC"
        fi
    done
}

check_etcd_cluster() {
    log "=== V√©rification cluster etcd ==="

    ETCD_ENDPOINTS="https://192.168.1.10:2379,https://192.168.1.11:2379,https://192.168.1.12:2379"

    # Sant√© du cluster etcd
    ETCD_HEALTH=$(ETCDCTL_API=3 etcdctl \
        --endpoints=$ETCD_ENDPOINTS \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        --cert=/etc/kubernetes/pki/etcd/server.crt \
        --key=/etc/kubernetes/pki/etcd/server.key \
        endpoint health 2>/dev/null)

    if [ $? -eq 0 ]; then
        log "‚úì Cluster etcd: Sain"
        echo "$ETCD_HEALTH" | while read line; do
            log "$line"
        done
    else
        log "‚úó Cluster etcd: Probl√®me d√©tect√©"
    fi
}

check_control_plane_pods() {
    log "=== V√©rification pods control plane ==="

    # V√©rification des pods syst√®me
    SYSTEM_PODS=$(kubectl --kubeconfig=$KUBECONFIG get pods -n kube-system \
        -l tier=control-plane -o custom-columns=NAME:.metadata.name,STATUS:.status.phase --no-headers)

    echo "$SYSTEM_PODS" | while read name status; do
        if [ "$status" = "Running" ]; then
            log "‚úì Pod $name: $status"
        else
            log "‚úó Pod $name: $status"
        fi
    done
}

check_cluster_connectivity() {
    log "=== Test connectivit√© cluster ==="

    # Test simple avec kubectl
    if kubectl --kubeconfig=$KUBECONFIG get nodes > /dev/null 2>&1; then
        log "‚úì Connectivit√© cluster: OK"

        # Nombre de n≈ìuds Ready
        READY_NODES=$(kubectl --kubeconfig=$KUBECONFIG get nodes --no-headers | grep " Ready " | wc -l)
        TOTAL_NODES=$(kubectl --kubeconfig=$KUBECONFIG get nodes --no-headers | wc -l)
        log "N≈ìuds Ready: $READY_NODES/$TOTAL_NODES"

        if [ $READY_NODES -lt $TOTAL_NODES ]; then
            log "ATTENTION: Des n≈ìuds ne sont pas Ready"
            kubectl --kubeconfig=$KUBECONFIG get nodes | grep -v " Ready "
        fi
    else
        log "‚úó Connectivit√© cluster: √âCHEC"
    fi
}

check_load_balancer() {
    log "=== V√©rification Load Balancer ==="

    # Test de l'endpoint HA
    if curl -k -s https://192.168.1.100:6443/healthz > /dev/null; then
        log "‚úì Load Balancer endpoint: OK"
    else
        log "‚úó Load Balancer endpoint: √âCHEC"
    fi

    # V√©rification HAProxy (si utilis√©)
    if systemctl is-active --quiet haproxy; then
        log "‚úì Service HAProxy: Actif"

        # Stats HAProxy
        HAPROXY_STATS=$(echo "show stat" | socat stdio /var/run/haproxy/admin.sock 2>/dev/null)
        if [ $? -eq 0 ]; then
            echo "$HAPROXY_STATS" | grep "k8s-api-backend" | while IFS=',' read -r line; do
                backend=$(echo $line | cut -d',' -f1-2)
                status=$(echo $line | cut -d',' -f18)
                log "Backend $backend: $status"
            done
        fi
    else
        log "‚úó Service HAProxy: Inactif"
    fi
}

# Fonction principale
main() {
    log "=== D√âBUT MONITORING KUBERNETES HA ==="

    check_api_servers
    check_etcd_cluster
    check_control_plane_pods
    check_cluster_connectivity
    check_load_balancer

    log "=== FIN MONITORING KUBERNETES HA ==="
}

# Ex√©cution
main

# Si erreurs d√©tect√©es, envoyer une alerte
if grep -q "‚úó" $LOG_FILE; then
    tail -50 $LOG_FILE | mail -s "Alerte Kubernetes HA" admin@example.com
fi
```

### Configuration des sondes de sant√©

**HealthCheck avanc√© pour API Server :**

```yaml
# Custom health check pod
apiVersion: v1
kind: Pod
metadata:
  name: k8s-health-checker
  namespace: kube-system
spec:
  containers:
  - name: health-checker
    image: curlimages/curl:latest
    command:
    - /bin/sh
    - -c
    - |
      while true; do
        # Test API Server
        curl -k -s https://kubernetes.default.svc.cluster.local/healthz
        if [ $? -eq 0 ]; then
          echo "$(date): API Server OK"
        else
          echo "$(date): API Server KO"
        fi

        # Test etcd via API
        curl -k -s https://kubernetes.default.svc.cluster.local/api/v1/namespaces/default
        if [ $? -eq 0 ]; then
          echo "$(date): etcd OK"
        else
          echo "$(date): etcd KO"
        fi

        sleep 30
      done
  restartPolicy: Always
```

---

## 12.3.4 R√©plication de services

### R√©plication de bases de donn√©es

**MySQL Master-Slave :**

```bash
# Configuration du master MySQL
sudo tee -a /etc/mysql/mysql.conf.d/mysqld.cnf << 'EOF'
[mysqld]
server-id = 1
log-bin = mysql-bin
binlog-do-db = production_db
EOF

sudo systemctl restart mysql

# Cr√©ation de l'utilisateur de r√©plication
mysql -u root -p << 'EOF'
CREATE USER 'replication'@'%' IDENTIFIED BY 'replica_password';
GRANT REPLICATION SLAVE ON *.* TO 'replication'@'%';
FLUSH PRIVILEGES;
SHOW MASTER STATUS;
EOF

# Configuration du slave
sudo tee -a /etc/mysql/mysql.conf.d/mysqld.cnf << 'EOF'
[mysqld]
server-id = 2
relay-log = mysql-relay-bin
read-only = 1
EOF

sudo systemctl restart mysql

# Configuration de la r√©plication sur le slave
mysql -u root -p << 'EOF'
CHANGE MASTER TO
  MASTER_HOST='192.168.1.10',
  MASTER_USER='replication',
  MASTER_PASSWORD='replica_password',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;

START SLAVE;
SHOW SLAVE STATUS\G;
EOF
```

**PostgreSQL Streaming Replication :**

```bash
# Configuration du master PostgreSQL
sudo tee -a /etc/postgresql/13/main/postgresql.conf << 'EOF'
# R√©plication
wal_level = replica
max_wal_senders = 3
max_replication_slots = 3
synchronous_commit = on
synchronous_standby_names = 'standby1'
EOF

sudo tee -a /etc/postgresql/13/main/pg_hba.conf << 'EOF'
# R√©plication
host replication replicator 192.168.1.11/32 md5
EOF

# Cr√©ation de l'utilisateur de r√©plication
sudo -u postgres psql << 'EOF'
CREATE USER replicator WITH REPLICATION ENCRYPTED PASSWORD 'replica_password';
EOF

sudo systemctl restart postgresql

# Configuration du standby
sudo systemctl stop postgresql

# Sauvegarde initiale
sudo -u postgres pg_basebackup -h 192.168.1.10 -D /var/lib/postgresql/13/main -U replicator -P -v -R -W

# Configuration standby
sudo tee /var/lib/postgresql/13/main/standby.signal << 'EOF'
EOF

sudo tee -a /var/lib/postgresql/13/main/postgresql.conf << 'EOF'
# Standby configuration
primary_conninfo = 'host=192.168.1.10 port=5432 user=replicator password=replica_password application_name=standby1'
promote_trigger_file = '/tmp/postgresql.trigger'
EOF

sudo systemctl start postgresql
```

### R√©plication d'applications avec Kubernetes

**D√©ploiement haute disponibilit√© :**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-ha
  labels:
    app: web-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      # Anti-affinit√© pour r√©partir sur diff√©rents n≈ìuds
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web-app
              topologyKey: kubernetes.io/hostname

      containers:
      - name: web-app
        image: nginx:alpine
        ports:
        - containerPort: 80

        # Probes de sant√©
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

        # Ressources
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"

        # Variables d'environnement
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name

---
# Service pour exposer l'application
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP

---
# HorizontalPodAutoscaler pour auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app-ha
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### R√©plication de donn√©es avec rsync

**Script de synchronisation en temps r√©el :**

```bash
#!/bin/bash
# R√©plication en temps r√©el avec inotify et rsync

SOURCE_DIR="/data/production"
REPLICA_HOST="replica.example.com"
REPLICA_DIR="/data/replica"
LOG_FILE="/var/log/replication.log"
LOCK_FILE="/var/run/replication.lock"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

# V√©rification du lock
if [ -f $LOCK_FILE ]; then
    PID=$(cat $LOCK_FILE)
    if ps -p $PID > /dev/null 2>&1; then
        log "R√©plication d√©j√† en cours (PID: $PID)"
        exit 1
    else
        rm -f $LOCK_FILE
    fi
fi

echo $$ > $LOCK_FILE

# Fonction de nettoyage
cleanup() {
    log "Arr√™t de la r√©plication"
    rm -f $LOCK_FILE
    kill $INOTIFY_PID 2>/dev/null
    exit 0
}

trap cleanup SIGTERM SIGINT

# Synchronisation initiale
log "Synchronisation initiale..."
rsync -avz --delete \
    --exclude='*.tmp' \
    --exclude='*.log' \
    $SOURCE_DIR/ $REPLICA_HOST:$REPLICA_DIR/

if [ $? -eq 0 ]; then
    log "‚úì Synchronisation initiale r√©ussie"
else
    log "‚úó √âchec de la synchronisation initiale"
    rm -f $LOCK_FILE
    exit 1
fi

# Surveillance des changements avec inotifywait
log "D√©marrage de la surveillance en temps r√©el..."
inotifywait -m -r -e modify,create,delete,move $SOURCE_DIR --format '%w%f %e' |
while read file event; do
    log "Changement d√©tect√©: $file ($event)"

    # Synchronisation incr√©mentale
    rsync -avz --delete \
        --exclude='*.tmp' \
        --exclude='*.log' \
        $SOURCE_DIR/ $REPLICA_HOST:$REPLICA_DIR/

    if [ $? -eq 0 ]; then
        log "‚úì Synchronisation incr√©mentale r√©ussie"
    else
        log "‚úó √âchec de la synchronisation incr√©mentale"
    fi
done &

INOTIFY_PID=$!
log "Surveillance active (PID: $INOTIFY_PID)"

# Attente infinie
wait $INOTIFY_PID
```

### R√©plication cross-datacenter

**Configuration multi-r√©gion :**

```bash
#!/bin/bash
# R√©plication multi-r√©gion

REGIONS=("us-east-1" "eu-west-1" "ap-southeast-1")
PRIMARY_REGION="us-east-1"
DATA_DIR="/data/global"
LOG_FILE="/var/log/cross-dc-replication.log"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

replicate_to_region() {
    local region=$1
    local endpoint="replica-${region}.example.com"

    log "R√©plication vers $region..."

    # Utilisation de rsync avec compression et limitation de bande passante
    rsync -avz --bwlimit=10000 \
        --timeout=300 \
        --exclude='*.tmp' \
        --exclude='cache/*' \
        $DATA_DIR/ $endpoint:$DATA_DIR/

    if [ $? -eq 0 ]; then
        log "‚úì R√©plication vers $region r√©ussie"

        # Notification de succ√®s
        curl -X POST "https://monitoring.example.com/api/events" \
            -H "Content-Type: application/json" \
            -d "{\"type\":\"replication_success\",\"region\":\"$region\",\"timestamp\":\"$(date -Iseconds)\"}"
    else
        log "‚úó √âchec r√©plication vers $region"

        # Alerte
        curl -X POST "https://monitoring.example.com/api/alerts" \
            -H "Content-Type: application/json" \
            -d "{\"type\":\"replication_failure\",\"region\":\"$region\",\"timestamp\":\"$(date -Iseconds)\"}"
    fi
}

# R√©plication s√©quentielle vers toutes les r√©gions
for region in "${REGIONS[@]}"; do
    if [ "$region" != "$PRIMARY_REGION" ]; then
        replicate_to_region $region
        sleep 60  # D√©lai entre r√©plications
    fi
done

log "Cycle de r√©plication termin√©"
```

---

## 12.3.5 Monitoring et failover automatique

### Monitoring avec Prometheus et Grafana

**Configuration Prometheus pour HA :**

```yaml
# prometheus-ha.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "ha-rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager-1:9093
          - alertmanager-2:9093

scrape_configs:
  # Monitoring des API Servers Kubernetes
  - job_name: 'kubernetes-api-servers'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - default
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecure_skip_verify: true
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      action: keep
      regex: kubernetes;https

  # Monitoring HAProxy
  - job_name: 'haproxy'
    static_configs:
    - targets: ['localhost:8080']
    metrics_path: /stats/prometheus

  # Monitoring etcd
  - job_name: 'etcd'
    static_configs:
    - targets:
      - '192.168.1.10:2379'
      - '192.168.1.11:2379'
      - '192.168.1.12:2379'
    scheme: https
    tls_config:
      ca_file: /etc/kubernetes/pki/etcd/ca.crt
      cert_file: /etc/kubernetes/pki/etcd/server.crt
      key_file: /etc/kubernetes/pki/etcd/server.key

  # Monitoring Pacemaker
  - job_name: 'pacemaker'
    static_configs:
    - targets: ['node1:9664', 'node2:9664', 'node3:9664']
```

**R√®gles d'alerte pour HA :**

```yaml
# ha-rules.yml
groups:
- name: high_availability
  rules:

  # API Server down
  - alert: KubernetesAPIServerDown
    expr: up{job="kubernetes-api-servers"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Kubernetes API Server down"
      description: "API Server {{ $labels.instance }} is down for more than 1 minute"

  # etcd cluster issues
  - alert: EtcdClusterDegraded
    expr: etcd_server_has_leader == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "etcd cluster has no leader"
      description: "etcd cluster on {{ $labels.instance }} has no leader"

  - alert: EtcdHighLatency
    expr: histogram_quantile(0.99, etcd_disk_wal_fsync_duration_seconds_bucket) > 0.5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "etcd high disk latency"
      description: "etcd 99th percentile disk latency is {{ $value }}s on {{ $labels.instance }}"

  # HAProxy backend down
  - alert: HAProxyBackendDown
    expr: haproxy_backend_up == 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "HAProxy backend down"
      description: "HAProxy backend {{ $labels.proxy }} is down"

  # Node issues
  - alert: NodeDown
    expr: up{job="node-exporter"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node down"
      description: "Node {{ $labels.instance }} is down for more than 5 minutes"

  # Pacemaker cluster issues
  - alert: PacemakerResourceFailed
    expr: pacemaker_resource_failed == 1
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Pacemaker resource failed"
      description: "Pacemaker resource {{ $labels.resource }} failed on {{ $labels.node }}"
```

### Scripts de failover automatique

**Failover automatique Kubernetes :**

```bash
#!/bin/bash
# Failover automatique Kubernetes

API_SERVERS=("192.168.1.10:6443" "192.168.1.11:6443" "192.168.1.12:6443")
LOAD_BALANCER="192.168.1.100:6443"
LOG_FILE="/var/log/k8s-failover.log"
ALERT_WEBHOOK="https://alerts.example.com/webhook"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

send_alert() {
    local message=$1
    local severity=${2:-warning}

    curl -X POST $ALERT_WEBHOOK \
        -H "Content-Type: application/json" \
        -d "{\"message\":\"$message\",\"severity\":\"$severity\",\"timestamp\":\"$(date -Iseconds)\"}" \
        2>/dev/null
}

check_api_server() {
    local endpoint=$1

    if curl -k -s --max-time 5 https://$endpoint/healthz | grep -q "ok"; then
        return 0
    else
        return 1
    fi
}

update_haproxy_config() {
    local disabled_server=$1

    log "Mise √† jour configuration HAProxy - D√©sactivation de $disabled_server"

    # D√©sactivation via socket HAProxy
    echo "disable server k8s-api-backend/$disabled_server" | \
        socat stdio /var/run/haproxy/admin.sock

    if [ $? -eq 0 ]; then
        log "‚úì Serveur $disabled_server d√©sactiv√© dans HAProxy"
        send_alert "API Server $disabled_server d√©sactiv√© automatiquement" "warning"
    else
        log "‚úó √âchec d√©sactivation $disabled_server dans HAProxy"
    fi
}

reactivate_haproxy_server() {
    local server=$1

    log "R√©activation de $server dans HAProxy"

    echo "enable server k8s-api-backend/$server" | \
        socat stdio /var/run/haproxy/admin.sock

    if [ $? -eq 0 ]; then
        log "‚úì Serveur $server r√©activ√© dans HAProxy"
        send_alert "API Server $server r√©activ√© automatiquement" "info"
    fi
}

main() {
    log "D√©but v√©rification API Servers"

    local healthy_servers=0
    local failed_servers=()

    # V√©rification de chaque API Server
    for server in "${API_SERVERS[@]}"; do
        if check_api_server $server; then
            log "‚úì API Server $server: Healthy"
            healthy_servers=$((healthy_servers + 1))

            # R√©activation si pr√©c√©demment d√©sactiv√©
            reactivate_haproxy_server $server
        else
            log "‚úó API Server $server: Failed"
            failed_servers+=($server)

            # D√©sactivation dans HAProxy
            update_haproxy_config $server
        fi
    done

    # Alerte si trop peu de serveurs sains
    if [ $healthy_servers -lt 2 ]; then
        log "CRITIQUE: Seulement $healthy_servers API Server(s) disponible(s)"
        send_alert "CRITIQUE: Cluster Kubernetes d√©grad√© - $healthy_servers/$((${#API_SERVERS[@]})) API Servers disponibles" "critical"
    elif [ $healthy_servers -lt ${#API_SERVERS[@]} ]; then
        log "ATTENTION: $healthy_servers/${#API_SERVERS[@]} API Servers disponibles"
    fi

    # Test de l'endpoint load balancer
    if check_api_server $LOAD_BALANCER; then
        log "‚úì Load Balancer endpoint: OK"
    else
        log "‚úó Load Balancer endpoint: FAILED"
        send_alert "CRITIQUE: Load Balancer Kubernetes inaccessible" "critical"
    fi

    log "Fin v√©rification API Servers"
}

# Ex√©cution avec gestion d'erreurs
set -e
trap 'log "Erreur dans le script de failover"; exit 1' ERR

main
```

**Failover base de donn√©es MySQL :**

# 12.3 Haute disponibilit√© moderne - Suite monitoring et failover

## Failover base de donn√©es MySQL (suite)

```bash
#!/bin/bash
# Failover automatique MySQL Master-Slave (continuation)

MASTER_HOST="192.168.1.20"
SLAVE_HOST="192.168.1.21"
VIP="192.168.1.25"
MYSQL_USER="monitor"
MYSQL_PASS="monitor_password"
LOG_FILE="/var/log/mysql-failover.log"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

check_mysql_health() {
    local host=$1

    # Test de connectivit√© MySQL
    mysql -h $host -u $MYSQL_USER -p$MYSQL_PASS -e "SELECT 1;" >/dev/null 2>&1
    return $?
}

check_replication_lag() {
    local slave_host=$1

    # V√©rification du lag de r√©plication
    local lag=$(mysql -h $slave_host -u $MYSQL_USER -p$MYSQL_PASS \
        -e "SHOW SLAVE STATUS\G" | grep "Seconds_Behind_Master" | awk '{print $2}')

    if [ "$lag" = "NULL" ] || [ -z "$lag" ]; then
        return 1
    elif [ $lag -gt 60 ]; then
        log "ATTENTION: Lag de r√©plication √©lev√©: ${lag}s"
        return 1
    fi

    return 0
}

promote_slave_to_master() {
    local slave_host=$1

    log "Promotion du slave $slave_host en master..."

    # Arr√™t de la r√©plication
    mysql -h $slave_host -u root -p$MYSQL_PASS << 'EOF'
STOP SLAVE;
RESET SLAVE ALL;
SET GLOBAL read_only = 0;
EOF

    if [ $? -eq 0 ]; then
        log "‚úì Slave promu en master avec succ√®s"

        # D√©placement de l'IP virtuelle
        move_vip $slave_host

        return 0
    else
        log "‚úó √âchec de la promotion du slave"
        return 1
    fi
}

move_vip() {
    local new_host=$1

    log "D√©placement de l'IP virtuelle vers $new_host"

    # Suppression de l'IP sur l'ancien master (si possible)
    ssh $MASTER_HOST "ip addr del $VIP/24 dev eth0" 2>/dev/null || true

    # Ajout de l'IP sur le nouveau master
    ssh $new_host "ip addr add $VIP/24 dev eth0"

    if [ $? -eq 0 ]; then
        log "‚úì IP virtuelle d√©plac√©e vers $new_host"

        # Mise √† jour du DNS/load balancer si n√©cessaire
        update_dns_record $new_host

        return 0
    else
        log "‚úó √âchec du d√©placement de l'IP virtuelle"
        return 1
    fi
}

update_dns_record() {
    local new_host=$1

    # Exemple avec API DNS CloudFlare
    curl -X PUT "https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID" \
        -H "Authorization: Bearer $CLOUDFLARE_TOKEN" \
        -H "Content-Type: application/json" \
        --data "{\"type\":\"A\",\"name\":\"mysql.example.com\",\"content\":\"$new_host\"}" \
        >/dev/null 2>&1

    log "‚úì Enregistrement DNS mis √† jour"
}

main() {
    log "=== D√©but v√©rification MySQL HA ==="

    # V√©rification du master
    if check_mysql_health $MASTER_HOST; then
        log "‚úì Master MySQL: Healthy"

        # V√©rification de la r√©plication
        if check_replication_lag $SLAVE_HOST; then
            log "‚úì R√©plication MySQL: OK"
        else
            log "‚ö† R√©plication MySQL: Probl√®me d√©tect√©"
        fi
    else
        log "‚úó Master MySQL: Failed"

        # V√©rification du slave
        if check_mysql_health $SLAVE_HOST; then
            log "Slave disponible - Initiation du failover"

            if promote_slave_to_master $SLAVE_HOST; then
                log "‚úì Failover MySQL r√©ussi"

                # Notification
                echo "Failover MySQL effectu√© - Nouveau master: $SLAVE_HOST" | \
                    mail -s "MySQL Failover" admin@example.com

                # Webhook pour int√©grations (Slack, Teams, etc.)
                curl -X POST https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK \
                    -H 'Content-type: application/json' \
                    --data "{\"text\":\"üîÑ MySQL Failover r√©ussi: $SLAVE_HOST est maintenant le master\"}"

            else
                log "‚úó √âchec du failover MySQL"
                echo "CRITIQUE: √âchec du failover MySQL - Intervention manuelle requise" | \
                    mail -s "URGENT: MySQL Failover Failed" admin@example.com
            fi
        else
            log "‚úó Slave MySQL √©galement indisponible - Situation critique"
            echo "CRITIQUE: Master et Slave MySQL indisponibles" | \
                mail -s "URGENT: MySQL Cluster Down" admin@example.com
        fi
    fi

    log "=== Fin v√©rification MySQL HA ==="
}

# Gestion des erreurs et nettoyage
set -e
trap 'log "Erreur dans le script de failover MySQL"; exit 1' ERR

# V√©rification des pr√©requis
if ! command -v mysql &> /dev/null; then
    log "ERREUR: Client MySQL non install√©"
    exit 1
fi

# Ex√©cution principale
main
```

### Monitoring avec Grafana - Tableaux de bord HA

**Dashboard Kubernetes HA :**

```json
{
  "dashboard": {
    "id": null,
    "title": "Kubernetes High Availability",
    "panels": [
      {
        "title": "API Server Status",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"kubernetes-api-servers\"}",
            "legendFormat": "{{instance}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        }
      },
      {
        "title": "etcd Cluster Health",
        "type": "graph",
        "targets": [
          {
            "expr": "etcd_server_has_leader",
            "legendFormat": "Has Leader - {{instance}}"
          },
          {
            "expr": "etcd_cluster_version",
            "legendFormat": "Cluster Version"
          }
        ]
      },
      {
        "title": "API Server Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, apiserver_request_duration_seconds_bucket)",
            "legendFormat": "99th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, apiserver_request_duration_seconds_bucket)",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "etcd Disk Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, etcd_disk_wal_fsync_duration_seconds_bucket)",
            "legendFormat": "WAL fsync 99th percentile - {{instance}}"
          }
        ]
      }
    ]
  }
}
```

**Dashboard HAProxy :**

```json
{
  "dashboard": {
    "title": "HAProxy Load Balancer",
    "panels": [
      {
        "title": "Backend Server Status",
        "type": "table",
        "targets": [
          {
            "expr": "haproxy_backend_up",
            "format": "table"
          }
        ]
      },
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(haproxy_backend_http_requests_total[5m])",
            "legendFormat": "{{proxy}} - {{server}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "haproxy_backend_response_time_average_seconds",
            "legendFormat": "{{proxy}} - {{server}}"
          }
        ]
      },
      {
        "title": "Active Sessions",
        "type": "graph",
        "targets": [
          {
            "expr": "haproxy_backend_current_sessions",
            "legendFormat": "{{proxy}}"
          }
        ]
      }
    ]
  }
}
```

### Automatisation compl√®te avec Ansible

**Playbook de d√©ploiement HA :**

```yaml
# ha-deployment.yml
---
- name: Deploy High Availability Infrastructure
  hosts: all
  become: yes
  vars:
    cluster_vip: "192.168.1.100"
    mysql_replication_user: "replicator"
    mysql_replication_password: "{{ vault_mysql_replication_password }}"

  tasks:
    # Configuration Pacemaker
    - name: Install Pacemaker cluster
      block:
        - name: Install Pacemaker packages
          apt:
            name:
              - pacemaker
              - corosync
              - pcs
              - fence-agents-all
            state: present

        - name: Configure corosync
          template:
            src: corosync.conf.j2
            dest: /etc/corosync/corosync.conf
            backup: yes
          notify: restart corosync

        - name: Start and enable cluster services
          systemd:
            name: "{{ item }}"
            state: started
            enabled: yes
          loop:
            - corosync
            - pacemaker

    # Configuration HAProxy
    - name: Setup HAProxy load balancer
      block:
        - name: Install HAProxy
          apt:
            name: haproxy
            state: present

        - name: Configure HAProxy
          template:
            src: haproxy.cfg.j2
            dest: /etc/haproxy/haproxy.cfg
            backup: yes
          notify: restart haproxy

        - name: Enable HAProxy stats socket
          file:
            path: /var/run/haproxy
            state: directory
            owner: haproxy
            group: haproxy

    # Configuration MySQL HA
    - name: Setup MySQL replication
      block:
        - name: Install MySQL server
          apt:
            name: mysql-server
            state: present

        - name: Configure MySQL for replication
          template:
            src: mysql-replication.cnf.j2
            dest: /etc/mysql/mysql.conf.d/replication.cnf
          notify: restart mysql

        - name: Create replication user (master only)
          mysql_user:
            name: "{{ mysql_replication_user }}"
            password: "{{ mysql_replication_password }}"
            priv: "*.*:REPLICATION SLAVE"
            host: "%"
            state: present
          when: inventory_hostname == groups['mysql_masters'][0]

    # Installation monitoring
    - name: Setup monitoring stack
      block:
        - name: Install node_exporter
          get_url:
            url: https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz
            dest: /tmp/node_exporter.tar.gz

        - name: Extract node_exporter
          unarchive:
            src: /tmp/node_exporter.tar.gz
            dest: /opt/
            remote_src: yes

        - name: Create node_exporter service
          template:
            src: node_exporter.service.j2
            dest: /etc/systemd/system/node_exporter.service
          notify: restart node_exporter

  handlers:
    - name: restart corosync
      systemd:
        name: corosync
        state: restarted

    - name: restart haproxy
      systemd:
        name: haproxy
        state: restarted

    - name: restart mysql
      systemd:
        name: mysql
        state: restarted

    - name: restart node_exporter
      systemd:
        name: node_exporter
        state: restarted
        enabled: yes
```

### Orchestration compl√®te avec Kubernetes Operators

**Custom Resource Definition pour HA :**

```yaml
# ha-cluster-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: haclusters.ha.example.com
spec:
  group: ha.example.com
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              replicas:
                type: integer
                minimum: 2
              backend:
                type: object
                properties:
                  type:
                    type: string
                    enum: ["mysql", "postgresql", "redis"]
                  version:
                    type: string
                  resources:
                    type: object
              loadBalancer:
                type: object
                properties:
                  type:
                    type: string
                    enum: ["haproxy", "nginx"]
                  virtualIP:
                    type: string
              monitoring:
                type: object
                properties:
                  enabled:
                    type: boolean
                  prometheus:
                    type: boolean
                  alertmanager:
                    type: boolean
          status:
            type: object
            properties:
              phase:
                type: string
              conditions:
                type: array
                items:
                  type: object
  scope: Namespaced
  names:
    plural: haclusters
    singular: hacluster
    kind: HACluster

---
# Exemple d'utilisation
apiVersion: ha.example.com/v1
kind: HACluster
metadata:
  name: production-db-cluster
  namespace: production
spec:
  replicas: 3
  backend:
    type: mysql
    version: "8.0"
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
      limits:
        memory: "4Gi"
        cpu: "2000m"
  loadBalancer:
    type: haproxy
    virtualIP: "10.0.1.100"
  monitoring:
    enabled: true
    prometheus: true
    alertmanager: true
```

### Tests de robustesse et chaos engineering

**Script de test de failover :**

```bash
#!/bin/bash
# Tests automatis√©s de failover

TEST_CONFIG="/etc/ha-test-config.conf"
LOG_FILE="/var/log/ha-test-$(date +%Y%m%d-%H%M%S).log"
REPORT_FILE="/tmp/ha-test-report.html"

# Configuration
API_ENDPOINT="https://192.168.1.100:6443"
MYSQL_ENDPOINT="192.168.1.25:3306"
WEB_ENDPOINT="http://192.168.1.100"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

test_kubernetes_failover() {
    log "=== Test Kubernetes API Server Failover ==="

    # Test initial de connectivit√©
    log "Test connectivit√© initiale..."
    if curl -k -s $API_ENDPOINT/healthz | grep -q "ok"; then
        log "‚úì API accessible initialement"
    else
        log "‚úó API inaccessible avant test"
        return 1
    fi

    # Simulation de panne d'un API server
    log "Simulation panne API Server node1..."
    ssh 192.168.1.10 "sudo systemctl stop kube-apiserver"

    # Test de continuit√© de service
    sleep 10
    local attempts=0
    local max_attempts=12

    while [ $attempts -lt $max_attempts ]; do
        if curl -k -s --max-time 5 $API_ENDPOINT/healthz | grep -q "ok"; then
            log "‚úì API toujours accessible apr√®s panne (tentative $((attempts+1)))"
            break
        else
            log "‚è≥ API inaccessible (tentative $((attempts+1))/$max_attempts)"
            attempts=$((attempts+1))
            sleep 5
        fi
    done

    if [ $attempts -ge $max_attempts ]; then
        log "‚úó √âchec du failover Kubernetes"
        return 1
    fi

    # Restauration du service
    ssh 192.168.1.10 "sudo systemctl start kube-apiserver"
    sleep 30

    log "‚úì Test Kubernetes failover r√©ussi"
    return 0
}

test_mysql_failover() {
    log "=== Test MySQL Master Failover ==="

    # Test connectivit√© initiale
    if mysql -h $MYSQL_ENDPOINT -u monitor -pmonitor_password -e "SELECT 1;" >/dev/null 2>&1; then
        log "‚úì MySQL accessible initialement"
    else
        log "‚úó MySQL inaccessible avant test"
        return 1
    fi

    # Simulation de panne du master
    log "Simulation panne MySQL master..."
    ssh 192.168.1.20 "sudo systemctl stop mysql"

    # Test de continuit√©
    sleep 30  # Temps pour le failover automatique

    local attempts=0
    local max_attempts=10

    while [ $attempts -lt $max_attempts ]; do
        if mysql -h $MYSQL_ENDPOINT -u monitor -pmonitor_password -e "SELECT 1;" >/dev/null 2>&1; then
            log "‚úì MySQL accessible apr√®s failover (tentative $((attempts+1)))"
            break
        else
            attempts=$((attempts+1))
            sleep 5
        fi
    done

    if [ $attempts -ge $max_attempts ]; then
        log "‚úó √âchec du failover MySQL"
        return 1
    fi

    # Restauration
    ssh 192.168.1.20 "sudo systemctl start mysql"

    log "‚úì Test MySQL failover r√©ussi"
    return 0
}

test_load_balancer() {
    log "=== Test Load Balancer Failover ==="

    local backend_servers=("192.168.1.20" "192.168.1.21" "192.168.1.22")

    # Test initial
    if curl -s --max-time 5 $WEB_ENDPOINT/health >/dev/null; then
        log "‚úì Application web accessible initialement"
    else
        log "‚úó Application web inaccessible avant test"
        return 1
    fi

    # Test de panne s√©quentielle des backends
    for server in "${backend_servers[@]}"; do
        log "Arr√™t du serveur $server..."
        ssh $server "sudo systemctl stop nginx" 2>/dev/null || true

        sleep 5

        # Test de continuit√©
        if curl -s --max-time 5 $WEB_ENDPOINT/health >/dev/null; then
            log "‚úì Service toujours accessible sans $server"
        else
            log "‚ö† Service impact√© par l'arr√™t de $server"
        fi

        # Red√©marrage
        ssh $server "sudo systemctl start nginx" 2>/dev/null || true
        sleep 5
    done

    log "‚úì Test Load Balancer termin√©"
    return 0
}

generate_report() {
    log "G√©n√©ration du rapport de test..."

    cat > $REPORT_FILE << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Rapport de Test HA - $(date)</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .success { color: green; }
        .warning { color: orange; }
        .error { color: red; }
        .log { background-color: #f5f5f5; padding: 10px; margin: 10px 0; font-family: monospace; }
    </style>
</head>
<body>
    <h1>Rapport de Test de Haute Disponibilit√©</h1>
    <p><strong>Date:</strong> $(date)</p>
    <p><strong>Dur√©e:</strong> $(($(date +%s) - $START_TIME)) secondes</p>

    <h2>R√©sum√© des Tests</h2>
    <ul>
EOF

    # Analyse des r√©sultats depuis le log
    if grep -q "‚úì Test Kubernetes failover r√©ussi" $LOG_FILE; then
        echo "        <li class=\"success\">‚úì Kubernetes Failover: R√âUSSI</li>" >> $REPORT_FILE
    else
        echo "        <li class=\"error\">‚úó Kubernetes Failover: √âCHEC</li>" >> $REPORT_FILE
    fi

    if grep -q "‚úì Test MySQL failover r√©ussi" $LOG_FILE; then
        echo "        <li class=\"success\">‚úì MySQL Failover: R√âUSSI</li>" >> $REPORT_FILE
    else
        echo "        <li class=\"error\">‚úó MySQL Failover: √âCHEC</li>" >> $REPORT_FILE
    fi

    if grep -q "‚úì Test Load Balancer termin√©" $LOG_FILE; then
        echo "        <li class=\"success\">‚úì Load Balancer: R√âUSSI</li>" >> $REPORT_FILE
    else
        echo "        <li class=\"error\">‚úó Load Balancer: √âCHEC</li>" >> $REPORT_FILE
    fi

    cat >> $REPORT_FILE << EOF
    </ul>

    <h2>Logs D√©taill√©s</h2>
    <div class="log">
$(cat $LOG_FILE | sed 's/</\&lt;/g' | sed 's/>/\&gt;/g')
    </div>

    <h2>Recommandations</h2>
    <ul>
        <li>V√©rifier les alertes g√©n√©r√©es pendant les tests</li>
        <li>Analyser les temps de r√©cup√©ration</li>
        <li>Optimiser les configurations si n√©cessaire</li>
        <li>Programmer le prochain test</li>
    </ul>
</body>
</html>
EOF

    log "‚úì Rapport g√©n√©r√©: $REPORT_FILE"
}

# Fonction principale
main() {
    START_TIME=$(date +%s)
    log "=== D√âBUT DES TESTS DE HAUTE DISPONIBILIT√â ==="

    test_kubernetes_failover
    test_mysql_failover
    test_load_balancer

    generate_report

    log "=== FIN DES TESTS DE HAUTE DISPONIBILIT√â ==="

    # Envoi du rapport par email
    echo "Rapport de test HA en pi√®ce jointe" | \
        mail -s "Rapport Test HA - $(date +%Y-%m-%d)" \
             -a $REPORT_FILE \
             admin@example.com
}

# Ex√©cution avec gestion d'erreurs
set -e
trap 'log "Erreur dans les tests HA"; exit 1' ERR

main
```

---

## Conclusion

La mise en place d'une infrastructure hautement disponible moderne n√©cessite une approche multicouche combinant :

### Technologies fondamentales
- **Pacemaker/Corosync** pour la gestion de cluster traditionnelle
- **HAProxy/NGINX** pour l'√©quilibrage de charge
- **Kubernetes HA** pour l'orchestration de conteneurs
- **R√©plication de donn√©es** pour la persistance

### Pratiques op√©rationnelles
- **Monitoring proactif** avec Prometheus/Grafana
- **Alerting intelligent** avec des seuils adapt√©s
- **Failover automatique** bas√© sur des scripts robustes
- **Tests r√©guliers** pour valider les proc√©dures

### Principes de conception
- **√âlimination des SPOF** (Single Point of Failure)
- **Redondance g√©ographique** pour la r√©silience
- **Automatisation maximale** pour r√©duire les erreurs humaines
- **Observabilit√© compl√®te** pour la d√©tection pr√©coce

### √âvolution vers le cloud-native
- **Int√©gration Kubernetes** comme plateforme d'orchestration
- **Operators** pour l'automatisation des t√¢ches complexes
- **Service Mesh** pour la communication inter-services
- **Chaos Engineering** pour tester la r√©silience

Cette approche hybride permet de b√©n√©ficier de la maturit√© des solutions traditionnelles tout en adoptant progressivement les innovations cloud-native, garantissant ainsi une √©volution coh√©rente vers les architectures modernes.

‚è≠Ô∏è
